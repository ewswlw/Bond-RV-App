# Bond RV App - Cursor AI Rules

**Last Updated**: 2025-11-07 16:45 ET - Poetry Environment Migration  
**Purpose**: Guide AI assistant in maintaining and extending bond data processing system

> **IMPORTANT**: Always update `.cursorrules` after every applicable change. Override where necessary, delete or change sections so that this file always reflects the most up-to-date project state based on actual codebase. Include date and time stamps for all major changes.

---

## AI Assistant Core Workflow

### General AI Behavior
- **Think deeply** about prompt applicability given end goal and current context
- **Use judgment** when interpreting prompts (e.g., if algo trading prompt mentions volume but raw data only has price, ignore volume requirement)
- **Always notify** user when ignoring requirements or using judgment in interpretation
- **Always ask clarifying questions** in planning mode; keep asking until all possible questions covering virtually any ambiguity are answered
- **Execute all project commands via** `poetry run ...` **(or inside an active** `poetry shell` **) to ensure the Poetry environment is used**

### File Change Summary
- **After finishing requests** that add/edit files, show summary of:
  - What was changed in each file
  - File paths affected
- **Track all modifications** for user review

### Temporary File Cleanup Protocol
- **ALWAYS clean up temporary files** created during chat sessions
- **Before completing tasks**, remove any temporary files created:
  - Migration scripts (`*_migrate*.py`)
  - Restore scripts (`*_restore*.py`)
  - Temporary test files (`*_temp*.py`)
  - Backup files (`.backup`, `*_backup*`)
- **List deleted files** in the final summary
- **Clean up at the END** of each chat session that creates temporary files
- **Common temporary files to remove**:
  - Scripts in `utils/` created for one-time operations
  - Backup parquet files in `bond_data/parquet/`
  - Test output files
  - Temporary data files

### Planning Mode Protocol
1. Ask clarifying questions first
2. Continue asking until ambiguity is eliminated
3. Present plan for approval
4. Execute after confirmation

---

## Project Overview

This is a **modular data engineering pipeline** for processing bond data for relative value trading applications.

### Core Purpose
- Process Excel bond data files into optimized Parquet time-series tables
- Support incremental data loading for daily updates

### Architecture
- **Modular ETL**: Separate modules for Extract → Transform → Load
- **Data Pipeline**: Excel + BQL workbooks → Parquet tables (historical_bond_details, universe, bql)
- **Incremental Loading**: Append mode (new dates only) vs Override mode (rebuild all)
- **Data Integrity**: Primary key enforcement, deduplication, validation

### Industry Best Practices
- **Modular design**: Project must be set up in modular way with correct folder patterns according to industry best practices
- **Single responsibility**: Each module has clear, focused purpose
- **Separation of concerns**: Clear boundaries between extraction, transformation, and loading
- **Configuration management**: All paths, constants, and settings centralized

---

## Technology Stack

### Language & Version
- **Python 3.11+** (strict requirement)
- **Type hints required** for all function signatures
- **Docstrings required** for all modules, classes, and functions

### Key Dependencies
- `pandas>=2.0.0` - Data manipulation
- `pyarrow>=12.0.0` - Parquet I/O
- `openpyxl>=3.1.0` - Excel reading
- `ipykernel>=7.1.0` (dev) - Jupyter kernel integration for notebooks (added 2025-11-08 11:05 ET)
- `nbconvert>=7.16.6` (dev) - Automated notebook execution (added 2025-11-08 11:20 ET)
- `notebook>=7.4.7` (dev) - Local Jupyter server for VS Code kernel selection (added 2025-11-08 11:35 ET)
- `pytest>=7.0.0` - Testing framework

### Environment Management
- **Environment manager**: Poetry (`pyproject.toml`, `poetry.lock`)
- **Virtual environment location**: `.venv/` (created via `poetry config virtualenvs.in-project true`)
- **ALWAYS execute code with** `poetry run ...` or inside `poetry shell`
- **Install dependencies** via `poetry install`

---

## Code Style & Conventions

### Formatting
- **Line length**: 88 characters maximum
- **Indentation**: 4 spaces (no tabs)
- **Imports**: Standard library → Third-party → Local imports (blank lines between groups)
- **Naming**: snake_case for functions/variables, PascalCase for classes, UPPER_CASE for constants

### Documentation
- **Module docstrings**: First line is summary, followed by detailed description if needed
- **Function docstrings**: Use Google style with Args/Returns/Raises sections
- **Type hints**: Use modern Python typing (e.g., `Optional[str]`, `List[pd.DataFrame]`, `Tuple[str, int]`)
- **Date and time stamps**: Include date/time stamps in all documentation entries
- **Keep documentation current**: When creating new documentation, figure out if any old documentation needs updating
- **Version tracking**: Update existing docs when adding new information

### Example Function Signature
```python
def normalize_cusip(cusip: str) -> Optional[str]:
    """
    Normalize CUSIP to uppercase 9-character format.
    
    Args:
        cusip: Raw CUSIP string (may contain lowercase, spaces, extra text)
    
    Returns:
        Normalized CUSIP or None if invalid
    """
```

---

## Code Organization

### Module Structure
```
bond_pipeline/
├── config.py        # Configuration, paths, constants (includes RUNS config)
├── utils.py         # Helper functions (date parsing, validation, logging, runs utilities)
├── extract.py       # Excel file reading and date extraction
├── transform.py     # Data cleaning, normalization, deduplication
├── load.py          # Parquet writing (append/override modes)
├── pipeline.py      # Main orchestration script with CLI
└── README.md        # Module documentation

runs_pipeline/
├── __init__.py      # Package exports
├── extract.py       # RUNS Excel file reading and Date/Time parsing
├── transform.py     # End-of-day deduplication, CUSIP validation/orphan tracking
├── load.py          # Runs parquet writing (append/override modes)
└── pipeline.py      # Main orchestration script with CLI
```

### Design Principles
1. **Single Responsibility**: Each module has one clear purpose
2. **Dependency Injection**: Pass loggers, configs as constructor arguments
3. **Separation of Concerns**: Extract (read) → Transform (clean) → Load (write)
4. **Configuration Centralization**: All paths and constants in `config.py`
5. **Immutable Config**: Don't modify config values at runtime

### Class-Based Architecture
Each pipeline stage is a class with clear responsibilities:

**Bond Pipeline:**
- **ExcelExtractor**: File discovery, Excel reading, date extraction
- **DataTransformer**: Schema alignment, CUSIP normalization, NA cleaning, deduplication
- **ParquetLoader**: Append/override modes, date checking, universe creation
- **BondDataPipeline**: Orchestrates all components, CLI interface

**Runs Pipeline:**
- **RunsExtractor**: RUNS Excel reading, Date/Time parsing (from columns, not filename)
- **RunsTransformer**: End-of-day deduplication (vectorized, ~100x faster), CUSIP validation/orphan tracking (vectorized), schema alignment
- **RunsLoader**: Runs parquet writing, Date+Dealer+CUSIP primary key enforcement
- **RunsDataPipeline**: Orchestrates all components, CLI interface

**Pipeline Orchestrator:**
- **run_pipeline.py**: Unified entry point for both bond and runs pipelines
- User can select: Bond only, Runs only, or Both pipelines
- Shared mode selection (append/override) for selected pipelines

---

## Data Pipeline Patterns

### Bond Pipeline Input Data Format
- **Excel files**: Pattern `API MM.DD.YY.xlsx` (e.g., `API 10.20.25.xlsx`)
- **Header row**: Row 3 (index 2, 0-based)
- **Columns**: 75 columns in latest files (schema evolved from 59)
- **NA values**: `#N/A Field Not Applicable`, `#N/A Invalid Security`, etc.

### BQL Workbook Input Data Format
- **Excel file**: Fixed workbook `bql.xlsx` located in Support Files directory
- **Sheet**: `bql`
- **Structure**: Column 0 labelled `CUSIPs`; row 0 contains security names; remaining rows store daily timestamps and numeric spreads
- **Header cleaning**: Strip whitespace, remove trailing `Corp`, enforce 9-character uppercase CUSIP
- **Output expectation**: Long-form DataFrame with columns `Date`, `Name`, `CUSIP`, `Value`

### Runs Pipeline Input Data Format
- **Excel files**: Pattern `RUNS MM.DD.YY.xlsx` (e.g., `RUNS 10.31.25.xlsx`)
- **Header row**: Row 1 (index 0, 0-based)
- **Columns**: 30 columns in latest files (schema evolved from 28 in one old file)
- **Date/Time**: Date and Time come from columns (not filename)
  - Date: MM/DD/YY format (e.g., "10/31/25") → parsed to datetime object
  - Time: HH:MM format (e.g., "15:45") → parsed to datetime.time object
- **NA values**: Same as bond pipeline (`#N/A Field Not Applicable`, `#N/A Invalid Security`, etc.)

### Output Data Format

#### `historical_bond_details.parquet`
- **Purpose**: Time series of all bonds over time
- **Primary Key**: `Date + CUSIP` (unique combination)
- **Schema**: All 75 columns + `Date` column (first position)
- **Row count**: ~25,000+ rows spanning 2023-2025
- **Storage**: `bond_data/parquet/historical_bond_details.parquet`

#### `universe.parquet`
- **Purpose**: Current universe of all unique CUSIPs ever seen
- **Primary Key**: `CUSIP` (unique)
- **Schema**: 13 key columns only
  1. CUSIP, 2. Benchmark Cusip, 3. Custom_Sector, 4. Bloomberg Cusip, 5. Security,
  6. Benchmark, 7. Pricing Date, 8. Pricing Date (Bench), 9. Worst Date,
  10. Yrs (Worst), 11. Ticker, 12. Currency, 13. Equity Ticker
- **Update strategy**: Always rebuild from `historical_bond_details` (most recent date per CUSIP)
- **Storage**: `bond_data/parquet/universe.parquet`

#### `bql.parquet`
- **Purpose**: Long-form Bloomberg query spreads dataset
- **Primary Key**: `Date + CUSIP`
- **Schema**: Columns `Date`, `Name`, `CUSIP`, `Value`
- **Name mapping**: Derived from workbook header row; used for orphan logging
- **Storage**: `bond_data/parquet/bql.parquet`
- **Logging**: Warn when `bql` CUSIPs are missing from `universe.parquet`

#### `runs_timeseries.parquet`
- **Purpose**: Time series of all dealer quotes over time (end-of-day snapshots)
- **Primary Key**: `Date + Dealer + CUSIP` (unique combination, enforced after deduplication)
- **Schema**: All 30 columns with Date and Time as first columns
- **Row count**: ~130,000+ rows (after deduplication) spanning 2022-2025
- **Storage**: `bond_data/parquet/runs_timeseries.parquet`
- **Deduplication**: Keep latest Time per Date+Dealer+CUSIP (end-of-day snapshot)

### Processing Rules

#### Date Handling
- Extract from filename pattern: `API MM.DD.YY.xlsx` → `datetime(YYYY-MM-DD)`
- Store as `datetime64` in parquet (Date column first)
- Append mode: Skip dates already in parquet file
- Override mode: Delete existing parquet files, rebuild from scratch

#### CUSIP Normalization & Validation

**Bond Pipeline:**
- Convert to uppercase: `89678zab2` → `89678ZAB2`
- Remove extra text: ` 06418GAD9 Corp` → `06418GAD9`
- Validate length: Must be exactly 9 characters
- Invalid CUSIPs: Log warning but include in data
- Primary key enforcement: No duplicate Date+CUSIP combinations

**Runs Pipeline:**
- No normalization: Keep CUSIPs as-is from Excel (no uppercase conversion, no text removal)
- Validate length: Check if 9 characters using vectorized operations (but don't normalize)
- **Optimized**: Vectorized CUSIP validation (~100x faster than row-by-row iteration)
- Invalid CUSIPs: Log summary warning but include in data (same as bond pipeline)
- **Enhanced Orphan Tracking**: Log orphans with context (Security, Date, Dealer, Time, Ticker) to validation.log
- Primary key enforcement: No duplicate Date+Dealer+CUSIP combinations (after deduplication)

#### Schema Evolution
- **Master schema**: 75 columns from latest files (set dynamically)
- **Older files**: Fill missing columns with `NULL/NaN`
- **Column order**: Preserve order from master schema
- **Date column**: Always first column in all DataFrames

#### Deduplication

**Bond Pipeline:**
- Within file: Keep last occurrence of duplicate CUSIPs
- Across files: One record per Date+CUSIP combination
- Log all duplicates to `bond_data/logs/duplicates.log`

**Runs Pipeline:**
- End-of-day snapshots: Keep latest Time per Date+Dealer+CUSIP (most recent quote of day)
- Same-time tiebreaker: If multiple rows have same latest Time, keep last row by position
- **Optimized**: Uses vectorized sort + drop_duplicates (O(n log n)) instead of row-by-row iteration (O(n*groups))
- Within file and across files: One record per Date+Dealer+CUSIP combination
- Log summary of duplicates to `bond_data/logs/duplicates.log` (combined with bond pipeline logs)

#### Data Cleaning
- Convert NA values to NULL: `#N/A Field Not Applicable`, `#N/A Invalid Security`, `N/A`, etc.
- Empty cells → NULL/NaN
- Preserve all other values as-is

#### BQL Pipeline
- Normalize header row to derive security names and enforce 9-character CUSIPs
- Reshape wide workbook to long-form dataset (`Date`, `Name`, `CUSIP`, `Value`)
- Skip duplicate/invalid CUSIP columns and log issues for traceability
- Drop rows with non-numeric values before persisting
- Compare resulting CUSIPs against `universe.parquet` and log orphan details (CUSIP + Name)

---

## Logging Standards

### Log File Organization
```
bond_data/logs/
├── processing.log      # File extraction and loading operations
├── duplicates.log      # All duplicate CUSIPs detected
├── validation.log      # CUSIP validation warnings
└── summary.log         # Pipeline execution summary (run headers, stats)
```

### Logging Levels
- **DEBUG**: Detailed diagnostic info (file-by-file progress)
- **INFO**: General informational messages
- **WARNING**: CUSIP validation issues, schema mismatches
- **ERROR**: File read failures, data corruption
- **CRITICAL**: System failures

### Logging Best Practices
1. **Dual logging**: File logging (detailed) + Console logging (essential only)
2. **No console spam**: Suppress detailed logs on console (console_level=logging.CRITICAL)
3. **Run metadata**: Track run_id, timestamp, mode for each execution
4. **Log rotation**: Archive logs after N runs (currently 10)
5. **Structured info**: Include Date, CUSIP, file path in log messages
6. **BQL ingestion**: Log unique CUSIP/date counts and orphan CUSIPs with security names

### Example Logging Pattern
```python
# Setup logger (file only, no console)
self.logger = setup_logging(LOG_FILE_PROCESSING, 'extract', console_level=logging.CRITICAL)

# Log file processing
self.logger.info(f"Processing file: {filename}")
self.logger.info(f"Extracted {len(df)} rows for date {date_obj}")

# Log validation issues
self.logger.warning(f"Invalid CUSIP: {cusip} (length {len(cusip)})")

# Log duplicates
self.logger_dupes.info(f"Duplicate Date+CUSIP: {date} - {cusip} - {bond_name} (kept last occurrence)")
```

---

## Testing Standards

### Test Structure
```
tests/
├── conftest.py              # Shared fixtures (sample data, mocks)
├── unit/                    # Unit tests
│   ├── test_utils.py       # ✅ Bond utility & BQL helper tests
│   ├── test_extract.py     # TODO
│   ├── test_transform.py   # TODO
│   └── test_load.py        # TODO
└── integration/             # Integration tests
    └── test_pipeline.py    # TODO
```

### Testing Requirements
1. **Coverage**: Target 85%+ overall, 90%+ for critical modules
2. **Test location**: All tests in `tests/` directories within each module
3. **Naming**: Test files follow naming convention: `test_*.py`
4. **Framework**: Use pytest for testing
5. **Fixtures**: Reusable test data in `conftest.py`
6. **Isolation**: Tests must be independent (no shared state)
7. **Mocking**: Mock all file I/O, external dependencies
8. **Speed**: Full test suite should run in < 30 seconds

### Test Categories
- **Date parsing**: Valid/invalid formats, leap years, edge cases
- **CUSIP validation**: Valid/invalid lengths, characters, normalization
- **NA cleaning**: Standard patterns, mixed DataFrames
- **Schema alignment**: Old (59) vs new (75) column schemas
- **Deduplication**: Within file, across files, Date+CUSIP combinations
- **Append mode**: Skip existing dates, append new dates
- **Override mode**: Delete existing, rebuild from scratch
- **BQL ingestion**: Header normalization, wide→long reshape, orphan logging against universe

### Running Tests
```bash
# Install dependencies (first run or after updates)
poetry install

# Run all tests
poetry run pytest

# Run specific file
poetry run pytest tests/unit/test_utils.py

# Run with coverage
poetry run pytest --cov=bond_pipeline --cov-report=html

# Run specific test
poetry run pytest tests/unit/test_utils.py::TestCUSIPValidation::test_validate_cusip_valid
```

---

## File Organization Rules

### Directory Structure
```
Bond-RV-App/
├── bond_pipeline/          # Pipeline code (7 modules)
├── bond_data/              # Data directory (local only, git-ignored)
│   ├── parquet/           # Output parquet files
│   └── logs/              # Processing logs
├── .venv/                 # Poetry-managed virtual environment (git-ignored)
├── Documentation/          # Complete documentation
│   ├── Setup/             # Getting started guides
│   ├── Workflows/         # Step-by-step procedures
│   └── Architecture/      # Design documentation
├── Raw Data/              # Excel files (input)
├── References and Documentation/  # Ignored by Cursor
├── tests/                 # Test suite
├── utils/                 # Utility scripts
├── .cursorrules           # This file
├── .cursorignore          # Files to ignore
├── pyproject.toml         # Poetry project configuration
├── poetry.lock            # Locked dependency versions
├── requirements.txt       # Legacy dependency snapshot (reference only)
├── README.md              # Project overview
└── run_pipeline.py        # Simple pipeline runner
```

### File Naming Conventions
- **Modules**: `snake_case.py` (e.g., `bond_pipeline/utils.py`)
- **Test files**: `test_*.py` (e.g., `tests/unit/test_utils.py`)
- **Config files**: `.cursorrules`, `.cursorignore`, `pytest.ini`
- **Data files**: `historical_bond_details.parquet`, `universe.parquet`

### Important Paths
- **Project root**: `PROJECT_ROOT = Path(__file__).parent.parent` (from config.py)
- **Data directory**: `bond_data/` (git-ignored)
- **Parquet files**: `bond_data/parquet/` (git-tracked)
- **Logs**: `bond_data/logs/` (git-ignored)
- **Default input**: Dropbox folder (Windows-specific path in config.py)

### Path Handling Rules
- **Use `pathlib.Path`** for all path operations
- **Scripts must detect their location** and adjust paths accordingly
- **Support running from multiple directory locations** (absolute and relative paths)
- **Avoid hardcoded paths**: Use relative paths with config-based resolution

---

## Git & Version Control

### Git-Ignored Files
- Excel files: `*.xlsx`, `*.xls`
- Log files: `bond_data/logs/*.log`
- Virtual environment: `.venv/`
- Python cache: `__pycache__/`, `*.pyc`
- Test output: `tests/test_output/`
- References folder: `References and Documentation/` (in .cursorignore)

### Git-Tracked Files
- Parquet files: `bond_data/parquet/*.parquet` (processed data)
- Source code: `bond_pipeline/`, `utils/`
- Tests: `tests/`
- Documentation: `Documentation/`
- Config: `pyproject.toml`, `poetry.lock`, `requirements.txt`, `.cursorrules`, `.cursorignore`
- Scripts: `run_pipeline.py`

### Git Commit Protocol
- **Update `.cursorrules`** before every git commit
- **Verify file timestamps** and version consistency
- **Document changes** in commit messages with reference to `.cursorrules` updates

### Unicode & Encoding
- **Avoid unicode encoding issues** for Windows
- **Use ASCII-compatible characters** when needed
- **Test paths** on Windows file system before committing

---

## Code Modification Guidelines

### Before Making Changes
1. **Analyze entire codebase**: Understand dependencies and relationships
2. **Map file impacts**: Identify ALL files that need modification
3. **Check existing logic**: Extend or refactor before creating new code
4. **Consider ripple effects**: Changes impact downstream modules

### Coding Principles
1. **Extend existing logic**: Never create new files when existing code can be extended
2. **Refactor first**: Improve existing structure before adding new features
3. **Minimal changes**: Make smallest change that achieves the goal
4. **Backward compatibility**: Don't break existing functionality
5. **Documentation**: Update docstrings when modifying functions

### File Creation Rules
- **List new files**: Announce file name and path before creating
- **Test files**: Always create in `tests/` folder when applicable
- **Avoid duplication**: Check if existing modules can handle the requirement
- **Justification**: Explain why new file is needed vs extending existing
- **Clean up temporary files**: Remove any migration/restore/cleanup scripts after use

### When AI Assistant Helps
1. **Read existing code**: Always read relevant files before suggesting changes
2. **Show context**: Reference existing code with `startLine:endLine:filepath` format
3. **Propose edits**: Suggest specific changes with line-by-line diffs
4. **Test changes**: Run relevant tests after modifications
5. **Check lints**: Fix any linter errors introduced
6. **Clean up temporary files**: Remove all temporary files, backups, and migration scripts before completing the task

### Example Code Modification
**Before**: Adding a new validation function
```python
# 1. Read existing utils.py to understand patterns
# 2. Check if similar functions exist
# 3. Extend existing module rather than create new file
# 4. Add function following existing naming/documentation conventions
# 5. Add tests in tests/unit/test_utils.py
# 6. Update module docstring if needed
```

---

## Execution & Workflow

### Running the Pipeline
```bash
# 1. Optionally enter Poetry shell
poetry shell

# 2. Run automated pipeline runner
poetry run python run_pipeline.py
# Prompts: 1) Pipeline selection, 2) Mode (override/append), 3) (Bond) Include BQL workbook

# 3. Or use direct CLI (Bond Pipeline)
cd bond_pipeline
poetry run python pipeline.py -i "../Raw Data/" -m append --process-bql
poetry run python pipeline.py -i "../Raw Data/" -m override --process-bql

# 4. Or use Python module (Bond Pipeline)
poetry run python -m bond_pipeline.pipeline -i "Raw Data/" -m append

# 5. Runs Pipeline CLI
poetry run python -m runs_pipeline.pipeline -i "Historical Runs/" -m append
poetry run python -m runs_pipeline.pipeline -i "Historical Runs/" -m override

# 6. Unified Pipeline Orchestrator (Recommended)
poetry run python run_pipeline.py
# Prompts: Select pipeline(s), choose mode, decide on BQL ingestion when running Bond pipeline
```

### Pipeline Modes

#### Append Mode (Default, Daily Use)
- **Use case**: Add new Excel files to existing parquet tables
- **Behavior**: Skip dates already in parquet, append only new dates
- **Output**: Updated `historical_bond_details.parquet` (with new dates appended)
- **Universe**: Rebuild `universe.parquet` from complete historical data

#### Override Mode (Rebuild Everything)
- **Use case**: First-time setup, schema changes, data corruption recovery
- **Behavior**: Delete existing parquet files, process all Excel files from scratch
- **Output**: New `historical_bond_details.parquet`, new `universe.parquet`
- **Speed**: Slower but ensures clean data

### Checking Results
```bash
# View output parquet files
ls bond_data/parquet/

# Check logs
cat bond_data/logs/summary.log       # Pipeline execution summary
cat bond_data/logs/processing.log    # File-by-file details
cat bond_data/logs/duplicates.log    # Duplicate CUSIPs
cat bond_data/logs/validation.log    # CUSIP validation issues

# Quick stats in Python
import pandas as pd
df = pd.read_parquet('bond_data/parquet/historical_bond_details.parquet')
print(f"Rows: {len(df)}, Columns: {len(df.columns)}, Date range: {df['Date'].min()} to {df['Date'].max()}")

# Runs pipeline stats
df_runs = pd.read_parquet('bond_data/parquet/runs_timeseries.parquet')
print(f"Rows: {len(df_runs)}, Columns: {len(df_runs.columns)}")
print(f"Date range: {df_runs['Date'].min()} to {df_runs['Date'].max()}")
print(f"Unique CUSIPs: {df_runs['CUSIP'].nunique()}, Unique Dealers: {df_runs['Dealer'].nunique()}")

# BQL spreads stats
df_bql = pd.read_parquet('bond_data/parquet/bql.parquet')
print(f"BQL rows: {len(df_bql)}, Unique CUSIPs: {df_bql['CUSIP'].nunique()}, Unique Dates: {df_bql['Date'].nunique()}")
```

---

## Common Patterns & Examples

### Excel File Pattern Matching
```python
import re
from config import FILE_PATTERN

filename = "API 10.20.25.xlsx"
match = re.match(FILE_PATTERN, filename)
# Returns: datetime(2025, 10, 20)
```

### CUSIP Normalization
```python
from utils import normalize_cusip

raw = "89678zab2"  # lowercase
normalized = normalize_cusip(raw)  # Returns: "89678ZAB2"

raw = " 06418GAD9 Corp"  # extra text
normalized = normalize_cusip(raw)  # Returns: "06418GAD9"
```

### Schema Alignment
```python
from utils import align_to_master_schema

old_schema_df = read_excel(old_file)  # 59 columns
master_schema = [... 75 columns ...]
aligned_df = align_to_master_schema(old_schema_df, master_schema)  # 75 columns with NULL in new cols
```

### Deduplication
```python
from transform import DataTransformer

transformer = DataTransformer(log_dupes, log_valid)
df_cleaned = transformer.deduplicate(df, bond_date)

# Within same file: keep last occurrence
# Across files: Date+CUSIP primary key enforcement
```

### Logging Pattern
```python
from pathlib import Path
import logging
from utils import setup_logging

log_file = Path("bond_data/logs/processing.log")
logger = setup_logging(log_file, 'module_name', console_level=logging.CRITICAL)

logger.info("Processing file: API 10.20.25.xlsx")
logger.warning("Invalid CUSIP: 1234567890 (length 10)")
logger.error("Failed to read file: missing_header.xlsx")
```

### Reading/Writing Parquet
```python
import pandas as pd
from config import HISTORICAL_PARQUET, DATE_COLUMN

# Read existing dates (for append mode)
df = pd.read_parquet(HISTORICAL_PARQUET, columns=[DATE_COLUMN])
existing_dates = set(df[DATE_COLUMN].unique())

# Write new data (append mode)
df_new.to_parquet(HISTORICAL_PARQUET, mode='append', index=False)

# Write new data (override mode)
df_new.to_parquet(HISTORICAL_PARQUET, mode='overwrite', index=False)
```

---

## Troubleshooting Common Issues

### Import Errors
```bash
# Make sure you're in project root and virtual environment is activated
cd Bond-RV-App
poetry shell

# Use module syntax for imports
python -m bond_pipeline.pipeline
```

### Virtual Environment Issues
```bash
# Reinstall dependencies
poetry install

# Spawn a Poetry-managed shell if needed
poetry shell
```

### Test Discovery Issues
```bash
# Run from project root
cd Bond-RV-App
pytest tests/

# Or specific file
pytest tests/unit/test_utils.py -v
```

### Parquet File Locking
```bash
# Close any open Python sessions reading parquet files
# Windows: Check Task Manager for Python processes
```

---

## Key Takeaways for AI Assistant

### Essential Rules
1. **Always analyze entire codebase** before making changes
2. **Execute all code in current virtual environment** (Bond-RV-App)
3. **Create tests in tests/ folder** when applicable
4. **Never create new files** unless explicitly required by user
5. **List file names and paths** when creating new files
6. **Include .cursorrules for context** in planning
7. **Map dependencies** before suggesting edits
8. **Extend existing logic** rather than duplicating
9. **Follow modular architecture**: config → utils → extract → transform → load → pipeline
10. **Respect logging patterns**: file logging (detailed) + console (minimal)

### Workflow Rules
11. **Think deeply** about prompt applicability before acting
12. **Use judgment** when interpreting conflicting requirements
13. **Notify user** when ignoring requirements or using judgment
14. **Ask clarifying questions** until all ambiguity is eliminated
15. **Provide file change summary** after completing requests
16. **ALWAYS clean up temporary files** created during chat sessions
17. **Always update .cursorrules** after applicable changes
18. **Include date/time stamps** in all documentation
19. **Update .cursorrules before git commits**

### Quality Standards
19. **Use pathlib.Path** for all path operations
20. **Avoid unicode issues** on Windows
21. **Modular design** following industry best practices
22. **Keep documentation current** when creating new docs

---

**File Created**: January 2025  
**Last Major Update**: 2025-11-08 11:30 ET  
**Project**: Bond RV App - Data Pipeline  
**Version**: 1.0

---

**Change Log**:
- **2025-01**: Initial comprehensive `.cursorrules` file created
- **2025-01**: Added AI Assistant Core Workflow section
- **2025-01**: Added git commit protocol and unicode handling
- **2025-01**: Enhanced documentation timestamp requirements
- **2025-01**: Added path handling and encoding rules
- **2025-01**: Removed poetry environment references, switched to standard pip/virtual environment
- **2025-01**: Removed all Outlook email pipeline functionality (monitor_outlook.py, runs_miner.py, utils/outlook_monitor.py, related documentation, and pywin32 dependency)
- **2025-01**: Added runs_pipeline module documentation:
  - Added runs_pipeline/ to module structure section
  - Added runs_timeseries.parquet to output data format section
  - Documented RUNS file pattern, header row, primary key (Date+Dealer+CUSIP)
  - Documented end-of-day snapshot deduplication logic (latest Time per Date+Dealer+CUSIP)
  - Documented CUSIP orphan tracking requirements (compare with universe.parquet)
  - Added runs_pipeline execution examples to workflow section
  - Updated class-based architecture section with runs pipeline classes
- **2025-01-02**: Performance optimizations:
  - Documented vectorized deduplication (~100x faster, O(n log n) vs O(n*groups))
  - Documented vectorized CUSIP validation (~100x faster, vectorized string operations)
  - Updated logging patterns to reflect summary logging instead of row-by-row
- **2025-01-02**: Enhanced orphan CUSIP logging:
  - Documented enhanced logging with context (Security, Date, Dealer, Time, Ticker)
  - Updated orphan tracking section to reflect detailed context in logs
- **2025-01-02**: Added unified pipeline orchestrator (run_pipeline.py):
  - Documented run_pipeline.py as recommended entry point
  - Added user selection for pipeline(s) and mode
- **2025-11-07 15:30 ET**: Added BQL workbook ingestion pipeline:
  - Documented `bql.xlsx` processing and `bql.parquet` output
  - Updated CLI/orchestrator guidance with `--process-bql` flag and prompt
  - Captured orphan logging requirements (CUSIP + Name) for BQL dataset
  - Added testing coverage notes for BQL helpers in `tests/unit/test_utils.py`
- **2025-11-07 16:20 ET**: Created/activated standard `Bond-RV-App` virtual environment and installed dependencies from `requirements.txt`
- **2025-11-07 16:45 ET**: Migrated to Poetry-based workflow (`pyproject.toml`, `poetry.lock`, in-project `.venv`), updated execution/testing instructions, and removed legacy `Bond-RV-App/` virtualenv
- **2025-11-08 11:05 ET**: Added `ipykernel` dev dependency and registered `Bond RV App (.venv)` Jupyter kernel for notebook execution
- **2025-11-08 11:20 ET**: Added `nbconvert` dev dependency to support automated notebook execution for testing kernels
- **2025-11-08 11:30 ET**: Added VS Code workspace settings to pin notebooks to the Poetry-managed `.venv` interpreter
- **2025-11-08 11:35 ET**: Added `notebook` dev dependency to ensure VS Code can start a local Jupyter server from the Poetry environment

