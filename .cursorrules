# Bond RV App - Cursor AI Rules

**Last Updated**: 2025-01-17 20:00 ET - Updated documentation to reflect actual codebase structure: consolidated all individual pair analytics scripts into single comb.py file (11 analysis functions), removed references to individual CSV outputs (now consolidated into comb.xlsx), documented empty utils/ directory, added tests/patterns/ directory documentation, cleaned up outdated file references  
**Purpose**: Guide AI assistant in maintaining and extending bond data processing system

> **IMPORTANT**: Always update `.cursorrules` after every applicable change. Override where necessary, delete or change sections so that this file always reflects the most up-to-date project state based on actual codebase. Include date and time stamps for all major changes.

---

## AI Assistant Core Workflow

### General AI Behavior
- **Think deeply** about prompt applicability given end goal and current context
- **Use judgment** when interpreting prompts (e.g., if algo trading prompt mentions volume but raw data only has price, ignore volume requirement)
- **Always notify** user when ignoring requirements or using judgment in interpretation
- **Always ask clarifying questions** in planning mode; keep asking until all possible questions covering virtually any ambiguity are answered
- **Execute all project commands via** `poetry run ...` **(or inside an active** `poetry shell` **) to ensure the Poetry environment is used**

### File Change Summary
- **After finishing requests** that add/edit files, show summary of:
  - What was changed in each file
  - File paths affected
- **Track all modifications** for user review

### Temporary File Cleanup Protocol
- **ALWAYS clean up temporary files** created during chat sessions
- **Before completing tasks**, remove any temporary files created:
  - Migration scripts (`*_migrate*.py`)
  - Restore scripts (`*_restore*.py`)
  - Temporary test files (`*_temp*.py`)
  - Backup files (`.backup`, `*_backup*`)
- **List deleted files** in the final summary
- **Clean up at the END** of each chat session that creates temporary files
- **Common temporary files to remove**:
  - Scripts in `utils/` created for one-time operations
  - Backup parquet files in `bond_data/parquet/`
  - Test output files
  - Temporary data files

### Planning Mode Protocol
1. Ask clarifying questions first
2. Continue asking until ambiguity is eliminated
3. Present plan for approval
4. Execute after confirmation

---

## Project Overview

This is a **modular data engineering pipeline** for processing bond data for relative value trading applications.

### Core Purpose
- Process Excel bond data files into optimized Parquet time-series tables
- Support incremental data loading for daily updates

### Architecture
- **Modular ETL**: Separate modules for Extract → Transform → Load
- **Data Pipeline**: Excel + BQL workbooks → Parquet tables (historical_bond_details, universe, bql)
- **Incremental Loading**: Append mode (new dates only) vs Override mode (rebuild all)
- **Data Integrity**: Primary key enforcement, deduplication, validation

### Industry Best Practices
- **Modular design**: Project must be set up in modular way with correct folder patterns according to industry best practices
- **Single responsibility**: Each module has clear, focused purpose
- **Separation of concerns**: Clear boundaries between extraction, transformation, and loading
- **Configuration management**: All paths, constants, and settings centralized

---

## Technology Stack

### Language & Version
- **Python 3.11+** (strict requirement)
- **Type hints required** for all function signatures
- **Docstrings required** for all modules, classes, and functions

### Key Dependencies
- `pandas>=2.0.0` - Data manipulation
- `pyarrow>=12.0.0` - Parquet I/O
- `openpyxl>=3.1.0` - Excel reading
- `ipykernel>=7.1.0` (dev) - Jupyter kernel integration for notebooks (added 2025-11-08 11:05 ET)
- `nbconvert>=7.16.6` (dev) - Automated notebook execution (added 2025-11-08 11:20 ET)
- `notebook>=7.4.7` (dev) - Local Jupyter server for VS Code kernel selection (added 2025-11-08 11:35 ET)
- `pytest>=7.0.0` - Testing framework

### Environment Management
- **Environment manager**: Poetry (`pyproject.toml`, `poetry.lock`)
- **Virtual environment location**: `.venv/` (created via `poetry config virtualenvs.in-project true`)
- **ALWAYS execute code with** `poetry run ...` or inside `poetry shell`
- **Install dependencies** via `poetry install`

---

## Code Style & Conventions

### Formatting
- **Line length**: 88 characters maximum
- **Indentation**: 4 spaces (no tabs)
- **Imports**: Standard library → Third-party → Local imports (blank lines between groups)
- **Naming**: snake_case for functions/variables, PascalCase for classes, UPPER_CASE for constants

### Documentation
- **Module docstrings**: First line is summary, followed by detailed description if needed
- **Function docstrings**: Use Google style with Args/Returns/Raises sections
- **Type hints**: Use modern Python typing (e.g., `Optional[str]`, `List[pd.DataFrame]`, `Tuple[str, int]`)
- **Date and time stamps**: Include date/time stamps in all documentation entries
- **Keep documentation current**: When creating new documentation, figure out if any old documentation needs updating
- **Version tracking**: Update existing docs when adding new information

### Example Function Signature
```python
def normalize_cusip(cusip: str) -> Optional[str]:
    """
    Normalize CUSIP to uppercase 9-character format.
    
    Args:
        cusip: Raw CUSIP string (may contain lowercase, spaces, extra text)
    
    Returns:
        Normalized CUSIP or None if invalid
    """
```

---

## Code Organization

### Module Structure
```
bond_pipeline/
├── config.py        # Configuration, paths, constants (includes RUNS config)
├── utils.py         # Helper functions (date parsing, validation, logging, runs utilities)
├── extract.py       # Excel file reading and date extraction
├── transform.py     # Data cleaning, normalization, deduplication
├── load.py          # Parquet writing (append/override modes)
├── pipeline.py      # Main orchestration script with CLI
└── README.md        # Module documentation

runs_pipeline/
├── __init__.py      # Package exports
├── extract.py       # RUNS Excel file reading and Date/Time parsing
├── transform.py     # End-of-day deduplication, CUSIP validation/orphan tracking
├── load.py          # Runs parquet writing (append/override modes)
└── pipeline.py      # Main orchestration script with CLI

portfolio_pipeline/
├── __init__.py      # Package exports
├── extract.py       # Portfolio Excel file reading and date extraction from filename
├── transform.py     # CUSIP normalization, NA cleaning, deduplication (Date+CUSIP+ACCOUNT+PORTFOLIO)
├── load.py          # Portfolio parquet writing (append/override modes)
└── pipeline.py      # Main orchestration script with CLI
```

### Design Principles
1. **Single Responsibility**: Each module has one clear purpose
2. **Dependency Injection**: Pass loggers, configs as constructor arguments
3. **Separation of Concerns**: Extract (read) → Transform (clean) → Load (write)
4. **Configuration Centralization**: All paths and constants in `config.py`
5. **Immutable Config**: Don't modify config values at runtime

### Class-Based Architecture
Each pipeline stage is a class with clear responsibilities:

**Bond Pipeline:**
- **ExcelExtractor**: File discovery, Excel reading, date extraction
- **DataTransformer**: Schema alignment, CUSIP normalization, NA cleaning, deduplication
- **ParquetLoader**: Append/override modes, date checking, universe creation
- **BondDataPipeline**: Orchestrates all components, CLI interface

**Runs Pipeline:**
- **RunsExtractor**: RUNS Excel reading, Date/Time parsing (from columns, not filename)
- **RunsTransformer**: End-of-day deduplication (vectorized, ~100x faster), CUSIP validation/orphan tracking (vectorized), schema alignment
– **RunsLoader**: Runs parquet writing, Date+Dealer+CUSIP primary key enforcement, granular load metrics (new rows/dates/CUSIPs/Dealers)
– **RunsDataPipeline**: Orchestrates all components, CLI interface, append-mode file selection + enhanced run summaries

**Portfolio Pipeline:**
- **PortfolioExtractor**: Portfolio Excel reading, date extraction from filename (Aggies MM.DD.YY.xlsx pattern), removes Unnamed columns
- **PortfolioTransformer**: CUSIP normalization (same as bond pipeline), row filtering (drops rows with blank SECURITY or CUSIP), NA cleaning, deduplication (Date+CUSIP+ACCOUNT+PORTFOLIO)
- **PortfolioLoader**: Portfolio parquet writing, Date+CUSIP+ACCOUNT+PORTFOLIO primary key enforcement, append/override modes
- **PortfolioDataPipeline**: Orchestrates all components, CLI interface, enhanced run summaries

**Pipeline Orchestrator:**
- **run_pipeline.py**: Unified entry point for bond, runs, and portfolio pipelines
- User can select: Bond only, Runs only, Portfolio only, Both Bond and Runs, or Individual Parquet Files
- Individual Parquet Files option allows regenerating specific parquet outputs:
  - `historical_bond_details.parquet` (also regenerates universe)
  - `bql.parquet` (standalone BQL processing)
  - `runs_timeseries.parquet` (standalone runs processing)
  - `historical_portfolio.parquet` (standalone portfolio processing)
- Shared mode selection (append/override) for selected pipelines
- After successful runs, generates `bond_data/logs/parquet_stats.log` with dataset diagnostics (df.info/head/tail)

---

## Data Pipeline Patterns

### Bond Pipeline Input Data Format
- **Excel files**: Pattern `API MM.DD.YY.xlsx` (e.g., `API 10.20.25.xlsx`)
- **Header row**: Row 3 (index 2, 0-based)
- **Columns**: 75 columns in latest files (schema evolved from 59)
- **NA values**: `#N/A Field Not Applicable`, `#N/A Invalid Security`, etc.

### BQL Workbook Input Data Format
- **Excel file**: Fixed workbook `bql.xlsx` located in Support Files directory
- **Sheet**: `bql`
- **Multi-index header**: 4-row column header structure:
  - Row 0 (index 0): Ignored
  - Row 1 (index 1): Security names (1st level)
  - Row 2 (index 2): CUSIPs (2nd level)
  - Row 3 (index 3): Ignored
  - Row 4 (index 4): Data rows start here
- **Column 0**: First column labelled `CUSIPs` contains daily timestamps
- **Header processing**: Extract names from level 1 (row 1) and CUSIPs from level 2 (row 2) of multi-index columns
- **CUSIP cleaning**: Strip whitespace, remove trailing `Corp`, enforce 9-character uppercase CUSIP
- **Output expectation**: Long-form DataFrame with columns `Date`, `Name`, `CUSIP`, `Value`

### Runs Pipeline Input Data Format
- **Excel files**: Pattern `RUNS MM.DD.YY.xlsx` (e.g., `RUNS 10.31.25.xlsx`)
- **Header row**: Row 1 (index 0, 0-based)
- **Columns**: 30 columns in latest files (schema evolved from 28 in one old file)
- **Date/Time**: Date and Time come from columns (not filename)
  - Date: MM/DD/YY format (e.g., "10/31/25") → parsed to datetime object
  - Time: HH:MM format (e.g., "15:45") → parsed to datetime.time object
- **NA values**: Same as bond pipeline (`#N/A Field Not Applicable`, `#N/A Invalid Security`, etc.)

### Portfolio Pipeline Input Data Format
- **Excel files**: Pattern `Aggies MM.DD.YY.xlsx` (e.g., `Aggies 11.04.25.xlsx`)
- **Header row**: Row 1 (index 0, 0-based)
- **Columns**: 82 columns (schema detected dynamically from latest file)
- **Date extraction**: Date extracted from filename (MM/DD/YYYY format)
- **Column filtering**: Unnamed columns (Unnamed: 0, Unnamed: 1, etc.) are automatically removed during extraction
- **Row filtering**: Rows with blank SECURITY or CUSIP are dropped during transformation
- **NA values**: Same as bond pipeline (`#N/A Field Not Applicable`, `#N/A Invalid Security`, etc.)

### Output Data Format

#### `historical_bond_details.parquet`
- **Purpose**: Time series of all bonds over time
- **Primary Key**: `Date + CUSIP` (unique combination)
- **Schema**: All 75 columns + `Date` column (first position)
- **Row count**: ~25,000+ rows spanning 2023-2025
- **Storage**: `bond_data/parquet/historical_bond_details.parquet`

#### `universe.parquet`
- **Purpose**: Current universe of all unique CUSIPs ever seen
- **Primary Key**: `CUSIP` (unique)
- **Schema**: 13 key columns only
  1. CUSIP, 2. Benchmark Cusip, 3. Custom_Sector, 4. Bloomberg Cusip, 5. Security,
  6. Benchmark, 7. Pricing Date, 8. Pricing Date (Bench), 9. Worst Date,
  10. Yrs (Worst), 11. Ticker, 12. Currency, 13. Equity Ticker
- **Update strategy**: Always rebuild from `historical_bond_details` (most recent date per CUSIP)
- **Storage**: `bond_data/parquet/universe.parquet`

#### `bql.parquet`
- **Purpose**: Long-form Bloomberg query spreads dataset
- **Primary Key**: `Date + CUSIP`
- **Schema**: Columns `Date`, `Name`, `CUSIP`, `Value`
- **Name mapping**: Derived from workbook header row; used for orphan logging
- **Storage**: `bond_data/parquet/bql.parquet`
- **Logging**: Warn when `bql` CUSIPs are missing from `universe.parquet`

#### `runs_timeseries.parquet`
- **Purpose**: Time series of dealer quotes over time (end-of-day snapshots)
- **Primary Key**: `Date + Dealer + CUSIP` (unique combination, enforced after deduplication)
- **Schema**: All 30 columns with Date and Time as first columns
- **Dealer Filtering**: Only includes dealers: BMO, BNS, NBF, RBC, TD (other dealers filtered out during load)
- **Data Quality**: Negative Bid Spread and Ask Spread values are filtered out (set to NaN) before writing to parquet
- **Row count**: ~19,000+ rows (after deduplication and dealer filtering) spanning 2022-2025
- **Storage**: `bond_data/parquet/runs_timeseries.parquet`
- **Deduplication**: Keep latest Time per Date+Dealer+CUSIP (end-of-day snapshot)

#### `historical_portfolio.parquet`
- **Purpose**: Time series of portfolio holdings over time
- **Primary Key**: `Date + CUSIP + ACCOUNT + PORTFOLIO` (unique combination, preserves account/portfolio detail)
- **Schema**: All 82 columns (excluding Unnamed columns) with Date column first
- **Column filtering**: Unnamed columns (Unnamed: 0, Unnamed: 1, etc.) are automatically removed
- **Row filtering**: Rows with blank SECURITY or CUSIP are automatically dropped
- **Row count**: Varies based on portfolio holdings across dates
- **Storage**: `bond_data/parquet/historical_portfolio.parquet`
- **Deduplication**: Keep last occurrence per Date+CUSIP+ACCOUNT+PORTFOLIO

### Processing Rules

#### Date Handling
- Extract from filename pattern: `API MM.DD.YY.xlsx` → `datetime(YYYY-MM-DD)`
- Store as `datetime64` in parquet (Date column first)
- Append mode: Skip dates already in parquet file
- Override mode: Delete existing parquet files, rebuild from scratch

#### CUSIP Normalization & Validation

**Bond Pipeline:**
- Convert to uppercase: `89678zab2` → `89678ZAB2`
- Remove extra text: ` 06418GAD9 Corp` → `06418GAD9`
- Validate length: Must be exactly 9 characters
- Invalid CUSIPs: Log warning but include in data
- Primary key enforcement: No duplicate Date+CUSIP combinations

**Runs Pipeline:**
- No normalization: Keep CUSIPs as-is from Excel (no uppercase conversion, no text removal)
- Validate length: Check if 9 characters using vectorized operations (but don't normalize)
- **Optimized**: Vectorized CUSIP validation (~100x faster than row-by-row iteration)
- Invalid CUSIPs: Log summary warning but include in data (same as bond pipeline)
- **Enhanced Orphan Tracking**: Log orphans with context (Security, Date, Dealer, Time, Ticker) to validation.log
- **Dealer Filtering**: Only dealers BMO, BNS, NBF, RBC, TD are stored in `runs_timeseries.parquet` (other dealers filtered out during load)
- **Negative Spread Filtering**: Negative Bid Spread and Ask Spread values are filtered out (set to NaN) before writing to parquet and during aggregation (data quality issue)
- Primary key enforcement: No duplicate Date+Dealer+CUSIP combinations (after deduplication)
- Append mode inspects existing parquet dates, skips fully-loaded files, and filters mixed files down to unseen dates before extraction.
- Pipeline summaries surface files processed/skipped plus new rows, dates, CUSIPs, and dealers added during each run.

**Portfolio Pipeline:**
- CUSIP normalization: Same as bond pipeline (uppercase, remove extra text, validate 9 characters)
- Row filtering: Drop rows where either SECURITY or CUSIP is blank/NaN/empty (done first in transformation)
- Column filtering: Remove all "Unnamed:" columns (Unnamed: 0, Unnamed: 1, etc.) during extraction
- Invalid CUSIPs: Log warning but include in data (same as bond pipeline)
- Primary key enforcement: No duplicate Date+CUSIP+ACCOUNT+PORTFOLIO combinations
- Schema evolution: Master schema (82 columns) detected dynamically from latest file
- Append mode: Skip dates already in parquet file
- Override mode: Delete existing parquet file, rebuild from scratch

#### Schema Evolution
- **Master schema**: 75 columns from latest files (set dynamically)
- **Older files**: Fill missing columns with `NULL/NaN`
- **Column order**: Preserve order from master schema
- **Date column**: Always first column in all DataFrames

#### Deduplication

**Bond Pipeline:**
- Within file: Keep last occurrence of duplicate CUSIPs
- Across files: One record per Date+CUSIP combination
- Log all duplicates to `bond_data/logs/duplicates.log`

**Runs Pipeline:**
- End-of-day snapshots: Keep latest Time per Date+Dealer+CUSIP (most recent quote of day)
- Same-time tiebreaker: If multiple rows have same latest Time, keep last row by position
- **Optimized**: Uses vectorized sort + drop_duplicates (O(n log n)) instead of row-by-row iteration (O(n*groups))
- Within file and across files: One record per Date+Dealer+CUSIP combination
- Log summary of duplicates to `bond_data/logs/duplicates.log` (combined with bond pipeline logs)

**Portfolio Pipeline:**
- Within file: Keep last occurrence of duplicate Date+CUSIP+ACCOUNT+PORTFOLIO combinations
- Across files: One record per Date+CUSIP+ACCOUNT+PORTFOLIO combination
- Log duplicates to `bond_data/logs/duplicates.log` (combined with bond and runs pipeline logs)

#### Data Cleaning
- Convert NA values to NULL: `#N/A Field Not Applicable`, `#N/A Invalid Security`, `N/A`, etc.
- Empty cells → NULL/NaN
- **Negative Spread Filtering**: Negative Bid Spread and Ask Spread values are filtered out (set to NaN) in runs pipeline (both parquet storage and aggregation) - these are data quality issues
- Preserve all other values as-is

#### Numeric Column Conversion
- **Years columns**: `Yrs Since Issue`, `Yrs (Worst)`, and `Yrs (Cvn)` are automatically converted to numeric (`float64`) during transformation
- **Spread and metric columns**: `G Sprd`, `vs BI`, `vs BCE`, `MTD Equity`, `YTD Equity`, `Retracement`, `Z Score`, `Retracement2` are automatically converted to numeric (`float64`) during transformation
- **Conversion points**: 
  - Transform step: Converts during `transform_single_file()` processing via `convert_years_to_numeric()` method
  - Load step (append): Converts new data and existing data for compatibility
  - Load step (override): Converts all data before writing
- **Result**: These columns are always stored as numeric (`float64`) in parquet files, enabling proper binning, statistical analysis, and accurate merging with other datasets
- **Error handling**: Non-numeric values are coerced to NaN (errors='coerce')

#### BQL Pipeline
- Read Excel workbook with 4-row multi-index header (`header=[0,1,2,3]`)
- Extract security names from level 1 (row 1) and CUSIPs from level 2 (row 2) of multi-index columns
- Normalize header values to derive security names and enforce 9-character CUSIPs
- Reshape wide workbook to long-form dataset (`Date`, `Name`, `CUSIP`, `Value`)
- Skip duplicate/invalid CUSIP columns and log issues for traceability
- Drop rows with non-numeric values before persisting
- Compare resulting CUSIPs against `universe.parquet` and log orphan details (CUSIP + Name)

---

## Logging Standards

### Log File Organization
```
bond_data/logs/
├── processing.log      # File extraction and loading operations
├── duplicates.log      # All duplicate CUSIPs detected
├── validation.log      # CUSIP validation warnings
├── summary.log         # Pipeline execution summary (run headers, stats)
├── parquet_stats.log   # Snapshot diagnostics for each parquet dataset (df.info/head/tail)
└── runs_today.log     # Runs today analytics execution log (merge statistics, df.info, matching details)
```

### Logging Levels
- **DEBUG**: Detailed diagnostic info (file-by-file progress)
- **INFO**: General informational messages
- **WARNING**: CUSIP validation issues, schema mismatches
- **ERROR**: File read failures, data corruption
- **CRITICAL**: System failures

### Logging Best Practices
1. **Dual logging**: File logging (detailed) + Console logging (essential only)
2. **No console spam**: Suppress detailed logs on console (console_level=logging.CRITICAL)
3. **Run metadata**: Track run_id, timestamp, mode for each execution
4. **Log rotation**: Archive logs after N runs (currently 10)
5. **Structured info**: Include Date, CUSIP, file path in log messages
6. **BQL ingestion**: Log unique CUSIP/date counts and orphan CUSIPs with security names
7. **Unicode encoding**: File handlers use UTF-8 encoding with `errors='replace'` to handle Unicode characters in bond names
8. **ASCII-safe logging**: Use `sanitize_log_message()` function to convert Unicode characters to ASCII-safe replacements before logging (prevents Windows console encoding errors)

### Example Logging Pattern
```python
# Setup logger (file only, no console)
self.logger = setup_logging(LOG_FILE_PROCESSING, 'extract', console_level=logging.CRITICAL)

# Log file processing
self.logger.info(f"Processing file: {filename}")
self.logger.info(f"Extracted {len(df)} rows for date {date_obj}")

# Log validation issues
self.logger.warning(f"Invalid CUSIP: {cusip} (length {len(cusip)})")

# Log duplicates
self.logger_dupes.info(f"Duplicate Date+CUSIP: {date} - {cusip} - {bond_name} (kept last occurrence)")
```

---

## Testing Standards

### Test Structure
```
tests/
├── conftest.py              # Shared fixtures (sample data, mocks)
├── unit/                    # Unit tests
│   ├── test_utils.py       # ✅ Bond utility & BQL helper tests
│   ├── test_extract.py     # TODO
│   ├── test_transform.py   # TODO
│   └── test_load.py        # TODO
└── integration/             # Integration tests
    └── test_pipeline.py    # TODO
```

### Testing Requirements
1. **Coverage**: Target 85%+ overall, 90%+ for critical modules
2. **Test location**: All tests in `tests/` directories within each module
3. **Naming**: Test files follow naming convention: `test_*.py`
4. **Framework**: Use pytest for testing
5. **Fixtures**: Reusable test data in `conftest.py`
6. **Isolation**: Tests must be independent (no shared state)
7. **Mocking**: Mock all file I/O, external dependencies
8. **Speed**: Full test suite should run in < 30 seconds

### Test Categories
- **Date parsing**: Valid/invalid formats, leap years, edge cases
- **CUSIP validation**: Valid/invalid lengths, characters, normalization
- **NA cleaning**: Standard patterns, mixed DataFrames
- **Schema alignment**: Old (59) vs new (75) column schemas
- **Deduplication**: Within file, across files, Date+CUSIP combinations
- **Append mode**: Skip existing dates, append new dates
- **Override mode**: Delete existing, rebuild from scratch
- **BQL ingestion**: Header normalization, wide→long reshape, orphan logging against universe

### Running Tests
```bash
# Install dependencies (first run or after updates)
poetry install

# Run all tests
poetry run pytest

# Run specific file
poetry run pytest tests/unit/test_utils.py

# Run with coverage
poetry run pytest --cov=bond_pipeline --cov-report=html

# Run specific test
poetry run pytest tests/unit/test_utils.py::TestCUSIPValidation::test_validate_cusip_valid
```

---

## File Organization Rules

### Directory Structure
```
Bond-RV-App/
├── analytics/             # Analytics scripts and processed outputs
│   ├── comb/             # Pair analytics scripts
│   │   └── comb.py           # Consolidated pair analytics script (contains 11 analysis functions: all_comb, term_comb, ticker_comb, custom_sector, custom_bond_comb, custom_bond_vs_holdings, cad_cheap_vs_usd, cad_rich_vs_usd, executable_cr01_vs_holdings, executable_cr01_decent_bid_offer_vs_holdings, all_combos_vs_holdings; outputs single Excel file with multiple sheets, plus comb.txt and comb_validation.txt)
│   ├── runs/             # Runs analytics scripts
│   │   ├── runs_today.py     # Runs today analytics (reads parquet directly, aggregates required dates, computes DoD/MTD/YTD/1yr changes, filters negative spreads, matches by (CUSIP, Benchmark) tuples)
│   │   └── runs_views.py     # Custom formatted tables from runs_today.csv (portfolio and universe views with custom formatting, generates both .txt and .xlsx outputs, includes "Filters Applied:" sections for all tables, auto-validates and regenerates CSV if stale)
│   └── processed_data/   # Outputs from analytics scripts
│       ├── comb.xlsx         # Consolidated Excel file with all pair analytics (from comb.py, formatted tables on separate sheets, one sheet per analysis type)
│       ├── comb.txt          # Text summary of pair analytics results (from comb.py)
│       ├── comb_validation.txt  # Validation log for pair analytics (from comb.py)
│       ├── runs_today.csv    # Daily runs analytics output (from runs_today.py, used by runs_views.py and comb.py)
│       ├── portfolio_runs_view.txt  # Formatted portfolio tables (from runs_views.py, text output, 16 tables)
│       ├── portfolio_runs_view.xlsx  # Excel file with 16 portfolio tables (from runs_views.py, formatted Excel tables on separate sheets)
│       ├── uni_runs_view.txt  # Formatted universe tables (from runs_views.py, text output)
│       └── uni_runs_view.xlsx  # Excel file with universe tables (from runs_views.py, formatted Excel tables on separate sheets)
├── bond_pipeline/          # Pipeline code (7 modules)
├── bond_data/              # Data directory (local only, git-ignored)
│   ├── parquet/           # Output parquet files
│   └── logs/              # Processing logs
├── .venv/                 # Poetry-managed virtual environment (git-ignored)
├── Documentation/          # Complete documentation
│   ├── Setup/             # Getting started guides
│   ├── Workflows/         # Step-by-step procedures
│   └── Architecture/      # Design documentation
├── Raw Data/              # Excel files (input)
├── References and Documentation/  # Ignored by Cursor
├── tests/                 # Test suite
│   ├── unit/              # Unit tests
│   ├── integration/       # Integration tests
│   └── patterns/          # Pattern analysis and data exploration scripts (not production tests)
│       ├── analyze_runs_data.py
│       ├── data_analysis_and_questions.md
│       └── deep_duplicate_analysis.py
├── utils/                 # Utility scripts (currently empty, reserved for future utility scripts)
├── .cursorrules           # This file
├── .cursorignore          # Files to ignore
├── pyproject.toml         # Poetry project configuration
├── poetry.lock            # Locked dependency versions
├── requirements.txt       # Legacy dependency snapshot (reference only)
├── README.md              # Project overview
└── run_pipeline.py        # Simple pipeline runner
```

### File Naming Conventions
- **Modules**: `snake_case.py` (e.g., `bond_pipeline/utils.py`)
- **Test files**: `test_*.py` (e.g., `tests/unit/test_utils.py`)
- **Config files**: `.cursorrules`, `.cursorignore`, `pytest.ini`
- **Data files**: `historical_bond_details.parquet`, `universe.parquet`

### Important Paths
- **Project root**: `PROJECT_ROOT = Path(__file__).parent.parent` (from config.py)
- **Data directory**: `bond_data/` (git-ignored)
- **Parquet files**: `bond_data/parquet/` (git-tracked)
- **Logs**: `bond_data/logs/` (git-ignored)
- **Default input**: Dropbox folder (Windows-specific path in config.py)

### Path Handling Rules
- **Use `pathlib.Path`** for all path operations
- **Scripts must detect their location** and adjust paths accordingly
- **Support running from multiple directory locations** (absolute and relative paths)
- **Avoid hardcoded paths**: Use relative paths with config-based resolution
- **Analytics script path resolution**:
  - Scripts in `analytics/comb/` and `analytics/runs/` use `SCRIPT_DIR = Path(__file__).parent.resolve()`
  - Parquet files: `SCRIPT_DIR.parent.parent / "bond_data" / "parquet" / "*.parquet"` (goes up 2 levels to project root)
  - Output directory: `SCRIPT_DIR.parent / "processed_data"` (goes up 1 level to `analytics/processed_data/`)
  - Cross-references: `SCRIPT_DIR.parent / "processed_data" / "*.csv"` (for scripts reading other analytics outputs)

---

## Git & Version Control

### Git-Ignored Files
- Excel files: `*.xlsx`, `*.xls`
- Log files: `bond_data/logs/*.log`
- Virtual environment: `.venv/`
- Python cache: `__pycache__/`, `*.pyc`
- Test output: `tests/test_output/`
- References folder: `References and Documentation/` (in .cursorignore)

### Git-Tracked Files
- Parquet files: `bond_data/parquet/*.parquet` (processed data)
- Source code: `bond_pipeline/`, `utils/`
- Tests: `tests/`
- Documentation: `Documentation/`
- Config: `pyproject.toml`, `poetry.lock`, `requirements.txt`, `.cursorrules`, `.cursorignore`
- Scripts: `run_pipeline.py`

### Git Commit Protocol
- **Update `.cursorrules`** before every git commit
- **Verify file timestamps** and version consistency
- **Document changes** in commit messages with reference to `.cursorrules` updates

### Unicode & Encoding
- **Avoid unicode encoding issues** for Windows
- **Use ASCII-compatible characters** when needed
- **Test paths** on Windows file system before committing

---

## Code Modification Guidelines

### Before Making Changes
1. **Search codebase first**: Always search within the codebase to see if any existing functions and/or classes can be used before building new logic
2. **Analyze entire codebase**: Understand dependencies and relationships
3. **Map file impacts**: Identify ALL files that need modification
4. **Check existing logic**: Extend or refactor before creating new code
5. **Consider ripple effects**: Changes impact downstream modules

### Coding Principles
1. **Extend existing logic**: Never create new files when existing code can be extended
2. **Refactor first**: Improve existing structure before adding new features
3. **Minimal changes**: Make smallest change that achieves the goal
4. **Backward compatibility**: Don't break existing functionality
5. **Documentation**: Update docstrings when modifying functions

### File Creation Rules
- **List new files**: Announce file name and path before creating
- **Test files**: Always create in `tests/` folder when applicable
- **Avoid duplication**: Check if existing modules can handle the requirement
- **Justification**: Explain why new file is needed vs extending existing
- **Clean up temporary files**: Remove any migration/restore/cleanup scripts after use

### When AI Assistant Helps
1. **Read existing code**: Always read relevant files before suggesting changes
2. **Show context**: Reference existing code with `startLine:endLine:filepath` format
3. **Propose edits**: Suggest specific changes with line-by-line diffs
4. **Test changes**: Run relevant tests after modifications
5. **Check lints**: Fix any linter errors introduced
6. **Clean up temporary files**: Remove all temporary files, backups, and migration scripts before completing the task

### Example Code Modification
**Before**: Adding a new validation function
```python
# 1. Read existing utils.py to understand patterns
# 2. Check if similar functions exist
# 3. Extend existing module rather than create new file
# 4. Add function following existing naming/documentation conventions
# 5. Add tests in tests/unit/test_utils.py
# 6. Update module docstring if needed
```

---

## Execution & Workflow

### Running the Pipeline
```bash
# 1. Optionally enter Poetry shell
poetry shell

# 2. Run automated pipeline runner
poetry run python run_pipeline.py
# Prompts: 1) Pipeline selection (1=Bond, 2=Runs, 3=Portfolio, 4=Both Bond+Runs, 5=Individual Parquet), 2) Mode (override/append), 3) (Bond) Include BQL workbook

# 3. Or use direct CLI (Bond Pipeline)
cd bond_pipeline
poetry run python pipeline.py -i "../Raw Data/" -m append --process-bql
poetry run python pipeline.py -i "../Raw Data/" -m override --process-bql

# 4. Or use Python module (Bond Pipeline)
poetry run python -m bond_pipeline.pipeline -i "Raw Data/" -m append

# 5. Runs Pipeline CLI
poetry run python -m runs_pipeline.pipeline -i "Historical Runs/" -m append
poetry run python -m runs_pipeline.pipeline -i "Historical Runs/" -m override

# 6. Portfolio Pipeline CLI
poetry run python -m portfolio_pipeline.pipeline -i "AD History/" -m append
poetry run python -m portfolio_pipeline.pipeline -i "AD History/" -m override

# 7. Unified Pipeline Orchestrator (Recommended)
poetry run python run_pipeline.py
# Prompts: Select pipeline(s) or individual parquet files, choose mode, decide on BQL ingestion when running Bond pipeline
# Option 5 allows regenerating individual parquet files (historical_bond_details, bql, runs_timeseries, or historical_portfolio)
```

### Pipeline Modes

#### Append Mode (Default, Daily Use)
- **Use case**: Add new Excel files to existing parquet tables
- **Behavior**: Skip dates already in parquet, append only new dates
- **Output**: Updated `historical_bond_details.parquet` (with new dates appended)
- **Universe**: Rebuild `universe.parquet` from complete historical data

#### Override Mode (Rebuild Everything)
- **Use case**: First-time setup, schema changes, data corruption recovery
- **Behavior**: Delete existing parquet files, process all Excel files from scratch
- **Output**: New `historical_bond_details.parquet`, new `universe.parquet`
- **Speed**: Slower but ensures clean data

### Checking Results
```bash
# View output parquet files
ls bond_data/parquet/

# Check logs
cat bond_data/logs/summary.log       # Pipeline execution summary
cat bond_data/logs/processing.log    # File-by-file details
cat bond_data/logs/duplicates.log    # Duplicate CUSIPs
cat bond_data/logs/validation.log    # CUSIP validation issues

# Quick stats in Python
import pandas as pd
df = pd.read_parquet('bond_data/parquet/historical_bond_details.parquet')
print(f"Rows: {len(df)}, Columns: {len(df.columns)}, Date range: {df['Date'].min()} to {df['Date'].max()}")

# Runs pipeline stats
df_runs = pd.read_parquet('bond_data/parquet/runs_timeseries.parquet')
print(f"Rows: {len(df_runs)}, Columns: {len(df_runs.columns)}")
print(f"Date range: {df_runs['Date'].min()} to {df_runs['Date'].max()}")
print(f"Unique CUSIPs: {df_runs['CUSIP'].nunique()}, Unique Dealers: {df_runs['Dealer'].nunique()}")

# BQL spreads stats
df_bql = pd.read_parquet('bond_data/parquet/bql.parquet')
print(f"BQL rows: {len(df_bql)}, Unique CUSIPs: {df_bql['CUSIP'].nunique()}, Unique Dates: {df_bql['Date'].nunique()}")

# Portfolio pipeline stats
df_portfolio = pd.read_parquet('bond_data/parquet/historical_portfolio.parquet')
print(f"Rows: {len(df_portfolio)}, Columns: {len(df_portfolio.columns)}")
print(f"Date range: {df_portfolio['Date'].min()} to {df_portfolio['Date'].max()}")
print(f"Unique CUSIPs: {df_portfolio['CUSIP'].nunique()}, Unique Accounts: {df_portfolio['ACCOUNT'].nunique()}, Unique Portfolios: {df_portfolio['PORTFOLIO'].nunique()}")
```

---

## Common Patterns & Examples

### Excel File Pattern Matching
```python
import re
from config import FILE_PATTERN

filename = "API 10.20.25.xlsx"
match = re.match(FILE_PATTERN, filename)
# Returns: datetime(2025, 10, 20)
```

### CUSIP Normalization
```python
from utils import normalize_cusip

raw = "89678zab2"  # lowercase
normalized = normalize_cusip(raw)  # Returns: "89678ZAB2"

raw = " 06418GAD9 Corp"  # extra text
normalized = normalize_cusip(raw)  # Returns: "06418GAD9"
```

### Schema Alignment
```python
from utils import align_to_master_schema

old_schema_df = read_excel(old_file)  # 59 columns
master_schema = [... 75 columns ...]
aligned_df = align_to_master_schema(old_schema_df, master_schema)  # 75 columns with NULL in new cols
```

### Deduplication
```python
from transform import DataTransformer

transformer = DataTransformer(log_dupes, log_valid)
df_cleaned = transformer.deduplicate(df, bond_date)

# Within same file: keep last occurrence
# Across files: Date+CUSIP primary key enforcement
```

### Logging Pattern
```python
from pathlib import Path
import logging
from utils import setup_logging

log_file = Path("bond_data/logs/processing.log")
logger = setup_logging(log_file, 'module_name', console_level=logging.CRITICAL)

logger.info("Processing file: API 10.20.25.xlsx")
logger.warning("Invalid CUSIP: 1234567890 (length 10)")
logger.error("Failed to read file: missing_header.xlsx")
```

### Reading/Writing Parquet
```python
import pandas as pd
from config import HISTORICAL_PARQUET, DATE_COLUMN

# Read existing dates (for append mode)
df = pd.read_parquet(HISTORICAL_PARQUET, columns=[DATE_COLUMN])
existing_dates = set(df[DATE_COLUMN].unique())

# Write new data (append mode)
df_new.to_parquet(HISTORICAL_PARQUET, mode='append', index=False)

# Write new data (override mode)
df_new.to_parquet(HISTORICAL_PARQUET, mode='overwrite', index=False)
```

---

## Troubleshooting Common Issues

### Import Errors
```bash
# Make sure you're in project root and virtual environment is activated
cd Bond-RV-App
poetry shell

# Use module syntax for imports
python -m bond_pipeline.pipeline
```

### Virtual Environment Issues
```bash
# Reinstall dependencies
poetry install

# Spawn a Poetry-managed shell if needed
poetry shell
```

### Test Discovery Issues
```bash
# Run from project root
cd Bond-RV-App
pytest tests/

# Or specific file
pytest tests/unit/test_utils.py -v
```

### Parquet File Locking
```bash
# Close any open Python sessions reading parquet files
# Windows: Check Task Manager for Python processes
```

---

## Key Takeaways for AI Assistant

### Essential Rules
1. **Search codebase first**: Always search within the codebase to see if any existing functions and/or classes can be used before building new logic
2. **Always analyze entire codebase** before making changes
3. **Execute all code in current virtual environment** (Bond-RV-App)
4. **Create tests in tests/ folder** when applicable
5. **Never create new files** unless explicitly required by user
6. **List file names and paths** when creating new files
7. **Include .cursorrules for context** in planning
8. **Map dependencies** before suggesting edits
9. **Extend existing logic** rather than duplicating
10. **Follow modular architecture**: config → utils → extract → transform → load → pipeline
11. **Respect logging patterns**: file logging (detailed) + console (minimal)

### Workflow Rules
12. **Think deeply** about prompt applicability before acting
13. **Use judgment** when interpreting conflicting requirements
14. **Notify user** when ignoring requirements or using judgment
15. **Ask clarifying questions** until all ambiguity is eliminated
16. **Provide file change summary** after completing requests
17. **ALWAYS clean up temporary files** created during chat sessions
18. **Always update .cursorrules** after applicable changes
19. **Include date/time stamps** in all documentation
20. **Update .cursorrules before git commits**

### Quality Standards
21. **Use pathlib.Path** for all path operations
22. **Avoid unicode issues** on Windows; all console/log output must remain ASCII-safe
23. **Modular design** following industry best practices
24. **Keep documentation current** when creating new docs

---

**File Created**: January 2025  
**Last Major Update**: 2025-01-17 20:00 ET  
**Project**: Bond RV App - Data Pipeline  
**Version**: 1.0

---

**Change Log**:
- **2025-01**: Initial comprehensive `.cursorrules` file created
- **2025-01**: Added AI Assistant Core Workflow section
- **2025-01**: Added git commit protocol and unicode handling
- **2025-01**: Enhanced documentation timestamp requirements
- **2025-01**: Added path handling and encoding rules
- **2025-01**: Removed poetry environment references, switched to standard pip/virtual environment
- **2025-01**: Removed all Outlook email pipeline functionality (monitor_outlook.py, runs_miner.py, utils/outlook_monitor.py, related documentation, and pywin32 dependency)
- **2025-01**: Added runs_pipeline module documentation:
  - Added runs_pipeline/ to module structure section
  - Added runs_timeseries.parquet to output data format section
  - Documented RUNS file pattern, header row, primary key (Date+Dealer+CUSIP)
  - Documented end-of-day snapshot deduplication logic (latest Time per Date+Dealer+CUSIP)
  - Documented CUSIP orphan tracking requirements (compare with universe.parquet)
  - Added runs_pipeline execution examples to workflow section
  - Updated class-based architecture section with runs pipeline classes
- **2025-01-02**: Performance optimizations:
  - Documented vectorized deduplication (~100x faster, O(n log n) vs O(n*groups))
  - Documented vectorized CUSIP validation (~100x faster, vectorized string operations)
  - Updated logging patterns to reflect summary logging instead of row-by-row
- **2025-01-02**: Enhanced orphan CUSIP logging:
  - Documented enhanced logging with context (Security, Date, Dealer, Time, Ticker)
  - Updated orphan tracking section to reflect detailed context in logs
- **2025-01-02**: Added unified pipeline orchestrator (run_pipeline.py):
  - Documented run_pipeline.py as recommended entry point
  - Added user selection for pipeline(s) and mode
- **2025-11-07 15:30 ET**: Added BQL workbook ingestion pipeline:
  - Documented `bql.xlsx` processing and `bql.parquet` output
  - Updated CLI/orchestrator guidance with `--process-bql` flag and prompt
  - Captured orphan logging requirements (CUSIP + Name) for BQL dataset
  - Added testing coverage notes for BQL helpers in `tests/unit/test_utils.py`
- **2025-11-07 16:20 ET**: Created/activated standard `Bond-RV-App` virtual environment and installed dependencies from `requirements.txt`
- **2025-11-07 16:45 ET**: Migrated to Poetry-based workflow (`pyproject.toml`, `poetry.lock`, in-project `.venv`), updated execution/testing instructions, and removed legacy `Bond-RV-App/` virtualenv
- **2025-11-08 11:05 ET**: Added `ipykernel` dev dependency and registered `Bond RV App (.venv)` Jupyter kernel for notebook execution
- **2025-11-08 11:20 ET**: Added `nbconvert` dev dependency to support automated notebook execution for testing kernels
- **2025-11-08 11:30 ET**: Added VS Code workspace settings to pin notebooks to the Poetry-managed `.venv` interpreter
- **2025-11-08 11:35 ET**: Added `notebook` dev dependency to ensure VS Code can start a local Jupyter server from the Poetry environment
- **2025-11-09 13:30 ET**: Optimized RUNS append flow (skip fully loaded files, new load metrics), enhanced run summaries, and introduced `parquet_stats.log` diagnostics
- **2025-11-09 07:25 ET**: Added `cr01_comb.py` CR01 pair analytics script (BQL universe vs holdings spreads) - **MOVED to `analytics/cr01_comb.py` on 2025-11-09 15:30 ET**
  - Exports single CSV file `cr01_pair_analytics.csv` to `analytics/processed_data/` (overwrites on each run, no timestamps)
  - Computes pairwise spreads (universe minus holdings) with statistical metrics (Last, Avg, vs Avg, Z Score, Percentile)
- **2025-11-09 13:30 ET**: Updated BQL workbook processing to handle 4-row multi-index header:
  - Modified `read_bql_workbook` to read with `header=[0,1,2,3]` for multi-index columns
  - Updated `reshape_bql_to_long` to extract names from level 1 (row 1) and CUSIPs from level 2 (row 2)
  - Data rows start at row 4 (index 4) after header rows 0-3
  - Updated config constants (`BQL_NAME_ROW_INDEX`, `BQL_CUSIP_ROW_INDEX`, `BQL_DATA_START_ROW`)
  - Fixed Date column handling for multi-index DataFrames (find tuple column name, rename to standard)
  - Updated `.cursorrules` documentation to reflect new header structure
- **2025-11-09 14:00 ET**: Enhanced logging and pipeline features:
  - Added `sanitize_log_message()` function for ASCII-safe log output (prevents Unicode encoding errors on Windows)
  - Updated file handlers to use UTF-8 encoding with `errors='replace'` for Unicode character handling
  - Added individual parquet file regeneration option (option 4) to `run_pipeline.py`:
    - Allows regenerating `historical_bond_details.parquet`, `bql.parquet`, or `runs_timeseries.parquet` independently
    - Useful for quick fixes or testing without running full pipelines
- **2025-11-09 15:30 ET**: Created analytics folder structure and pair analytics scripts:
  - Created `analytics/` folder with `processed_data/` subfolder for CSV outputs
  - Moved `cr01_comb.py` from root to `analytics/comb/cr01_comb.py` with updated paths (script-relative path resolution)
  - Created `analytics/comb/all_comb.py` - all combinations pair analytics script:
    - Computes all pairwise spreads from BQL parquet data (all CAD CUSIPs)
    - Filters to CAD CUSIPs from `historical_bond_details.parquet` (Currency="CAD" on last date)
    - Filters to CUSIPs present in most recent 75% of dates with complete data
    - Uses vectorized numpy operations for performance optimization
    - Outputs top 80 pairs sorted by Z Score to `all_comb.csv`
    - Column names: `Bond_1`, `Bond_2`, `cusip_1`, `cusip_2` (instead of universe/holdings naming)
  - Both scripts use script-relative path resolution for portability
- **2025-11-09 16:00 ET**: Enhanced data processing and statistics:
  - Added dealer filtering for runs_timeseries.parquet (only BMO, BNS, NBF, RBC, TD dealers stored)
  - Added numeric conversion for years columns: `Yrs Since Issue`, `Yrs (Worst)`, and `Yrs (Cvn)` are always stored as numeric (`float64`) in parquet files
  - Conversion happens at multiple points (transform and load) to ensure consistency
  - Updated enhanced statistics to use `Yrs (Cvn)` instead of `Yrs (Worst)` for binning analysis
  - Statistics now include proper binning with `<1` bin to capture negative and small values
- **2025-11-09 17:00 ET**: Created runs adjusted timeseries analytics script:
  - Created `analytics/runs/runs-adjusted_ts.py` - runs timeseries aggregation script (MERGED into runs_today.py on 2025-01-XX):
    - Reads `runs_timeseries.parquet` and groups by Date, CUSIP, and Benchmark
    - Computes 23 aggregated metrics including:
      - Basic columns: Date, CUSIP, Time (latest), Bid Workout Risk (average), Security
      - Spread metrics: Tight Bid >3mm, Wide Offer >3mm, Tight Bid, Wide Offer
      - Dealer and size columns for >3mm metrics
      - CR01 calculations: CR01 @ Tight Bid, CR01 @ Wide Offer (using Bid Workout Risk avg * Size / 10000)
      - Cumulative sizes: Cumm. Bid Size, Cumm. Offer Size
      - Counts: # of Bids >3mm, # of Offers >3mm (unique dealers)
      - RBC dealer columns: Bid RBC, Ask RBC, Bid Size RBC, Offer Size RBC
    - All numeric columns properly typed as `float64`
    - Sorts by Date (earliest to latest)
    - Exports to `runs_adjusted_ts.csv` in `analytics/processed_data/` (no longer generated)
    - Displays table preview and `df.info()` for validation
    - Uses script-relative path resolution for portability
- **2025-11-09 18:00 ET**: Created runs today analytics script:
  - Created `analytics/runs/runs_today.py` - daily runs analytics with Day-over-Day changes:
    - Reads `runs_timeseries.parquet` directly and aggregates required dates only (optimized)
    - Only includes CUSIPs present on both last date and second-to-last date
    - Computes aggregated metrics including:
      - Basic columns: CUSIP, Security, Bid Workout Risk
      - Spread metrics: Tight Bid >3mm, Wide Offer >3mm, Bid/Offer>3mm, Tight Bid, Wide Offer, Bid/Offer
      - Dealer columns: Current dealers and T-1 dealers (from second-to-last date)
      - Size, CR01, cumulative, and count metrics
      - RBC dealer columns
      - 16 Day-over-Day (DoD) change columns (Last Date - Second Last Date)
      - 16 MTD change columns (Last Date - First Date of Month)
      - 16 YTD change columns (Last Date - First Date of Year)
      - 16 1yr change columns (Last Date - Closest Date ~1 Year Ago)
    - Merges external columns from `historical_portfolio.parquet` (last date): QUANTITY, POSITION CR01
    - Merges external columns from `historical_bond_details.parquet` (most recent available date): G Sprd, Yrs (Cvn), vs BI, vs BCE, MTD Equity, YTD Equity, Retracement, Yrs Since Issue, Z Score, Retracement2, Rating, Custom_Sector
    - CUSIP normalization: Normalizes CUSIPs for matching (runs CUSIPs may not be normalized, bond/portfolio CUSIPs are normalized)
    - Portfolio aggregation: Sums QUANTITY and POSITION CR01 when multiple ACCOUNT/PORTFOLIO rows exist per CUSIP
    - Bond details date handling: Uses most recent available date if exact date match not found
    - Bid/Offer calculations only populate when both source columns have values
    - All numeric columns properly typed as `float64`
    - Sorts by CUSIP
    - Exports to `runs_today.csv` in `analytics/processed_data/`
    - Comprehensive logging to `bond_data/logs/runs_today.log`:
      - Merge statistics (match counts, percentages, unmatched CUSIPs)
      - Full `df.info()` output for input parquet and final CSV (verbose mode)
      - Column summaries and sample data
      - CUSIP normalization details
    - Uses script-relative path resolution for portability
    - Configuration section at top for easy column management
- **2025-01-XX XX:XX ET**: Merged runs-adjusted_ts.py into runs_today.py and optimized date processing:
  - Merged `analytics/runs/runs-adjusted_ts.py` into `analytics/runs/runs_today.py` (single flow)
  - Removed intermediate `runs_adjusted_ts.csv` output file (no longer generated)
  - Optimized to process only required dates (last, second-to-last, MTD ref, YTD ref, 1yr ref) instead of all dates
  - Reads directly from `runs_timeseries.parquet` instead of intermediate CSV
  - Single workflow: aggregation → today analysis → CSV output
  - Keeps `compute_group_metrics()` and `ensure_ascii()` as helper functions
  - Improved performance by filtering parquet data before grouping/aggregation
  - Deleted `runs-adjusted_ts.py` file
- **2025-01-XX XX:XX ET**: Enhanced runs today analytics with MTD, YTD, and 1yr changes:
  - Added Month-to-Date (MTD) change columns for all DoD columns (compares last date with first available date of current month)
  - Added Year-to-Date (YTD) change columns for all DoD columns (compares last date with first available date of current year)
  - Added 1-year (1yr) change columns for all DoD columns (compares last date with closest available date approximately 1 year ago)
  - Naming convention: `DoD Chg {column}` → `MTD Chg {column}`, `YTD Chg {column}`, `1yr Chg {column}`
  - All MTD, YTD, and 1yr columns properly typed as `float64`
  - Column ordering: DoD columns first, then MTD, then YTD, then 1yr (grouped by change type)
  - Handles cases where reference dates don't exist (returns NA values)
  - Updated module and function docstrings to reflect new change calculations
- **2025-01-XX XX:XX ET**: Created portfolio combinations pair analytics script:
  - Created `analytics/comb/port_comb.py` - portfolio pair analytics script:
    - Similar to `all_comb.py` but filters pairs where `cusip_2` is in portfolio CUSIP list
    - Computes all pairwise combinations first, then filters to portfolio CUSIPs, then selects top 80 by Z Score
    - Portfolio CUSIP list defined as constant (59 CUSIPs, normalized to uppercase)
    - Shows bond names (not just CUSIPs) in warning messages for missing portfolio CUSIPs from BQL data
    - Uses same filtering logic as `all_comb.py` (CAD CUSIPs, recent dates, complete data)
    - Outputs to `port_comb.csv` in `analytics/processed_data/`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Created portfolio executable combinations pair analytics script:
  - Created `analytics/comb/port_executable.py` - portfolio executable pair analytics script:
    - Similar to `port_comb.py` but adds additional filter on `cusip_1`
    - Filters `cusip_1` to CUSIPs from `runs_today.csv` where `CR01 @ Wide Offer >= 2000`
    - Filters `cusip_2` to portfolio CUSIPs (same as `port_comb.py`)
    - Both filters applied (AND condition) after computing all pairs
    - Raises error if `CR01 @ Wide Offer` column missing or no matching CUSIPs found
    - Excludes rows with NaN/null `CR01 @ Wide Offer` values
    - Outputs to `port_executable.csv` in `analytics/processed_data/`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Created CAD cheap vs USD pair analytics script:
  - Created `analytics/comb/cad_cheap_vs_usd.py` - CAD vs USD pair analytics script:
    - Based on `all_comb.py` but removes CAD-only filter from BQL data
    - Computes all pairwise combinations first, then filters results
    - Filters to pairs where `cusip_1` has Currency="CAD" and `cusip_2` has Currency="USD"
    - Requires matching Ticker values (from last date in historical_bond_details.parquet)
    - Requires matching Custom_Sector values (from last date)
    - Requires absolute difference in "Yrs (Cvn)" <= 2 (from last date)
    - Excludes CUSIPs with missing/null Currency, Ticker, Custom_Sector, or Yrs (Cvn)
    - Outputs top 80 pairs sorted by Z Score to `cad_cheap_vs_usd.csv`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Created CAD rich vs USD pair analytics script:
  - Created `analytics/comb/cad_rich_vs_usd.py` - USD vs CAD pair analytics script:
    - Same as `cad_cheap_vs_usd.py` but with swapped Currency roles
    - Filters to pairs where `cusip_1` has Currency="USD" and `cusip_2` has Currency="CAD"
    - Requires matching Ticker values (from last date in historical_bond_details.parquet)
    - Requires matching Custom_Sector values (from last date)
    - Requires absolute difference in "Yrs (Cvn)" <= 2 (from last date)
    - Excludes CUSIPs with missing/null Currency, Ticker, Custom_Sector, or Yrs (Cvn)
    - Outputs top 80 pairs sorted by Z Score to `cad_rich_vs_usd.csv`
    - Uses script-relative path resolution for portability
    - Spread calculation: `cusip_1_values - cusip_2_values` (USD - CAD, opposite of cheap version)
- **2025-01-XX XX:XX ET**: Created term combinations pair analytics script:
  - Created `analytics/comb/term_comb.py` - term combinations pair analytics script:
    - Based on `all_comb.py` but adds pre-filtering by "Yrs (Cvn)" difference
    - Filters pairs where `abs(yrs_cvn_1 - yrs_cvn_2) <= 0.8` before computing spreads
    - Filters to CAD CUSIPs with valid "Yrs (Cvn)" data on last date
    - Excludes CUSIPs without "Yrs (Cvn)" data
    - Outputs top 80 pairs sorted by Z Score to `term_comb.csv`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Created ticker combinations pair analytics script:
  - Created `analytics/comb/ticker_comb.py` - ticker combinations pair analytics script:
    - Based on `all_comb.py` but adds pre-filtering by matching Ticker values
    - Filters pairs where `ticker_1 == ticker_2` (exact match, case-sensitive) before computing spreads
    - Filters to CAD CUSIPs with valid Ticker data on last date
    - Excludes CUSIPs without Ticker data
    - Outputs top 80 pairs sorted by Z Score to `ticker_comb.csv`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Created custom sector combinations pair analytics script:
  - Created `analytics/comb/custom_sector.py` - custom sector combinations pair analytics script:
    - Based on `all_comb.py` but adds pre-filtering by matching Custom_Sector values
    - Filters pairs where `custom_sector_1 == custom_sector_2` (exact match, case-sensitive) before computing spreads
    - Filters to CAD CUSIPs with valid Custom_Sector data on last date
    - Excludes CUSIPs without Custom_Sector data
    - Outputs top 80 pairs sorted by Z Score to `custom_sector_comb.csv`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Created custom bond combinations pair analytics script:
  - Created `analytics/comb/custom_bond_comb.py` - custom bond combinations pair analytics script:
    - Pairs all CAD CUSIPs (cusip_1) with a fixed target bond (cusip_2)
    - Target bond configurable via `TARGET_BOND_TICKER` (takes precedence) or `TARGET_BOND_SECURITY`
    - Searches `universe.parquet` for target bond(s) by Ticker (exact match) or Security name
    - If Ticker specified and multiple bonds match, uses all matching CUSIPs (creates pairs for each)
    - Falls back to Security name if Ticker not found (with warning)
    - Displays Security name in Bond_2 column
    - Filters to CAD CUSIPs present in most recent 75% of dates
    - Validates target bond(s) have BQL data and are in recent dates
    - Outputs all pairs sorted by Z Score to `custom_bond_comb.csv`
    - Uses script-relative path resolution for portability
    - Configuration section at top: `TARGET_BOND_TICKER`, `TARGET_BOND_SECURITY`, `RECENT_DATE_PERCENT`, `TOP_N_PAIRS`
- **2025-01-XX XX:XX ET**: Created custom bond vs holdings pair analytics script:
  - Created `analytics/comb/custom_bond_vs_holdings.py` - custom bond vs holdings pair analytics script:
    - Pairs target CAD bonds (cusip_1/Bond_1) from `TARGET_BOND_TICKER/SECURITY` with CR01-filtered holdings (cusip_2/Bond_2)
    - cusip_1: Filters to CAD CUSIPs first, then finds target bond(s) via TARGET_BOND_TICKER/SECURITY in universe.parquet
    - cusip_2: Loads holdings with CR01 @ Tight Bid > 2000 from runs_today.csv, matched with portfolio (last date)
    - Both cusip_1 and cusip_2 filtered to recent dates (75% of dates) using BQL data
    - Spread calculation: cusip_1 - cusip_2
    - BQL data required only for cusip_1 (target bonds); holdings filtered to recent dates but don't require BQL data themselves
    - Outputs all pairs sorted by Z Score to `custom_bond_vs_holdings.csv`
    - Uses script-relative path resolution for portability
    - Configuration section at top: `TARGET_BOND_TICKER`, `TARGET_BOND_SECURITY`, `RECENT_DATE_PERCENT`, `TOP_N_PAIRS`
- **2025-01-XX XX:XX ET**: Added codebase search requirement:
  - Added rule to "Before Making Changes" section: Always search codebase first to see if existing functions/classes can be used before building new logic
  - Added rule to "Essential Rules" section in Key Takeaways to reinforce this principle
  - Ensures reuse of existing code and prevents unnecessary duplication
- **2025-01-XX XX:XX ET**: Replaced SCM with BNS dealer name throughout runs pipeline:
  - Added SCM to BNS replacement in transform step (before deduplication)
  - Added SCM to BNS replacement in load step (when reading existing data and before writing)
  - Updated ALLOWED_DEALERS to include BNS: ['BMO', 'BNS', 'NBF', 'RBC', 'TD']
  - Updated RUNS_KNOWN_DEALERS to replace SCM with BNS for validation consistency
  - Updated documentation to reflect BNS in allowed dealers list
- **2025-01-XX XX:XX ET**: Reorganized analytics folder structure:
  - Created `analytics/comb/` subfolder for all pair analytics scripts (9 scripts)
  - Created `analytics/runs/` subfolder for runs analytics scripts (2 scripts)
  - Updated all path references in analytics scripts:
    - Parquet paths: `SCRIPT_DIR.parent.parent` (2 levels up to project root)
    - Output directory: `SCRIPT_DIR.parent / "processed_data"` (1 level up to `analytics/processed_data/`)
    - Cross-references: `SCRIPT_DIR.parent / "processed_data" / "*.csv"` (for scripts reading other analytics outputs)
  - All scripts tested and verified to run correctly from new locations
- **2025-01-XX XX:XX ET**: Added Portfolio Pipeline implementation:
  - Created `portfolio_pipeline/` module with extract, transform, load, and pipeline modules
  - Added portfolio configuration to `bond_pipeline/config.py`:
    - `PORTFOLIO_INPUT_DIR` - Input directory path (AD History)
    - `PORTFOLIO_FILE_PATTERN` - Filename pattern regex (Aggies MM.DD.YY.xlsx)
    - `PORTFOLIO_HEADER_ROW` - Header row index (0)
    - `PORTFOLIO_PARQUET` - Output parquet file path
    - `PORTFOLIO_PRIMARY_KEY` - Primary key columns ['Date', 'CUSIP', 'ACCOUNT', 'PORTFOLIO']
  - Added `extract_portfolio_date_from_filename()` function to `bond_pipeline/utils.py`
  - Portfolio extract module:
    - Reads Excel files with header row 0 (row 1 in Excel)
    - Extracts date from filename (Aggies MM.DD.YY.xlsx pattern)
    - Automatically removes Unnamed columns (Unnamed: 0, Unnamed: 1, etc.)
  - Portfolio transform module:
    - CUSIP normalization (same as bond pipeline: uppercase, validate 9 chars)
    - Row filtering: Drops rows where either SECURITY or CUSIP is blank/NaN/empty
    - Deduplication: Date+CUSIP+ACCOUNT+PORTFOLIO primary key (keeps last occurrence)
    - NA cleaning and schema alignment (82 columns, dynamically detected)
  - Portfolio load module:
    - Append/override modes with date checking
    - Date+CUSIP+ACCOUNT+PORTFOLIO primary key enforcement
  - Portfolio pipeline orchestrator:
    - CLI interface matching bond/runs pipeline patterns
    - Enhanced run summaries with statistics
  - Updated `run_pipeline.py`:
    - Added option 3: "Portfolio Pipeline only"
    - Updated option 5: "Individual Parquet Files" (was option 4)
    - Added option 4 in individual parquet files: `historical_portfolio.parquet`
  - Updated `bond_pipeline/utils.py`:
    - Added portfolio statistics section to `log_enhanced_parquet_stats()`:
      - Date range
      - Unique CUSIPs per Date (formatted table)
      - CUSIPs not in universe.parquet (Date, CUSIP, Security) - formatted table
    - Updated `log_parquet_diagnostics()` to include `historical_portfolio.parquet`
  - Output: `historical_portfolio.parquet` with Date+CUSIP+ACCOUNT+PORTFOLIO primary key
  - All 82 columns stored (excluding Unnamed columns)
  - Rows with blank SECURITY or CUSIP automatically filtered out
- **2025-01-XX XX:XX ET**: Created dynamic CR01 pair analytics scripts:
  - Created `analytics/comb/executable cr01 decent bid offer vs holdings.py`:
    - Dynamically loads CR01 holdings CUSIPs from runs_today.csv (CR01 @ Tight Bid > 2000, Bid/Offer>3mm < 3) matched with portfolio (last date)
    - Dynamically loads CR01 universe CUSIPs from runs_today.csv (CR01 @ Wide Offer > 2000, Bid/Offer>3mm < 3) matched with portfolio (last date)
    - Computes all pairwise spreads (universe minus holdings) with statistical metrics
    - Outputs to `executable cr01 decent bid offer vs holdings.csv`
    - Uses script-relative path resolution for portability
  - Created `analytics/comb/executable cr01 vs holdings.py`:
    - Same as above but removes Bid/Offer>3mm filters (only CR01 thresholds)
    - Outputs to `executable cr01 vs holdings.csv`
  - Created `analytics/comb/all combos vs holdings.py`:
    - Loads all CUSIPs from historical_portfolio.parquet (last date) for both universe and holdings
    - No CR01 filters - computes all pairwise combinations within portfolio
    - Outputs to `all combos vs holdings.csv`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Consolidated pair analytics into single Excel output:
  - Created `analytics/comb/comb.py` - consolidated script that runs all 11 pair analytics types
  - Replaced individual CSV file outputs with single `comb.xlsx` Excel file
  - Each analysis type gets its own sheet with formatted Excel tables (filters, banded rows, auto-fitted columns)
  - Tab names use shorter versions (max 31 characters) mapped via `EXCEL_TAB_NAMES` configuration
  - Numeric columns formatted to 1 decimal place
  - Empty analyses automatically skipped (no empty sheets)
  - Still generates `comb.txt` (text summary) and `comb_validation.txt` (validation log) for reference
  - All individual analysis functions updated to return DataFrames instead of writing CSVs
  - Excel tables use `TableStyleMedium9` style with filters and banded rows enabled
- **2025-01-XX XX:XX ET**: Added "xxcy_diff" column to CAD/USD pair analytics:
  - Added `get_cad_equiv_swap_mapping()` helper function to load CAD Equiv Swap values from `historical_bond_details.parquet` (last date)
  - Updated `run_cad_cheap_vs_usd_analysis()` and `run_cad_rich_vs_usd_analysis()` to include "xxcy_diff" column
  - Column calculation: `xxcy_diff = CAD Equiv Swap (cusip_1) - CAD Equiv Swap (cusip_2)`
  - Column positioned immediately after "Last" column in results DataFrame
  - Values set to `None` (NaN) if either CUSIP lacks CAD Equiv Swap data
  - Column formatted to 1 decimal place (via `format_numeric_columns()`)
  - Appears in Excel output (`comb.xlsx`), text summary (`comb.txt`), and console output
- **2025-11-14 18:30 ET**: Enhanced bond pipeline numeric conversion and runs_today.py merge functionality:
  - Extended `convert_years_to_numeric()` in `bond_pipeline/transform.py` to convert spread/metric columns to `float64`:
    - Added: `G Sprd`, `vs BI`, `vs BCE`, `MTD Equity`, `YTD Equity`, `Retracement`, `Z Score`, `Retracement2`
    - These columns are now stored as numeric in `historical_bond_details.parquet` instead of object/string
  - Updated `bond_pipeline/load.py` to convert numeric columns in both append and override modes
  - Added merge functionality to `analytics/runs/runs_today.py`:
    - Merges QUANTITY and POSITION CR01 from `historical_portfolio.parquet` (last date, aggregated by CUSIP)
    - Merges 11 columns from `historical_bond_details.parquet` (most recent available date)
    - CUSIP normalization for matching (runs CUSIPs normalized before merge)
    - Flexible configuration at top of file for easy column management
  - Added comprehensive logging to `runs_today.py`:
    - Creates `bond_data/logs/runs_today.log` with detailed merge statistics
    - Logs CUSIP normalization details and match statistics
    - Logs full `df.info()` output for input parquet and final CSV (verbose mode)
    - Logs column-level match percentages and sample matched/unmatched CUSIPs
    - Logs final DataFrame summary and sample data rows
- **2025-11-15 12:30 ET**: Created and enhanced runs_views.py for custom portfolio table views:
  - Created `analytics/runs/runs_views.py` - custom formatted tables generator from runs_today.csv
  - Generates both text (`portfolio_runs_view.txt`) and Excel (`portfolio_runs_view.xlsx`) outputs
  - Generates 16 portfolio tables with custom formatting:
    1. **Portfolio Sorted By CR01 Risk**: 
       - Filters to rows where QUANTITY > 0
       - Sorts by POSITION CR01 descending (largest to smallest)
       - Shows "Total CR01" summary at top (sum of POSITION CR01 from displayed table rows, calculated after table creation)
       - Includes all 31 columns (Security, QUANTITY, POSITION CR01, Yrs (Cvn), spread metrics, dealer info, CR01 metrics, DoD/MTD/YTD/1yr changes, equity metrics, Retracement, Custom_Sector)
    2. **Portfolio Less Liquid Lines**:
       - Replica of Portfolio Sorted By CR01 Risk table
       - Filters to rows where QUANTITY > 0 and Tight Bid >3mm is blank (NaN/null)
       - Sorts by POSITION CR01 descending (largest to smallest)
       - Shows "Total CR01" summary at top (sum of POSITION CR01 from displayed table rows, calculated after table creation)
       - Includes all 31 columns (same as CR01 Risk table)
    3. **Portfolio Sorted By DoD Bid Chg With >3MM on Bid**:
       - Filters to QUANTITY > 0, TB >3mm has value, and DoD TB>3mm non-zero
       - Sorts by DoD Chg Tight Bid >3mm descending
       - Includes all columns (same as CR01 Risk table)
    4. **Portfolio Sorted By MTD Bid Chg With >3MM on Bid**:
       - Filters to QUANTITY > 0, TB >3mm has value, and MTD TB non-zero
       - Sorts by MTD Chg Tight Bid descending
       - Excludes all DoD columns (uses PORTFOLIO_MTD_COLUMNS)
    5. **Portfolio Sorted By YTD Bid Chg With >3MM on Bid**:
       - Filters to QUANTITY > 0, TB >3mm has value, and YTD TB non-zero
       - Sorts by YTD Chg Tight Bid descending
       - Excludes DoD and MTD columns (uses PORTFOLIO_YTD_COLUMNS)
    6. **Portfolio Sorted By 1yr Bid Chg With >3MM on Bid**:
       - Filters to QUANTITY > 0, TB >3mm has value, and 1yr TB non-zero
       - Sorts by 1yr Chg Tight Bid descending
       - Excludes DoD, MTD, and YTD columns (uses PORTFOLIO_1YR_COLUMNS)
    7. **Size Bids**:
       - Same columns as CR01 Risk table
       - Filters to QUANTITY > 0, CR01 @ Tight Bid >= 1000, POSITION CR01 >= 1,000
       - Sorts by CR01 @ Tight Bid descending
       - Shows "Cumulative CR01 TB" summary at top (sum of CR01 @ Tight Bid)
    8. **Size Bids Struggling Names**:
       - Same filters as Size Bids plus Retracement < 0.5 (Retracement < 50%)
       - Sorts by CR01 @ Tight Bid descending
       - Shows "Cumulative CR01 TB" summary at top
    9. **Size Bids Heavily Offered Lines**:
       - Same filters as Size Bids plus Custom_Sector != "Bail In" and # of Offers >3mm > 2
       - Sorts by CR01 @ Tight Bid descending
       - Shows "Cumulative CR01 TB" summary at top
       - Numeric conversion applied to # of Offers >3mm column before filtering
    10. **Size Bids With Minimal Bid/Offer**:
       - Same filters as Size Bids plus Bid/Offer>3mm <= 3 (excludes > 3 or blank)
       - Sorts by CR01 @ Tight Bid descending
       - Shows "Cumulative CR01 TB" summary at top
    11. **Size Bids With Minimal Bid/Offer No Bail In**:
       - Same filters as Size Bids With Minimal Bid/Offer plus Custom_Sector != "Bail In"
       - Sorts by CR01 @ Tight Bid descending
       - Shows "Cumulative CR01 TB" summary at top
    12-16. **Size Bids With Minimal Bid/Offer: Where {Dealer} Is Best Bid** (5 dealer-specific tables):
       - One table for each unique dealer in "Dealer @ Tight Bid >3mm" (BMO, BNS, NBF, RBC, TD)
       - Same filters as Size Bids With Minimal Bid/Offer plus dealer filter
       - Each table shows "Cumulative CR01 TB" summary at top
       - Table title format: "Size Bids With Minimal Bid/Offer: Where {Dealer} Is Best Bid"
  - Custom formatting applied to all tables:
    - Numeric columns: Thousand separators, no decimals (rounded), except Yrs (Cvn) with 1 decimal
    - Retracement: Percentage format with 2 decimals (multiplied by 100: 1.0 = 100.00%)
    - Missing values: Blank (empty string) instead of "N/A"
    - Center-aligned columns and headers
    - Summary statistics (Total CR01, Cumulative CR01 TB) displayed at top of relevant tables with thousand separators
  - Filter descriptions: Each table includes a "Filters Applied:" section before the table data, listing all filters in bullet format (matching uni_runs_view.txt format)
  - Outputs all tables to `portfolio_runs_view.txt` in `analytics/processed_data/` (text format with center alignment, includes filter descriptions)
  - Generates `portfolio_runs_view.xlsx` Excel file with 16 sheets (one per table):
    - Each sheet contains formatted Excel table with filters and banded rows (TableStyleMedium9)
    - Summary statistics displayed at top of relevant sheets (Total CR01, Cumulative CR01 TB) without merged cells
    - Freeze panes set after Security column (B3 for sheets with summary, B2 for sheets without)
    - Sheet names use full table titles where possible (truncated to 31 chars), dealer tables use short names ("Min BO - {Dealer}")
    - All numeric formatting preserved (thousand separators, Retracement as percentage, Yrs (Cvn) to 1 decimal)
    - Full column names used (not display names)
    - Auto-fitted column widths
  - Header includes refresh timestamp and MTD/YTD reference dates from runs_timeseries.parquet
  - Configuration section at top for column display names and column order (PORTFOLIO_CR01_RISK_COLUMNS, PORTFOLIO_MTD_COLUMNS, PORTFOLIO_YTD_COLUMNS, PORTFOLIO_1YR_COLUMNS)
  - Enhanced `format_table()` function to support optional `summary_dict` parameter for displaying summary statistics at top of tables
  - Added Custom_Sector to runs_today.py merge:
    - Added "Custom_Sector" to BOND_DETAILS_MERGE_COLUMNS in `analytics/runs/runs_today.py`
    - Custom_Sector now merged from historical_bond_details.parquet into runs_today.csv
    - Appears as last column in all portfolio_runs_view.txt tables
- **2025-11-15 18:00 ET**: Added Excel file generation to runs_views.py:
  - Enhanced `analytics/runs/runs_views.py` to generate `portfolio_runs_view.xlsx` alongside text output
  - Created `write_excel_file()` function that writes all tables to separate Excel sheets (16 tables total)
  - Each table formatted as Excel table with filters and banded rows (TableStyleMedium9 style)
  - Summary statistics (Total CR01, Cumulative CR01 TB) written to row 1 without merged cells (to avoid Excel validation issues)
  - Tables start at row 2 (after summary) or row 1 (no summary)
  - Freeze panes implemented after Security column for all sheets
  - Sheet name uniqueness ensured via tracking mechanism (dealer tables use short names: "Min BO - {Dealer}")
  - Invalid Excel sheet name characters sanitized (/, \, ?, *, [, ], :)
  - All formatting preserved (thousand separators, Retracement as percentage, Yrs (Cvn) to 1 decimal)
  - Full column names used in Excel (not display names from COLUMN_DISPLAY_NAMES)
  - Auto-fitted column widths for optimal display
- **2025-11-15 19:00 ET**: Added "Portfolio Less Liquid Lines" table to runs_views.py:
  - Created `create_portfolio_less_liquid_lines_table()` function in `analytics/runs/runs_views.py`
  - Replica of Portfolio Sorted By CR01 Risk table but filters to rows where "Tight Bid >3mm" is blank (NaN/null)
  - Filters to QUANTITY > 0 and Tight Bid >3mm is blank
  - Sorts by POSITION CR01 descending (same as CR01 Risk table)
  - Uses same columns as PORTFOLIO_CR01_RISK_COLUMNS
  - Shows "Total CR01" summary at top (sum of POSITION CR01)
  - Added as Step 3.5 in main() function (right after Portfolio CR01 Risk table)
  - Included in both text output (`portfolio_runs_view.txt`) and Excel output (`portfolio_runs_view.xlsx`)
  - Excel file now contains 16 tables (was 15), with "Portfolio Less Liquid Lines" as Sheet 2
  - Table appears in text output right after "Portfolio Sorted By CR01 Risk" table
- **2025-11-15 19:30 ET**: Added negative spread filtering and fixed DoD Benchmark matching:
  - Added negative spread filtering to `runs_pipeline/load.py`:
    - Filters negative Bid Spread and Ask Spread values (set to NaN) before writing to parquet in both append and override modes
    - Filters negative spreads when reading existing data in append mode
    - Logs count of filtered negative spreads for transparency
  - Added negative spread filtering to `analytics/runs/runs_today.py`:
    - Filters negative Bid Spread and Ask Spread values (set to NaN) during aggregation in `compute_group_metrics()`
    - Prevents negative spreads from being used in "Tight Bid >3mm" and "Wide Offer >3mm" calculations
    - Applied to all spread calculations (Tight Bid, Wide Offer, RBC columns)
  - Fixed DoD calculation Benchmark matching in `analytics/runs/runs_today.py`:
    - Changed all lookups from CUSIP-only to `(CUSIP, Benchmark)` tuple matching
    - Ensures correct pairing when CUSIPs have multiple benchmarks
    - Applied to DoD, MTD, YTD, 1yr, and T-1 dealer lookups
    - Updated filtering logic to match by `(CUSIP, Benchmark)` pairs instead of CUSIP only
    - Added Benchmark column to `compute_group_metrics()` output and aggregated column order
  - Created universe RV views in `analytics/runs/runs_views.py`:
    - Added `generate_universe_views()` function for universe-wide analysis (not filtered by portfolio)
    - Created "Universe Sorted By DoD Moves" table:
      - Filters out excluded Custom_Sector values (Asset Backed Subs, Auto ABS, Bail In, CAD Govt, CASH CAD, CASH USD, CDX, CP, Covered, Dep Note, Financial Hybrid, HY, Non Financial Hybrid, Non Financial Hybrids, USD Govt, Utility)
      - Only includes rows where Tight Bid >3mm had a value on both last date and second-to-last date
      - Excludes rows where DoD Chg Tight Bid >3mm is 0 or blank
      - Shows top 20 and bottom 20 by DoD Chg Tight Bid >3mm (largest positive and most negative values)
      - Uses same formatting as portfolio views (thousand separators, Yrs (Cvn) to 1 decimal, Retracement as percentage)
    - Outputs to `uni_runs_view.txt` and `uni_runs_view.xlsx` in `analytics/processed_data/`
    - Configuration section at top for flexible customization (excluded sectors, top/bottom N, sort column, column list)
- **2025-11-15 20:00 ET**: Fixed Total CR01 calculation in runs_views.py:
  - Updated `main()` function in `analytics/runs/runs_views.py` to calculate Total CR01 from displayed table rows instead of pre-filtered data
  - Total CR01 for "Portfolio Sorted By CR01 Risk" table now sums POSITION CR01 from `portfolio_cr01_df` after table creation (ensures accuracy when CUSIPs appear multiple times with different Benchmarks)
  - Total CR01 for "Portfolio Less Liquid Lines" table now sums POSITION CR01 from `portfolio_less_liquid_df` after table creation
  - Calculation happens after all filtering and column selection, ensuring the summary matches exactly what's displayed in the table
  - "Cumulative CR01 TB" values for Size Bids tables were already correct (calculated from displayed table rows)
- **2025-11-17 19:30 ET**: Added automatic validation and regeneration of runs_today.csv:
  - Created `validate_runs_today_csv_is_current()` function in `analytics/runs/runs_views.py` and `analytics/comb/comb.py`
  - Validates that `runs_today.csv` is current with `runs_timeseries.parquet` by comparing file modification times
  - If parquet file is newer than CSV (or CSV doesn't exist), automatically regenerates CSV by running `runs_today.py`
  - Added validation step to `main()` and `generate_universe_views()` functions in `runs_views.py`
  - Added validation step to `main()` function in `comb.py`
  - Ensures all scripts that use data from `runs_timeseries.parquet` (directly or indirectly via `runs_today.csv`) always use the latest date
  - Prevents stale data issues by auto-regenerating CSV when parquet is updated
  - Provides clear warnings when regeneration occurs, with timestamps of parquet and CSV modification times
  - Uses subprocess to run `runs_today.py` with proper working directory and error handling
- **2025-01-XX XX:XX ET**: Added filter descriptions to portfolio tables in runs_views.py:
  - Added "Filters Applied:" sections before each of the 16 portfolio tables in `portfolio_runs_view.txt` output
  - Matches the format already used in `uni_runs_view.txt` for consistency
  - Each table now displays a clear bulleted list of filters applied:
    - QUANTITY filters
    - CR01 thresholds (CR01 @ Tight Bid >= 1000, POSITION CR01 >= 1,000)
    - Spread filters (Tight Bid >3mm, Bid/Offer>3mm <= 3)
    - Custom_Sector exclusions (Bail In)
    - Dealer-specific filters
    - Change filters (DoD/MTD/YTD/1yr non-zero)
    - Sort order descriptions
  - Uses existing `format_filter_description()` function for consistent formatting
  - Filter descriptions are written before each table with separator lines (`===`)
  - Improves transparency and makes it easier to understand what data is included in each table view
  - Affects all 16 portfolio tables:
    1. Portfolio Sorted By CR01 Risk
    2. Portfolio Less Liquid Lines
    3. Portfolio Sorted By DoD Bid Chg With >3MM on Bid
    4. Portfolio Sorted By MTD Bid Chg With >3MM on Bid
    5. Portfolio Sorted By YTD Bid Chg With >3MM on Bid
    6. Portfolio Sorted By 1yr Bid Chg With >3MM on Bid
    7. Size Bids
    8. Size Bids Struggling Names
    9. Size Bids Heavily Offered Lines
    10. Size Bids With Minimal Bid/Offer
    11. Size Bids With Minimal Bid/Offer No Bail In
    12-16. Size Bids With Minimal Bid/Offer: Where {Dealer} Is Best Bid (5 dealer tables)
- **2025-01-17 20:00 ET**: Consolidated pair analytics scripts into single comb.py file:
  - **IMPORTANT**: All individual pair analytics scripts (all_comb.py, term_comb.py, ticker_comb.py, custom_sector.py, custom_bond_comb.py, custom_bond_vs_holdings.py, port_comb.py, port_executable.py, cad_cheap_vs_usd.py, cad_rich_vs_usd.py, executable cr01 vs holdings.py, executable cr01 decent bid offer vs holdings.py, all combos vs holdings.py) have been consolidated into a single `analytics/comb/comb.py` file
  - The consolidated file contains 11 analysis functions (run_all_comb_analysis, run_term_comb_analysis, run_ticker_comb_analysis, run_custom_sector_analysis, run_custom_bond_comb_analysis, run_custom_bond_vs_holdings_analysis, run_cad_cheap_vs_usd_analysis, run_cad_rich_vs_usd_analysis, run_executable_cr01_vs_holdings_analysis, run_executable_cr01_decent_bid_offer_vs_holdings_analysis, run_all_combos_vs_holdings_analysis)
  - Individual CSV outputs have been replaced with a single Excel file (`comb.xlsx`) containing one sheet per analysis type
  - Text outputs remain: `comb.txt` (summary) and `comb_validation.txt` (validation log)
  - All individual CSV files referenced in earlier change log entries are no longer generated
  - Historical change log entries documenting individual script creation are preserved for reference but scripts no longer exist as separate files

