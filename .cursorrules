# Bond RV App - Cursor AI Rules

**Last Updated**: 2025-01-21 17:15 ET - Added CRITICAL .cursorrules Update Protocol requiring full project structure audit on every update (examine entire project, add/delete/overwrite to match reality, include timestamps, update CHANGELOG)  
**Purpose**: Guide AI assistant in maintaining and extending bond data processing system

> **IMPORTANT**: Always update `.cursorrules` after every applicable change. Override where necessary, delete or change sections so that this file always reflects the most up-to-date project state based on actual codebase. Include date and time stamps for all major changes.
> 
> **CRITICAL: .cursorrules Update Protocol**: Whenever the user asks to update `.cursorrules` (or whenever updating it for any reason), you MUST:
> 1. **Examine the entire current project structure** - Use `list_dir`, `glob_file_search`, and `read_file` tools to audit actual files and folders
> 2. **Compare with documented structure** - Verify what's documented matches what actually exists
> 3. **Add missing items** - Document any files/folders that exist but aren't documented
> 4. **Delete outdated items** - Remove references to files/folders that no longer exist
> 5. **Overwrite incorrect information** - Update any sections that don't match current reality
> 6. **Include date/time stamps** - Add `YYYY-MM-DD HH:MM ET` timestamp to "Last Updated" header and any modified sections
> 7. **Update CHANGELOG.md** - Document the audit/update in changelog with timestamp
> 
> **This is DEFAULT BEHAVIOR** - Always perform a full project structure audit before updating `.cursorrules`, not just incremental updates.
> 
> **CHANGELOG REQUIREMENT**: Whenever you make changes to the codebase that are worth documenting (new features, bug fixes, significant refactoring, schema changes, new scripts, etc.), you MUST also update `CHANGELOG.md` with a new entry including the date/time (ET) and a clear description of what changed. This ensures the project history is properly tracked.
> 
> **DOCUMENTATION REQUIREMENT**: When updating `.cursorrules`, always evaluate whether documentation files need updates. See "Documentation vs .cursorrules Decision Logic" section below for detailed rules and decision process. Always update `CHANGELOG.md` when updating `.cursorrules` for significant changes.

---

## AI Assistant Core Workflow

### General AI Behavior
- **Think deeply** about prompt applicability given end goal and current context
- **Use judgment** when interpreting prompts (e.g., if algo trading prompt mentions volume but raw data only has price, ignore volume requirement)
- **Always notify** user when ignoring requirements or using judgment in interpretation
- **Always ask clarifying questions** in planning mode; keep asking until all possible questions covering virtually any ambiguity are answered
- **Execute all project commands via** `poetry run ...` **(or inside an active** `poetry shell` **) to ensure the Poetry environment is used**

### File Change Summary
- **After finishing requests** that add/edit files, show summary of:
  - What was changed in each file
  - File paths affected
- **Track all modifications** for user review

### .cursorrules Update Protocol (DEFAULT BEHAVIOR)
- **When user requests `.cursorrules` update** (or when updating for any reason), ALWAYS:
  1. **Audit entire project structure** first:
     - Use `list_dir` to examine actual directory contents
     - Use `glob_file_search` to find all files matching patterns
     - Use `read_file` to verify file contents when needed
     - Compare actual structure with documented structure
  2. **Update to match reality**:
     - **Add** missing files/folders that exist but aren't documented
     - **Delete** references to files/folders that no longer exist
     - **Overwrite** incorrect information with accurate current state
     - **Fix** any inconsistencies between documentation and actual codebase
  3. **Include timestamps**:
     - Update "Last Updated" header with `YYYY-MM-DD HH:MM ET` format
     - Add timestamps to modified sections where appropriate
  4. **Update CHANGELOG.md**:
     - Document the audit/update with timestamp
     - Describe what was added/deleted/overwritten
  5. **Verify accuracy**:
     - Ensure directory structure section matches actual project
     - Verify file counts and descriptions are accurate
     - Check that all paths and references are correct
- **This is NOT optional** - Every `.cursorrules` update must include a full project audit

### Change Log Maintenance
- **Update `CHANGELOG.md` for significant changes**: When making changes worth documenting (new features, bug fixes, significant refactoring, schema changes, new scripts, configuration updates, etc.), add an entry to `CHANGELOG.md` with date/time (ET) and a clear description
- **Entry format**: `- **YYYY-MM-DD HH:MM ET**: Brief description of change`
- **What to document**: New features, bug fixes, schema changes, new scripts, configuration changes, significant refactoring, performance optimizations, breaking changes
- **What NOT to document**: Minor typo fixes, formatting-only changes, temporary debugging code

### Documentation vs .cursorrules Decision Logic

**CRITICAL RULE**: When updating `.cursorrules`, you MUST also evaluate whether documentation files need updates. Always follow this decision process:

#### When to Update `.cursorrules` vs Documentation

**`.cursorrules` is for**:
- **AI Assistant Instructions**: Rules, workflows, and guidelines for AI agents working on the codebase
- **Code Patterns**: How code should be structured, formatted, and organized
- **Technical Specifications**: File structures, module organization, naming conventions
- **Execution Workflows**: How to run pipelines, tests, analytics
- **Configuration Details**: Paths, constants, environment setup
- **Current State**: What the codebase currently does (factual, implementation-focused)
- **Quick Reference**: Essential information needed during active development

**Documentation (`documentation/` folder) is for**:
- **Business Logic**: Formulas, calculations, business rules (see `Business_Logic_Reference.md`)
- **System Architecture**: High-level design, data flow, system overview (see `Data_Architecture.md`)
- **User Guides**: Step-by-step procedures for end users
- **Conceptual Explanations**: Why things work the way they do, not just how
- **Comprehensive Reference**: Detailed explanations suitable for human readers
- **Historical Context**: Background information and rationale

#### Decision Process When Updating `.cursorrules`

**Step 1: Identify the Change Type**
- Is this a **code pattern** or **implementation detail**? → `.cursorrules`
- Is this a **business rule** or **calculation formula**? → Documentation
- Is this a **workflow instruction** for AI? → `.cursorrules`
- Is this a **system design** or **architecture**? → Documentation

**Step 2: Determine Audience**
- **AI Assistant** needs to know this to write/maintain code? → `.cursorrules`
- **Human Developer** needs to understand the system? → Documentation
- **Both**? → Update both (see Step 3)

**Step 3: Check for Overlap**
- If change affects **both** AI instructions AND business logic:
  1. Update `.cursorrules` with **implementation details** (what/how)
  2. Update relevant documentation with **business context** (why/what it means)
  3. Keep `.cursorrules` concise and technical
  4. Keep documentation comprehensive and explanatory

**Step 4: Specific Mapping Rules**

| Change Type | `.cursorrules` | Documentation | Both |
|------------|----------------|---------------|------|
| New pipeline module | ✅ Module structure, class names | ✅ Architecture overview | ✅ |
| New calculation/formula | ✅ Column names, data types | ✅ Formula explanation | ✅ |
| New file pattern | ✅ Pattern regex, extraction logic | ❌ | ❌ |
| New business rule | ✅ Implementation details | ✅ Business logic explanation | ✅ |
| Code style change | ✅ Formatting rules | ❌ | ❌ |
| New analytics script | ✅ Script location, inputs/outputs | ✅ Business purpose, formulas | ✅ |
| Schema change | ✅ Column names, types | ✅ Data architecture doc | ✅ |
| Workflow change | ✅ CLI commands, execution steps | ✅ User guide (if needed) | ✅ |
| Configuration change | ✅ Config paths, constants | ❌ | ❌ |

**Step 5: Documentation File Selection**

When updating documentation, choose the appropriate file:

- **`documentation/Business_Logic_Reference.md`**: 
  - Formulas and calculations
  - Business rules and logic
  - Filtering criteria
  - Aggregation methods
  - Change metric definitions

- **`documentation/Data_Architecture.md`**:
  - System architecture changes
  - Data flow modifications
  - Pipeline structure changes
  - Schema changes
  - New data sources or outputs

- **`documentation/Database_Migration_Guide.md`**:
  - Database-related changes only
  - Schema migrations
  - Query examples

- **`documentation/README.md`**:
  - Documentation index updates
  - Navigation changes
  - New documentation files

**Step 6: Update Sequence**

When updating both `.cursorrules` and documentation:

1. **Update `.cursorrules` first** (implementation details)
2. **Then update documentation** (business context)
3. **Update `CHANGELOG.md`** (always, for significant changes)
4. **Update `.cursorrules` "Last Updated" timestamp** (always)

#### Examples

**Example 1: New Spread Calculation**
- **`.cursorrules`**: Add to "Data Pipeline Patterns" → "Processing Rules" → "Spread Calculations" with column names and implementation details
- **`Business_Logic_Reference.md`**: Add to "Spread Calculations" section with formula, definition, and business context
- **`CHANGELOG.md`**: Add entry describing the new calculation

**Example 2: New Pipeline Module**
- **`.cursorrules`**: Add to "Code Organization" → "Module Structure" with file paths and class names
- **`Data_Architecture.md`**: Add to "Data Processing Pipelines" section with architecture overview
- **`CHANGELOG.md`**: Add entry describing the new module

**Example 3: Code Style Change**
- **`.cursorrules`**: Update "Code Style & Conventions" section
- **Documentation**: No update needed (implementation detail only)
- **`CHANGELOG.md`**: Only if significant (e.g., major formatting overhaul)

**Example 4: New Analytics Script**
- **`.cursorrules`**: Add to "File Organization Rules" → "Directory Structure" with script location and purpose
- **`Data_Architecture.md`**: Add to "Analytics and Calculations" section with business purpose
- **`Business_Logic_Reference.md`**: Add formulas if new calculations introduced
- **`CHANGELOG.md`**: Add entry describing the new script

#### Always Update Together

**When updating `.cursorrules`**, always:
1. ✅ Check if `CHANGELOG.md` needs an entry (almost always yes for significant changes)
2. ✅ Check if `Business_Logic_Reference.md` needs updates (if formulas/rules changed)
3. ✅ Check if `Data_Architecture.md` needs updates (if architecture/structure changed)
4. ✅ Update `.cursorrules` "Last Updated" timestamp with date/time (ET)

**When updating documentation**, always:
1. ✅ Check if `.cursorrules` needs corresponding updates (if implementation details changed)
2. ✅ Check if `CHANGELOG.md` needs an entry (if significant change)
3. ✅ Update documentation "Last Updated" timestamp

#### Anti-Patterns to Avoid

❌ **Don't duplicate**: Same information verbatim in both `.cursorrules` and documentation
- `.cursorrules` should be concise and implementation-focused
- Documentation should be comprehensive and explanatory

❌ **Don't skip**: Updating documentation when business logic changes
- If a formula changes, update `Business_Logic_Reference.md`
- If architecture changes, update `Data_Architecture.md`

❌ **Don't forget**: `CHANGELOG.md` when making significant changes
- Always update changelog when updating `.cursorrules` for significant changes
- Changelog entries should reference what changed and why

✅ **Do maintain**: Consistency between `.cursorrules` and documentation
- If `.cursorrules` says "X does Y", documentation should explain "X does Y because..."
- Keep both in sync when changes occur

### Temporary File Cleanup Protocol
- **ALWAYS clean up temporary files** created during chat sessions
- **Before completing tasks**, remove any temporary files created:
  - Migration scripts (`*_migrate*.py`)
  - Restore scripts (`*_restore*.py`)
  - Temporary test files (`*_temp*.py`)
  - Backup files (`.backup`, `*_backup*`)
- **List deleted files** in the final summary
- **Clean up at the END** of each chat session that creates temporary files
- **Common temporary files to remove**:
  - Scripts in `utils/` created for one-time operations
  - Backup parquet files in `bond_data/parquet/`
  - Test output files
  - Temporary data files

### Planning Mode Protocol
1. Ask clarifying questions first
2. Continue asking until ambiguity is eliminated
3. Present plan for approval
4. Execute after confirmation

---

## Project Overview

This is a **modular data engineering pipeline** for processing bond data for relative value trading applications.

### Core Purpose
- Process Excel bond data files into optimized Parquet time-series tables
- Support incremental data loading for daily updates

### Architecture
- **Modular ETL**: Separate modules for Extract → Transform → Load
- **Data Pipeline**: Excel + BQL workbooks → Parquet tables (historical_bond_details, universe, bql)
- **Incremental Loading**: Append mode (new dates only) vs Override mode (rebuild all)
- **Data Integrity**: Primary key enforcement, deduplication, validation

### Industry Best Practices
- **Modular design**: Project must be set up in modular way with correct folder patterns according to industry best practices
- **Single responsibility**: Each module has clear, focused purpose
- **Separation of concerns**: Clear boundaries between extraction, transformation, and loading
- **Configuration management**: All paths, constants, and settings centralized

---

## Technology Stack

### Language & Version
- **Python 3.11+** (strict requirement)
- **Type hints required** for all function signatures
- **Docstrings required** for all modules, classes, and functions

### Key Dependencies
- `pandas>=2.0.0` - Data manipulation
- `pyarrow>=12.0.0` - Parquet I/O
- `openpyxl>=3.1.0` - Excel reading
- `ipykernel>=7.1.0` (dev) - Jupyter kernel integration for notebooks (added 2025-11-08 11:05 ET)
- `nbconvert>=7.16.6` (dev) - Automated notebook execution (added 2025-11-08 11:20 ET)
- `notebook>=7.4.7` (dev) - Local Jupyter server for VS Code kernel selection (added 2025-11-08 11:35 ET)
- `jupyter>=1.1.1` (dev) - Jupyter notebook interface
- `jupyterlab>=4.4.10` (dev) - JupyterLab interface
- `matplotlib>=3.7.0` (dev) - Plotting library
- `pytest>=7.0.0` - Testing framework
- `pytest-cov>=4.0.0` (dev) - Coverage reporting
- `pytest-mock>=3.10.0` (dev) - Mocking utilities
- `ipywidgets` (notebook usage) - Interactive widgets for Jupyter notebooks (used in Untitled-1.ipynb, not in pyproject.toml)
- `beautifulsoup4` (notebook usage) - HTML parsing for notebook table manipulation (used in Untitled-1.ipynb, not in pyproject.toml)

### Environment Management
- **Environment manager**: Poetry (`pyproject.toml`, `poetry.lock`)
- **Virtual environment location**: `.venv/` (created via `poetry config virtualenvs.in-project true`)
- **ALWAYS execute code with** `poetry run ...` or inside `poetry shell`
- **Install dependencies** via `poetry install`

---

## Code Style & Conventions

### Formatting
- **Line length**: 88 characters maximum
- **Indentation**: 4 spaces (no tabs)
- **Imports**: Standard library → Third-party → Local imports (blank lines between groups)
- **Naming**: snake_case for functions/variables, PascalCase for classes, UPPER_CASE for constants

### Documentation
- **Module docstrings**: First line is summary, followed by detailed description if needed
- **Function docstrings**: Use Google style with Args/Returns/Raises sections
- **Type hints**: Use modern Python typing (e.g., `Optional[str]`, `List[pd.DataFrame]`, `Tuple[str, int]`)
- **Date and time stamps**: Include date/time stamps in all documentation entries
- **Keep documentation current**: When creating new documentation, figure out if any old documentation needs updating
- **Version tracking**: Update existing docs when adding new information

### Example Function Signature
```python
def normalize_cusip(cusip: str) -> Optional[str]:
    """
    Normalize CUSIP to uppercase 9-character format.
    
    Args:
        cusip: Raw CUSIP string (may contain lowercase, spaces, extra text)
    
    Returns:
        Normalized CUSIP or None if invalid
    """
```

---

## Code Organization

### Module Structure
```
bond_pipeline/
├── config.py        # Configuration, paths, constants (includes RUNS config)
├── utils.py         # Helper functions (date parsing, validation, logging, runs utilities)
├── extract.py       # Excel file reading and date extraction
├── transform.py     # Data cleaning, normalization, deduplication
├── load.py          # Parquet writing (append/override modes)
├── pipeline.py      # Main orchestration script with CLI
└── README.md        # Module documentation

runs_pipeline/
├── __init__.py      # Package exports
├── extract.py       # RUNS Excel file reading and Date/Time parsing
├── transform.py     # End-of-day deduplication, CUSIP validation/orphan tracking
├── load.py          # Runs parquet writing (append/override modes)
└── pipeline.py      # Main orchestration script with CLI

portfolio_pipeline/
├── __init__.py      # Package exports
├── extract.py       # Portfolio Excel file reading and date extraction from filename
├── transform.py     # CUSIP normalization, NA cleaning, deduplication (Date+CUSIP+ACCOUNT+PORTFOLIO)
├── load.py          # Portfolio parquet writing (append/override modes)
└── pipeline.py      # Main orchestration script with CLI
```

### Design Principles
1. **Single Responsibility**: Each module has one clear purpose
2. **Dependency Injection**: Pass loggers, configs as constructor arguments
3. **Separation of Concerns**: Extract (read) → Transform (clean) → Load (write)
4. **Configuration Centralization**: All paths and constants in `config.py`
5. **Immutable Config**: Don't modify config values at runtime

### Class-Based Architecture
Each pipeline stage is a class with clear responsibilities:

**Bond Pipeline:**
- **ExcelExtractor**: File discovery, Excel reading, date extraction
- **DataTransformer**: Schema alignment, CUSIP normalization, NA cleaning, deduplication
- **ParquetLoader**: Append/override modes, date checking, universe creation
- **BondDataPipeline**: Orchestrates all components, CLI interface

**Runs Pipeline:**
- **RunsExtractor**: RUNS Excel reading, Date/Time parsing (from columns, not filename)
- **RunsTransformer**: End-of-day deduplication (vectorized, ~100x faster), CUSIP validation/orphan tracking (vectorized), schema alignment
– **RunsLoader**: Runs parquet writing, Date+Dealer+CUSIP primary key enforcement, granular load metrics (new rows/dates/CUSIPs/Dealers), spread outlier filtering (filters rows outside [low - 20, high + 20] range based on Date+CUSIP group statistics)
– **RunsDataPipeline**: Orchestrates all components, CLI interface, append-mode file selection + enhanced run summaries

**Portfolio Pipeline:**
- **PortfolioExtractor**: Portfolio Excel reading, date extraction from filename (Aggies MM.DD.YY.xlsx pattern), removes Unnamed columns
- **PortfolioTransformer**: CUSIP normalization (same as bond pipeline), row filtering (drops rows with blank SECURITY or CUSIP), NA cleaning, deduplication (Date+CUSIP+ACCOUNT+PORTFOLIO)
- **PortfolioLoader**: Portfolio parquet writing, Date+CUSIP+ACCOUNT+PORTFOLIO primary key enforcement, append/override modes
- **PortfolioDataPipeline**: Orchestrates all components, CLI interface, enhanced run summaries

**Pipeline Orchestrator:**
- **run_pipeline.py**: Unified entry point for bond, runs, and portfolio pipelines
- User can select: Bond only, Runs only, Portfolio only, Both Bond and Runs, or Individual Parquet Files
- Individual Parquet Files option allows regenerating specific parquet outputs:
  - `historical_bond_details.parquet` (also regenerates universe)
  - `bql.parquet` (standalone BQL processing)
  - `runs_timeseries.parquet` (standalone runs processing)
  - `historical_portfolio.parquet` (standalone portfolio processing)
- Shared mode selection (append/override) for selected pipelines
- After successful runs, generates `bond_data/logs/parquet_stats.log` with dataset diagnostics (df.info/head/tail)

---

## Data Pipeline Patterns

### Bond Pipeline Input Data Format
- **Excel files**: Pattern `API MM.DD.YY.xlsx` (e.g., `API 10.20.25.xlsx`)
- **Header row**: Row 3 (index 2, 0-based)
- **Columns**: 75 columns in latest files (schema evolved from 59)
- **NA values**: `#N/A Field Not Applicable`, `#N/A Invalid Security`, etc.

### BQL Workbook Input Data Format
- **Excel file**: Fixed workbook `bql.xlsx` located in Support Files directory
- **Sheet**: `bql`
- **Multi-index header**: 4-row column header structure:
  - Row 0 (index 0): Ignored
  - Row 1 (index 1): Security names (1st level)
  - Row 2 (index 2): CUSIPs (2nd level)
  - Row 3 (index 3): Ignored
  - Row 4 (index 4): Data rows start here
- **Column 0**: First column labelled `CUSIPs` contains daily timestamps
- **Header processing**: Extract names from level 1 (row 1) and CUSIPs from level 2 (row 2) of multi-index columns
- **CUSIP cleaning**: Strip whitespace, remove trailing `Corp`, enforce 9-character uppercase CUSIP
- **Output expectation**: Long-form DataFrame with columns `Date`, `Name`, `CUSIP`, `Value`

### Runs Pipeline Input Data Format
- **Excel files**: Pattern `RUNS MM.DD.YY.xlsx` (e.g., `RUNS 10.31.25.xlsx`)
- **Header row**: Row 1 (index 0, 0-based)
- **Columns**: 30 columns in latest files (schema evolved from 28 in one old file)
- **Date/Time**: Date and Time come from columns (not filename)
  - Date: MM/DD/YY format (e.g., "10/31/25") → parsed to datetime object
  - Time: HH:MM format (e.g., "15:45") → parsed to datetime.time object
- **NA values**: Same as bond pipeline (`#N/A Field Not Applicable`, `#N/A Invalid Security`, etc.)

### Portfolio Pipeline Input Data Format
- **Excel files**: Pattern `Aggies MM.DD.YY.xlsx` (e.g., `Aggies 11.04.25.xlsx`)
- **Header row**: Row 1 (index 0, 0-based)
- **Columns**: 82 columns (schema detected dynamically from latest file)
- **Date extraction**: Date extracted from filename (MM/DD/YYYY format)
- **Column filtering**: Unnamed columns (Unnamed: 0, Unnamed: 1, etc.) are automatically removed during extraction
- **Row filtering**: Rows with blank SECURITY or CUSIP are dropped during transformation
- **NA values**: Same as bond pipeline (`#N/A Field Not Applicable`, `#N/A Invalid Security`, etc.)

### Output Data Format

#### `historical_bond_details.parquet`
- **Purpose**: Time series of all bonds over time
- **Primary Key**: `Date + CUSIP` (unique combination)
- **Schema**: All 75 columns + `Date` column (first position)
- **Row count**: ~25,000+ rows spanning 2023-2025
- **Storage**: `bond_data/parquet/historical_bond_details.parquet`

#### `universe.parquet`
- **Purpose**: Current universe of all unique CUSIPs ever seen
- **Primary Key**: `CUSIP` (unique)
- **Schema**: 13 key columns only
  1. CUSIP, 2. Benchmark Cusip, 3. Custom_Sector, 4. Bloomberg Cusip, 5. Security,
  6. Benchmark, 7. Pricing Date, 8. Pricing Date (Bench), 9. Worst Date,
  10. Yrs (Worst), 11. Ticker, 12. Currency, 13. Equity Ticker
- **Update strategy**: Always rebuild from `historical_bond_details` (most recent date per CUSIP)
- **Storage**: `bond_data/parquet/universe.parquet`

#### `bql.parquet`
- **Purpose**: Long-form Bloomberg query spreads dataset
- **Primary Key**: `Date + CUSIP`
- **Schema**: Columns `Date`, `Name`, `CUSIP`, `Value`
- **Name mapping**: Derived from workbook header row; used for orphan logging
- **Storage**: `bond_data/parquet/bql.parquet`
- **Logging**: Warn when `bql` CUSIPs are missing from `universe.parquet`

#### `runs_timeseries.parquet`
- **Purpose**: Time series of dealer quotes over time (end-of-day snapshots)
- **Primary Key**: `Date + Dealer + CUSIP` (unique combination, enforced after deduplication)
- **Schema**: All 30 columns with Date and Time as first columns
- **Dealer Filtering**: Only includes dealers: BMO, BNS, NBF, RBC, TD (other dealers filtered out during load)
- **Data Quality**: 
  - Negative Bid Spread and Ask Spread values are filtered out (set to NaN) before writing to parquet and during aggregation
  - **Outlier Spread Filtering**: Rows with Bid Spread or Ask Spread values outside `[low - 20, high + 20]` range (based on Date+CUSIP group statistics) are dropped before writing to parquet
  - Outlier filtering excludes CUSIPs with Custom_Sector values: "Non Financial Hybrid", "Non Financial Hybrids", "Financial Hybrid", "HY"
  - Outlier calculation excludes the current row's value from group statistics to prevent self-skewing
  - Both positive and negative outliers are filtered (not just negative values)
- **Row count**: ~19,000+ rows (after deduplication, dealer filtering, and outlier filtering) spanning 2022-2025
- **Storage**: `bond_data/parquet/runs_timeseries.parquet`
- **Deduplication**: Keep latest Time per Date+Dealer+CUSIP (end-of-day snapshot)

#### `historical_portfolio.parquet`
- **Purpose**: Time series of portfolio holdings over time
- **Primary Key**: `Date + CUSIP + ACCOUNT + PORTFOLIO` (unique combination, preserves account/portfolio detail)
- **Schema**: All 82 columns (excluding Unnamed columns) with Date column first
- **Column filtering**: Unnamed columns (Unnamed: 0, Unnamed: 1, etc.) are automatically removed
- **Row filtering**: Rows with blank SECURITY or CUSIP are automatically dropped
- **Row count**: Varies based on portfolio holdings across dates
- **Storage**: `bond_data/parquet/historical_portfolio.parquet`
- **Deduplication**: Keep last occurrence per Date+CUSIP+ACCOUNT+PORTFOLIO

### Processing Rules

#### Date Handling
- Extract from filename pattern: `API MM.DD.YY.xlsx` → `datetime(YYYY-MM-DD)`
- Store as `datetime64` in parquet (Date column first)
- Append mode: Skip dates already in parquet file
- Override mode: Delete existing parquet files, rebuild from scratch

#### CUSIP Normalization & Validation

**Bond Pipeline:**
- Convert to uppercase: `89678zab2` → `89678ZAB2`
- Remove extra text: ` 06418GAD9 Corp` → `06418GAD9`
- Validate length: Must be exactly 9 characters
- Invalid CUSIPs: Log warning but include in data
- Primary key enforcement: No duplicate Date+CUSIP combinations

**Runs Pipeline:**
- No normalization: Keep CUSIPs as-is from Excel (no uppercase conversion, no text removal)
- Validate length: Check if 9 characters using vectorized operations (but don't normalize)
- **Optimized**: Vectorized CUSIP validation (~100x faster than row-by-row iteration)
- Invalid CUSIPs: Log summary warning but include in data (same as bond pipeline)
- **Enhanced Orphan Tracking**: Log orphans with context (Security, Date, Dealer, Time, Ticker) to validation.log
- **Dealer Filtering**: Only dealers BMO, BNS, NBF, RBC, TD are stored in `runs_timeseries.parquet` (other dealers filtered out during load)
- **Negative Spread Filtering**: Negative Bid Spread and Ask Spread values are filtered out (set to NaN) before writing to parquet and during aggregation (data quality issue)
- **Outlier Spread Filtering**: Implemented in `runs_pipeline/load.py` via `filter_outlier_spreads()` method:
  - Groups data by Date+CUSIP
  - For each group, calculates high/low of Bid Spread and Ask Spread from valid positive values (excluding current row to prevent self-skewing)
  - Drops rows where Bid Spread or Ask Spread is outside `[low - 20, high + 20]` range (filters both positive and negative outliers)
  - Excludes CUSIPs with Custom_Sector values: "Non Financial Hybrid", "Non Financial Hybrids", "Financial Hybrid", "HY"
  - Logs dropped row count and detailed information (Date, CUSIP, Dealer, Security, Time, reason) for first 50 dropped rows
  - Runs after deduplication but before writing to parquet (in both append and override modes)
- Primary key enforcement: No duplicate Date+Dealer+CUSIP combinations (after deduplication)
- Append mode inspects existing parquet dates, skips fully-loaded files, and filters mixed files down to unseen dates before extraction.
- Pipeline summaries surface files processed/skipped plus new rows, dates, CUSIPs, and dealers added during each run.

**Portfolio Pipeline:**
- CUSIP normalization: Same as bond pipeline (uppercase, remove extra text, validate 9 characters)
- Row filtering: Drop rows where either SECURITY or CUSIP is blank/NaN/empty (done first in transformation)
- Column filtering: Remove all "Unnamed:" columns (Unnamed: 0, Unnamed: 1, etc.) during extraction
- Invalid CUSIPs: Log warning but include in data (same as bond pipeline)
- Primary key enforcement: No duplicate Date+CUSIP+ACCOUNT+PORTFOLIO combinations
- Schema evolution: Master schema (82 columns) detected dynamically from latest file
- Append mode: Skip dates already in parquet file
- Override mode: Delete existing parquet file, rebuild from scratch

#### Schema Evolution
- **Master schema**: 75 columns from latest files (set dynamically)
- **Older files**: Fill missing columns with `NULL/NaN`
- **Column order**: Preserve order from master schema
- **Date column**: Always first column in all DataFrames

#### Deduplication

**Bond Pipeline:**
- Within file: Keep last occurrence of duplicate CUSIPs
- Across files: One record per Date+CUSIP combination
- Log all duplicates to `bond_data/logs/duplicates.log`

**Runs Pipeline:**
- End-of-day snapshots: Keep latest Time per Date+Dealer+CUSIP (most recent quote of day)
- Same-time tiebreaker: If multiple rows have same latest Time, keep last row by position
- **Optimized**: Uses vectorized sort + drop_duplicates (O(n log n)) instead of row-by-row iteration (O(n*groups))
- Within file and across files: One record per Date+Dealer+CUSIP combination
- Log summary of duplicates to `bond_data/logs/duplicates.log` (combined with bond pipeline logs)

**Portfolio Pipeline:**
- Within file: Keep last occurrence of duplicate Date+CUSIP+ACCOUNT+PORTFOLIO combinations
- Across files: One record per Date+CUSIP+ACCOUNT+PORTFOLIO combination
- Log duplicates to `bond_data/logs/duplicates.log` (combined with bond and runs pipeline logs)

#### Data Cleaning
- Convert NA values to NULL: `#N/A Field Not Applicable`, `#N/A Invalid Security`, `N/A`, etc.
- Empty cells → NULL/NaN
- **Negative Spread Filtering**: Negative Bid Spread and Ask Spread values are filtered out (set to NaN) in runs pipeline (both parquet storage and aggregation) - these are data quality issues
- **Outlier Spread Filtering** (Runs Pipeline only):
  - Implemented in `runs_pipeline/load.py` via `filter_outlier_spreads()` method
  - Groups data by Date+CUSIP before writing to parquet
  - For each group, calculates high/low from valid positive Bid Spread and Ask Spread values (excluding current row)
  - Drops rows where Bid Spread or Ask Spread is outside `[low - 20, high + 20]` range
  - Filters both positive and negative outliers
  - Excludes CUSIPs with Custom_Sector: "Non Financial Hybrid", "Non Financial Hybrids", "Financial Hybrid", "HY"
  - Runs after deduplication but before writing to parquet
  - Logs dropped row details (Date, CUSIP, Dealer, Security, Time, reason) for first 50 rows
- Preserve all other values as-is

#### Numeric Column Conversion
- **Years columns**: `Yrs Since Issue`, `Yrs (Worst)`, and `Yrs (Cvn)` are automatically converted to numeric (`float64`) during transformation
- **Spread and metric columns**: `G Sprd`, `vs BI`, `vs BCE`, `MTD Equity`, `YTD Equity`, `Retracement`, `Z Score`, `Retracement2` are automatically converted to numeric (`float64`) during transformation
- **Conversion points**: 
  - Transform step: Converts during `transform_single_file()` processing via `convert_years_to_numeric()` method
  - Load step (append): Converts new data and existing data for compatibility
  - Load step (override): Converts all data before writing
- **Result**: These columns are always stored as numeric (`float64`) in parquet files, enabling proper binning, statistical analysis, and accurate merging with other datasets
- **Error handling**: Non-numeric values are coerced to NaN (errors='coerce')

#### BQL Pipeline
- Read Excel workbook with 4-row multi-index header (`header=[0,1,2,3]`)
- Extract security names from level 1 (row 1) and CUSIPs from level 2 (row 2) of multi-index columns
- Normalize header values to derive security names and enforce 9-character CUSIPs
- Reshape wide workbook to long-form dataset (`Date`, `Name`, `CUSIP`, `Value`)
- Skip duplicate/invalid CUSIP columns and log issues for traceability
- Drop rows with non-numeric values before persisting
- Compare resulting CUSIPs against `universe.parquet` and log orphan details (CUSIP + Name)

---

## Logging Standards

### Log File Organization
```
bond_data/logs/
├── archive/            # Archived log files (rotated after N runs, git-ignored)
├── processing.log      # File extraction and loading operations
├── duplicates.log      # All duplicate CUSIPs detected
├── validation.log      # CUSIP validation warnings
├── summary.log         # Pipeline execution summary (run headers, stats)
├── parquet_stats.log   # Snapshot diagnostics for each parquet dataset (df.info/head/tail)
├── runs_today.log      # Runs today analytics execution log (merge statistics, df.info, matching details)
├── test_dupes.log      # Test duplicate logs (git-ignored)
└── test_valid.log      # Test validation logs (git-ignored)
```

### Logging Levels
- **DEBUG**: Detailed diagnostic info (file-by-file progress)
- **INFO**: General informational messages
- **WARNING**: CUSIP validation issues, schema mismatches
- **ERROR**: File read failures, data corruption
- **CRITICAL**: System failures

### Logging Best Practices
1. **Dual logging**: File logging (detailed) + Console logging (essential only)
2. **No console spam**: Suppress detailed logs on console (console_level=logging.CRITICAL)
3. **Run metadata**: Track run_id, timestamp, mode for each execution
4. **Log rotation**: Archive logs after N runs (currently 10)
5. **Structured info**: Include Date, CUSIP, file path in log messages
6. **BQL ingestion**: Log unique CUSIP/date counts and orphan CUSIPs with security names
7. **Unicode encoding**: File handlers use UTF-8 encoding with `errors='replace'` to handle Unicode characters in bond names
8. **ASCII-safe logging**: Use `sanitize_log_message()` function to convert Unicode characters to ASCII-safe replacements before logging (prevents Windows console encoding errors)

### Example Logging Pattern
```python
# Setup logger (file only, no console)
self.logger = setup_logging(LOG_FILE_PROCESSING, 'extract', console_level=logging.CRITICAL)

# Log file processing
self.logger.info(f"Processing file: {filename}")
self.logger.info(f"Extracted {len(df)} rows for date {date_obj}")

# Log validation issues
self.logger.warning(f"Invalid CUSIP: {cusip} (length {len(cusip)})")

# Log duplicates
self.logger_dupes.info(f"Duplicate Date+CUSIP: {date} - {cusip} - {bond_name} (kept last occurrence)")
```

---

## Testing Standards

### Test Structure
```
tests/
├── conftest.py              # Shared fixtures (sample data, mocks)
├── unit/                    # Unit tests
│   ├── test_utils.py       # ✅ Bond utility & BQL helper tests
│   ├── test_utils_runs.py  # ✅ Runs utility tests
│   ├── test_extract_runs.py # ✅ Runs extract tests
│   ├── test_transform_runs.py # ✅ Runs transform tests
│   ├── test_load_runs.py   # ✅ Runs load tests
│   ├── test_extract.py     # TODO
│   ├── test_transform.py   # TODO
│   └── test_load.py        # TODO
├── integration/             # Integration tests
│   ├── test_pipeline_runs.py # ✅ Runs pipeline integration tests
│   └── test_run_pipeline.py  # ✅ Unified pipeline orchestrator tests
└── patterns/                 # Pattern analysis and data exploration scripts (not production tests)
    ├── analyze_runs_data.py
    ├── data_analysis_and_questions.md
    └── deep_duplicate_analysis.py
```

### Testing Requirements
1. **Coverage**: Target 85%+ overall, 90%+ for critical modules
2. **Test location**: All tests in `tests/` directories within each module
3. **Naming**: Test files follow naming convention: `test_*.py`
4. **Framework**: Use pytest for testing
5. **Fixtures**: Reusable test data in `conftest.py`
6. **Isolation**: Tests must be independent (no shared state)
7. **Mocking**: Mock all file I/O, external dependencies
8. **Speed**: Full test suite should run in < 30 seconds

### Test Categories
- **Date parsing**: Valid/invalid formats, leap years, edge cases
- **CUSIP validation**: Valid/invalid lengths, characters, normalization
- **NA cleaning**: Standard patterns, mixed DataFrames
- **Schema alignment**: Old (59) vs new (75) column schemas
- **Deduplication**: Within file, across files, Date+CUSIP combinations
- **Append mode**: Skip existing dates, append new dates
- **Override mode**: Delete existing, rebuild from scratch
- **BQL ingestion**: Header normalization, wide→long reshape, orphan logging against universe

### Running Tests
```bash
# Install dependencies (first run or after updates)
poetry install

# Run all tests
poetry run pytest

# Run specific file
poetry run pytest tests/unit/test_utils.py

# Run with coverage
poetry run pytest --cov=bond_pipeline --cov-report=html

# Run specific test
poetry run pytest tests/unit/test_utils.py::TestCUSIPValidation::test_validate_cusip_valid
```

---

## File Organization Rules

### Directory Structure
```
Bond-RV-App/
├── analytics/             # Analytics scripts and processed outputs
│   ├── comb/             # Pair analytics scripts
│   │   └── comb.py           # Consolidated pair analytics script (contains 11 analysis functions: all_comb, term_comb, ticker_comb, custom_sector, custom_bond_comb, custom_bond_vs_holdings, cad_cheap_vs_usd, cad_rich_vs_usd, executable_cr01_vs_holdings, executable_cr01_decent_bid_offer_vs_holdings, all_combos_vs_holdings; outputs single Excel file with multiple sheets, plus comb.txt, comb_validation.txt, and all_combinations.csv with all pairs from all_comb analysis) (updated 2025-01-21 16:30 ET)
│   ├── runs/             # Runs analytics scripts
│   │   ├── runs_today.py     # Runs today analytics (reads parquet directly, aggregates required dates, computes DoD/MTD/YTD/Custom Date changes, filters negative spreads, matches by (CUSIP, Benchmark) tuples)
│   │   └── runs_views.py     # Custom formatted tables from runs_today.csv (portfolio and universe views with custom formatting, generates both .txt and .xlsx outputs, includes "Filters Applied:" sections for all tables, auto-validates and regenerates CSV if stale; column order: DoD TB>3mm and DoD WO>3mm appear after B/O column; Portfolio DoD Offer Chg table sorts by DoD WO>3mm; Universe DoD Moves table (first table) shows all rows sorted by DoD WO>3mm with 50 rows in .txt, all rows in Excel; entry point calls both main() for portfolio views and generate_universe_views() for universe views)
│   └── processed_data/   # Outputs from analytics scripts
│       ├── comb.xlsx         # Consolidated Excel file with all pair analytics (from comb.py, formatted tables on separate sheets, one sheet per analysis type)
│       ├── comb.txt          # Text summary of pair analytics results (from comb.py)
│       ├── comb_validation.txt  # Validation log for pair analytics (from comb.py)
│       ├── all_combinations.csv  # All pairs from All Combinations analysis (from comb.py, contains all rows not just top 80, sorted by Z Score descending, includes CAD CUSIPs only after recent date filtering)
│       ├── runs_today.csv    # Daily runs analytics output (from runs_today.py, used by runs_views.py and comb.py)
│       ├── portfolio_runs_view.txt  # Formatted portfolio tables (from runs_views.py, text output, 16 tables)
│       ├── portfolio_runs_view.xlsx  # Excel file with 16 portfolio tables (from runs_views.py, formatted Excel tables on separate sheets)
│       ├── uni_runs_view.txt  # Formatted universe tables (from runs_views.py, text output, 16 tables including new "Universe Sorted By DoD Moves" as first table with 50 rows)
│       └── uni_runs_view.xlsx  # Excel file with universe tables (from runs_views.py, formatted Excel tables on separate sheets, 16 tables including new "Universe Sorted By DoD Moves" as first table with all rows)
├── bond_pipeline/          # Bond pipeline code (8 files: __init__, config, utils, extract, transform, load, pipeline, README.md)
│   ├── __init__.py
│   ├── config.py
│   ├── utils.py
│   ├── extract.py
│   ├── transform.py
│   ├── load.py
│   ├── pipeline.py
│   └── README.md           # Bond pipeline module documentation
├── runs_pipeline/          # Runs pipeline code (5 files: __init__, extract, transform, load, pipeline)
│   ├── __init__.py
│   ├── extract.py
│   ├── transform.py
│   ├── load.py
│   └── pipeline.py
├── portfolio_pipeline/     # Portfolio pipeline code (5 files: __init__, extract, transform, load, pipeline)
│   ├── __init__.py
│   ├── extract.py
│   ├── transform.py
│   ├── load.py
│   └── pipeline.py
├── bond_data/              # Data directory (local only, git-ignored)
│   ├── parquet/           # Output parquet files
│   │   ├── historical_bond_details.parquet
│   │   ├── universe.parquet
│   │   ├── bql.parquet
│   │   ├── runs_timeseries.parquet
│   │   └── historical_portfolio.parquet
│   └── logs/              # Processing logs
│       ├── archive/       # Archived log files (git-ignored)
│       ├── processing.log
│       ├── duplicates.log
│       ├── validation.log
│       ├── summary.log
│       ├── parquet_stats.log
│       ├── runs_today.log
│       ├── test_dupes.log  # Test logs (git-ignored)
│       └── test_valid.log  # Test logs (git-ignored)
├── .venv/                 # Poetry-managed virtual environment (git-ignored)
├── documentation/         # Complete documentation (lowercase folder name) (updated 2025-01-21 16:30 ET)
│   ├── Business_Logic_Reference.md  # Business logic formulas and calculations
│   ├── Data_Architecture.md         # System architecture and data flow
│   ├── Database_Migration_Guide.md  # SQLite migration guide
│   └── README.md                    # Documentation index
├── Raw Data/              # Excel files (input)
├── References and Documentation/  # Ignored by Cursor
├── tests/                 # Test suite
│   ├── __init__.py
│   ├── conftest.py        # Shared fixtures (sample data, mocks)
│   ├── README.md          # Test suite documentation
│   ├── unit/              # Unit tests
│   │   ├── __init__.py
│   │   ├── test_utils.py       # ✅ Bond utility & BQL helper tests
│   │   ├── test_utils_runs.py  # ✅ Runs utility tests
│   │   ├── test_extract_runs.py # ✅ Runs extract tests
│   │   ├── test_transform_runs.py # ✅ Runs transform tests
│   │   ├── test_load_runs.py   # ✅ Runs load tests
│   │   ├── test_extract.py     # TODO
│   │   ├── test_transform.py   # TODO
│   │   └── test_load.py        # TODO
│   ├── integration/       # Integration tests
│   │   ├── __init__.py
│   │   ├── test_pipeline_runs.py # ✅ Runs pipeline integration tests
│   │   └── test_run_pipeline.py  # ✅ Unified pipeline orchestrator tests
│   └── patterns/          # Pattern analysis and data exploration scripts (not production tests)
│       ├── analyze_runs_data.py
│       ├── data_analysis_and_questions.md
│       └── deep_duplicate_analysis.py
├── utils/                 # Utility scripts (currently empty, reserved for future utility scripts)
├── hi eddy.ipynb          # Interactive Jupyter notebook with ipywidgets for data exploration (dark mode, sortable/filterable tables with range sliders, column selector panel)
├── .cursorrules           # This file (AI assistant rules and project structure)
├── .cursorignore          # Files to ignore by Cursor AI
├── .gitignore             # Files to ignore by Git
├── CHANGELOG.md           # Project change history (updated with significant changes)
├── pyproject.toml         # Poetry project configuration
├── poetry.lock            # Locked dependency versions
├── requirements.txt       # Legacy dependency snapshot (reference only)
├── README.md              # Project overview (references non-existent Documentation/Workflows/ - needs update)
├── run_pipeline.py        # Unified pipeline orchestrator (bond, runs, portfolio, individual parquet files)
└── hi eddy.ipynb          # Interactive Jupyter notebook with ipywidgets for data exploration (dark mode, sortable/filterable tables with range sliders, column selector panel)
```

### File Naming Conventions
- **Modules**: `snake_case.py` (e.g., `bond_pipeline/utils.py`)
- **Test files**: `test_*.py` (e.g., `tests/unit/test_utils.py`)
- **Config files**: `.cursorrules`, `.cursorignore`, `pytest.ini`
- **Data files**: `historical_bond_details.parquet`, `universe.parquet`

### Important Paths
- **Project root**: `PROJECT_ROOT = Path(__file__).parent.parent` (from config.py)
- **Data directory**: `bond_data/` (git-ignored)
- **Parquet files**: `bond_data/parquet/` (git-tracked)
- **Logs**: `bond_data/logs/` (git-ignored)
- **Default input**: Dropbox folder (Windows-specific path in config.py)

### Path Handling Rules
- **Use `pathlib.Path`** for all path operations
- **Scripts must detect their location** and adjust paths accordingly
- **Support running from multiple directory locations** (absolute and relative paths)
- **Avoid hardcoded paths**: Use relative paths with config-based resolution
- **Analytics script path resolution**:
  - Scripts in `analytics/comb/` and `analytics/runs/` use `SCRIPT_DIR = Path(__file__).parent.resolve()`
  - Parquet files: `SCRIPT_DIR.parent.parent / "bond_data" / "parquet" / "*.parquet"` (goes up 2 levels to project root)
  - Output directory: `SCRIPT_DIR.parent / "processed_data"` (goes up 1 level to `analytics/processed_data/`)
  - Cross-references: `SCRIPT_DIR.parent / "processed_data" / "*.csv"` (for scripts reading other analytics outputs)

---

## Git & Version Control

### Git-Ignored Files
- Excel files: `*.xlsx`, `*.xls`
- Log files: `bond_data/logs/*.log`
- Virtual environment: `.venv/`
- Python cache: `__pycache__/`, `*.pyc`
- Test output: `tests/test_output/`
- References folder: `References and Documentation/` (in .cursorignore)

### Git-Tracked Files
- Parquet files: `bond_data/parquet/*.parquet` (processed data)
- Source code: `bond_pipeline/`, `runs_pipeline/`, `portfolio_pipeline/`, `analytics/`
- Tests: `tests/`
- Documentation: `documentation/` (lowercase folder name)
- Config: `pyproject.toml`, `poetry.lock`, `requirements.txt`, `.cursorrules`, `.cursorignore`
- Scripts: `run_pipeline.py`
- Notebooks: `hi eddy.ipynb` (interactive exploration notebook)

### Git Commit Protocol
- **Update `.cursorrules`** before every git commit
- **Verify file timestamps** and version consistency
- **Document changes** in commit messages with reference to `.cursorrules` updates

### Commit Message Generation
- **When user requests a commit message**, follow this process:
  1. **Examine the last commit**: Use `git log -1` or `git show HEAD` to understand what was in the previous commit (message, files changed, scope)
  2. **Analyze all changes since last commit**: Use `git status`, `git diff`, and `git diff --cached` to identify:
     - All modified files and their changes
     - All new files added
     - All deleted files
     - File paths and line-level changes where relevant
  3. **Generate detailed commit message**:
     - **Summary line**: Brief description (50-72 chars) of the main change
     - **Detailed body**: 
       - List all files modified/added/deleted with brief descriptions
       - Explain the purpose and impact of changes
       - Reference any related issues, features, or refactoring
       - Note any breaking changes or important considerations
     - **Format**: Use conventional commit format when appropriate (feat:, fix:, refactor:, docs:, etc.)
     - **Context**: Connect changes to the previous commit to show progression
     - **Detail level**: Provide enough context that someone reviewing the commit later understands what changed and why

### Unicode & Encoding
- **Avoid unicode encoding issues** for Windows
- **Use ASCII-compatible characters** when needed
- **Test paths** on Windows file system before committing

---

## Code Modification Guidelines

### Before Making Changes
1. **Search codebase first**: Always search within the codebase to see if any existing functions and/or classes can be used before building new logic
2. **Analyze entire codebase**: Understand dependencies and relationships
3. **Map file impacts**: Identify ALL files that need modification
4. **Check existing logic**: Extend or refactor before creating new code
5. **Consider ripple effects**: Changes impact downstream modules

### Coding Principles
1. **Extend existing logic**: Never create new files when existing code can be extended
2. **Refactor first**: Improve existing structure before adding new features
3. **Minimal changes**: Make smallest change that achieves the goal
4. **Backward compatibility**: Don't break existing functionality
5. **Documentation**: Update docstrings when modifying functions

### File Creation Rules
- **List new files**: Announce file name and path before creating
- **Test files**: Always create in `tests/` folder when applicable
- **Avoid duplication**: Check if existing modules can handle the requirement
- **Justification**: Explain why new file is needed vs extending existing
- **Clean up temporary files**: Remove any migration/restore/cleanup scripts after use

### When AI Assistant Helps
1. **Read existing code**: Always read relevant files before suggesting changes
2. **Show context**: Reference existing code with `startLine:endLine:filepath` format
3. **Propose edits**: Suggest specific changes with line-by-line diffs
4. **Test changes**: Run relevant tests after modifications
5. **Check lints**: Fix any linter errors introduced
6. **Clean up temporary files**: Remove all temporary files, backups, and migration scripts before completing the task

### Example Code Modification
**Before**: Adding a new validation function
```python
# 1. Read existing utils.py to understand patterns
# 2. Check if similar functions exist
# 3. Extend existing module rather than create new file
# 4. Add function following existing naming/documentation conventions
# 5. Add tests in tests/unit/test_utils.py
# 6. Update module docstring if needed
```

---

## Interactive Data Exploration

### Jupyter Notebook
- **File**: `hi eddy.ipynb` - Interactive data exploration notebook
- **Purpose**: Interactive table viewer with sorting, filtering, and range sliders for portfolio runs data
- **Features**:
  - Dark mode interface with custom CSS styling
  - Sortable columns (ascending/descending buttons)
  - Filterable non-numeric columns (contains/does not contain with multi-select dropdowns)
  - Range sliders for numeric columns (IntRangeSlider/FloatRangeSlider with formatted readouts)
  - Pinned "Security" column with horizontal scrolling for remaining columns
  - Formatted numeric display matching column formatting rules:
    - Whole numbers with thousand separators (most columns)
    - 1 decimal place: "Yrs (Cvn)", "MTD Equity", "YTD Equity"
    - 2 decimal places: "Retracement"
  - Column selector panel with visibility toggles and reordering (up/down buttons)
  - Clear buttons for sorts, filters, and ranges
  - Optimized performance: Removed debug print statements, optimized refresh logic
  - Fixed column selector panel width constraints to eliminate white space
- **Dependencies**: 
  - `ipywidgets` - Interactive widgets (not in pyproject.toml, install separately if needed)
  - `beautifulsoup4` - HTML parsing for table manipulation (not in pyproject.toml, install separately if needed)
- **Data Source**: Reads from `analytics/processed_data/portfolio_runs_view.xlsx`
- **Usage**: Open in Jupyter/JupyterLab, execute cells to display interactive table

---

## Execution & Workflow

### Running the Pipeline
```bash
# 1. Optionally enter Poetry shell
poetry shell

# 2. Run automated pipeline runner
poetry run python run_pipeline.py
# Prompts: 1) Pipeline selection (1=Bond, 2=Runs, 3=Portfolio, 4=Both Bond+Runs, 5=Individual Parquet), 2) Mode (override/append), 3) (Bond) Include BQL workbook

# 3. Or use direct CLI (Bond Pipeline)
cd bond_pipeline
poetry run python pipeline.py -i "../Raw Data/" -m append --process-bql
poetry run python pipeline.py -i "../Raw Data/" -m override --process-bql

# 4. Or use Python module (Bond Pipeline)
poetry run python -m bond_pipeline.pipeline -i "Raw Data/" -m append

# 5. Runs Pipeline CLI
poetry run python -m runs_pipeline.pipeline -i "Historical Runs/" -m append
poetry run python -m runs_pipeline.pipeline -i "Historical Runs/" -m override

# 6. Portfolio Pipeline CLI
poetry run python -m portfolio_pipeline.pipeline -i "AD History/" -m append
poetry run python -m portfolio_pipeline.pipeline -i "AD History/" -m override

# 7. Unified Pipeline Orchestrator (Recommended)
poetry run python run_pipeline.py
# Prompts: Select pipeline(s) or individual parquet files, choose mode, decide on BQL ingestion when running Bond pipeline
# Option 5 allows regenerating individual parquet files (historical_bond_details, bql, runs_timeseries, or historical_portfolio)
```

### Pipeline Modes

#### Append Mode (Default, Daily Use)
- **Use case**: Add new Excel files to existing parquet tables
- **Behavior**: Skip dates already in parquet, append only new dates
- **Output**: Updated `historical_bond_details.parquet` (with new dates appended)
- **Universe**: Rebuild `universe.parquet` from complete historical data

#### Override Mode (Rebuild Everything)
- **Use case**: First-time setup, schema changes, data corruption recovery
- **Behavior**: Delete existing parquet files, process all Excel files from scratch
- **Output**: New `historical_bond_details.parquet`, new `universe.parquet`
- **Speed**: Slower but ensures clean data

### Checking Results
```bash
# View output parquet files
ls bond_data/parquet/

# Check logs
cat bond_data/logs/summary.log       # Pipeline execution summary
cat bond_data/logs/processing.log    # File-by-file details
cat bond_data/logs/duplicates.log    # Duplicate CUSIPs
cat bond_data/logs/validation.log    # CUSIP validation issues

# Quick stats in Python
import pandas as pd
df = pd.read_parquet('bond_data/parquet/historical_bond_details.parquet')
print(f"Rows: {len(df)}, Columns: {len(df.columns)}, Date range: {df['Date'].min()} to {df['Date'].max()}")

# Runs pipeline stats
df_runs = pd.read_parquet('bond_data/parquet/runs_timeseries.parquet')
print(f"Rows: {len(df_runs)}, Columns: {len(df_runs.columns)}")
print(f"Date range: {df_runs['Date'].min()} to {df_runs['Date'].max()}")
print(f"Unique CUSIPs: {df_runs['CUSIP'].nunique()}, Unique Dealers: {df_runs['Dealer'].nunique()}")

# BQL spreads stats
df_bql = pd.read_parquet('bond_data/parquet/bql.parquet')
print(f"BQL rows: {len(df_bql)}, Unique CUSIPs: {df_bql['CUSIP'].nunique()}, Unique Dates: {df_bql['Date'].nunique()}")

# Portfolio pipeline stats
df_portfolio = pd.read_parquet('bond_data/parquet/historical_portfolio.parquet')
print(f"Rows: {len(df_portfolio)}, Columns: {len(df_portfolio.columns)}")
print(f"Date range: {df_portfolio['Date'].min()} to {df_portfolio['Date'].max()}")
print(f"Unique CUSIPs: {df_portfolio['CUSIP'].nunique()}, Unique Accounts: {df_portfolio['ACCOUNT'].nunique()}, Unique Portfolios: {df_portfolio['PORTFOLIO'].nunique()}")
```

---

## Common Patterns & Examples

### Excel File Pattern Matching
```python
import re
from config import FILE_PATTERN

filename = "API 10.20.25.xlsx"
match = re.match(FILE_PATTERN, filename)
# Returns: datetime(2025, 10, 20)
```

### CUSIP Normalization
```python
from utils import normalize_cusip

raw = "89678zab2"  # lowercase
normalized = normalize_cusip(raw)  # Returns: "89678ZAB2"

raw = " 06418GAD9 Corp"  # extra text
normalized = normalize_cusip(raw)  # Returns: "06418GAD9"
```

### Schema Alignment
```python
from utils import align_to_master_schema

old_schema_df = read_excel(old_file)  # 59 columns
master_schema = [... 75 columns ...]
aligned_df = align_to_master_schema(old_schema_df, master_schema)  # 75 columns with NULL in new cols
```

### Deduplication
```python
from transform import DataTransformer

transformer = DataTransformer(log_dupes, log_valid)
df_cleaned = transformer.deduplicate(df, bond_date)

# Within same file: keep last occurrence
# Across files: Date+CUSIP primary key enforcement
```

### Logging Pattern
```python
from pathlib import Path
import logging
from utils import setup_logging

log_file = Path("bond_data/logs/processing.log")
logger = setup_logging(log_file, 'module_name', console_level=logging.CRITICAL)

logger.info("Processing file: API 10.20.25.xlsx")
logger.warning("Invalid CUSIP: 1234567890 (length 10)")
logger.error("Failed to read file: missing_header.xlsx")
```

### Reading/Writing Parquet
```python
import pandas as pd
from config import HISTORICAL_PARQUET, DATE_COLUMN

# Read existing dates (for append mode)
df = pd.read_parquet(HISTORICAL_PARQUET, columns=[DATE_COLUMN])
existing_dates = set(df[DATE_COLUMN].unique())

# Write new data (append mode)
df_new.to_parquet(HISTORICAL_PARQUET, mode='append', index=False)

# Write new data (override mode)
df_new.to_parquet(HISTORICAL_PARQUET, mode='overwrite', index=False)
```

---

## Troubleshooting Common Issues

### Import Errors
```bash
# Make sure you're in project root and virtual environment is activated
cd Bond-RV-App
poetry shell

# Use module syntax for imports
python -m bond_pipeline.pipeline
```

### Virtual Environment Issues
```bash
# Reinstall dependencies
poetry install

# Spawn a Poetry-managed shell if needed
poetry shell
```

### Test Discovery Issues
```bash
# Run from project root
cd Bond-RV-App
pytest tests/

# Or specific file
pytest tests/unit/test_utils.py -v
```

### Parquet File Locking
```bash
# Close any open Python sessions reading parquet files
# Windows: Check Task Manager for Python processes
```

---

## Key Takeaways for AI Assistant

### Essential Rules
1. **Search codebase first**: Always search within the codebase to see if any existing functions and/or classes can be used before building new logic
2. **Always analyze entire codebase** before making changes
3. **Execute all code in current virtual environment** (Bond-RV-App)
4. **Create tests in tests/ folder** when applicable
5. **Never create new files** unless explicitly required by user
6. **List file names and paths** when creating new files
7. **Include .cursorrules for context** in planning
8. **Map dependencies** before suggesting edits
9. **Extend existing logic** rather than duplicating
10. **Follow modular architecture**: config → utils → extract → transform → load → pipeline
11. **Respect logging patterns**: file logging (detailed) + console (minimal)

### Workflow Rules
12. **Think deeply** about prompt applicability before acting
13. **Use judgment** when interpreting conflicting requirements
14. **Notify user** when ignoring requirements or using judgment
15. **Ask clarifying questions** until all ambiguity is eliminated
16. **Provide file change summary** after completing requests
17. **ALWAYS clean up temporary files** created during chat sessions
18. **Always update .cursorrules** after applicable changes
19. **Update `CHANGELOG.md`** for all significant changes (new features, bug fixes, schema changes, new scripts, etc.)
20. **Include date/time stamps** in all documentation and changelog entries
21. **Update .cursorrules before git commits**

### Quality Standards
22. **Use pathlib.Path** for all path operations
23. **Avoid unicode issues** on Windows; all console/log output must remain ASCII-safe
24. **Modular design** following industry best practices
25. **Keep documentation current** when creating new docs

---

**File Created**: January 2025  
**Last Major Update**: 2025-01-21 17:00 ET - Comprehensive project structure audit and updates  
**Project**: Bond RV App - Data Pipeline  
**Version**: 1.0

**Note**: `README.md` references non-existent `Documentation/Workflows/` folder. Actual documentation is in `documentation/` folder (lowercase). README.md needs update to remove broken links.

---

**Note**: Project change history is maintained in `CHANGELOG.md`. Update that file when making significant changes to the codebase.
