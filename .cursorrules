# Bond RV App - Cursor AI Rules

**Last Updated**: 2025-11-14 18:30 ET - Added numeric column conversion for spread/metric columns, runs_today.py merge functionality, and comprehensive logging  
**Purpose**: Guide AI assistant in maintaining and extending bond data processing system

> **IMPORTANT**: Always update `.cursorrules` after every applicable change. Override where necessary, delete or change sections so that this file always reflects the most up-to-date project state based on actual codebase. Include date and time stamps for all major changes.

---

## AI Assistant Core Workflow

### General AI Behavior
- **Think deeply** about prompt applicability given end goal and current context
- **Use judgment** when interpreting prompts (e.g., if algo trading prompt mentions volume but raw data only has price, ignore volume requirement)
- **Always notify** user when ignoring requirements or using judgment in interpretation
- **Always ask clarifying questions** in planning mode; keep asking until all possible questions covering virtually any ambiguity are answered
- **Execute all project commands via** `poetry run ...` **(or inside an active** `poetry shell` **) to ensure the Poetry environment is used**

### File Change Summary
- **After finishing requests** that add/edit files, show summary of:
  - What was changed in each file
  - File paths affected
- **Track all modifications** for user review

### Temporary File Cleanup Protocol
- **ALWAYS clean up temporary files** created during chat sessions
- **Before completing tasks**, remove any temporary files created:
  - Migration scripts (`*_migrate*.py`)
  - Restore scripts (`*_restore*.py`)
  - Temporary test files (`*_temp*.py`)
  - Backup files (`.backup`, `*_backup*`)
- **List deleted files** in the final summary
- **Clean up at the END** of each chat session that creates temporary files
- **Common temporary files to remove**:
  - Scripts in `utils/` created for one-time operations
  - Backup parquet files in `bond_data/parquet/`
  - Test output files
  - Temporary data files

### Planning Mode Protocol
1. Ask clarifying questions first
2. Continue asking until ambiguity is eliminated
3. Present plan for approval
4. Execute after confirmation

---

## Project Overview

This is a **modular data engineering pipeline** for processing bond data for relative value trading applications.

### Core Purpose
- Process Excel bond data files into optimized Parquet time-series tables
- Support incremental data loading for daily updates

### Architecture
- **Modular ETL**: Separate modules for Extract → Transform → Load
- **Data Pipeline**: Excel + BQL workbooks → Parquet tables (historical_bond_details, universe, bql)
- **Incremental Loading**: Append mode (new dates only) vs Override mode (rebuild all)
- **Data Integrity**: Primary key enforcement, deduplication, validation

### Industry Best Practices
- **Modular design**: Project must be set up in modular way with correct folder patterns according to industry best practices
- **Single responsibility**: Each module has clear, focused purpose
- **Separation of concerns**: Clear boundaries between extraction, transformation, and loading
- **Configuration management**: All paths, constants, and settings centralized

---

## Technology Stack

### Language & Version
- **Python 3.11+** (strict requirement)
- **Type hints required** for all function signatures
- **Docstrings required** for all modules, classes, and functions

### Key Dependencies
- `pandas>=2.0.0` - Data manipulation
- `pyarrow>=12.0.0` - Parquet I/O
- `openpyxl>=3.1.0` - Excel reading
- `ipykernel>=7.1.0` (dev) - Jupyter kernel integration for notebooks (added 2025-11-08 11:05 ET)
- `nbconvert>=7.16.6` (dev) - Automated notebook execution (added 2025-11-08 11:20 ET)
- `notebook>=7.4.7` (dev) - Local Jupyter server for VS Code kernel selection (added 2025-11-08 11:35 ET)
- `pytest>=7.0.0` - Testing framework

### Environment Management
- **Environment manager**: Poetry (`pyproject.toml`, `poetry.lock`)
- **Virtual environment location**: `.venv/` (created via `poetry config virtualenvs.in-project true`)
- **ALWAYS execute code with** `poetry run ...` or inside `poetry shell`
- **Install dependencies** via `poetry install`

---

## Code Style & Conventions

### Formatting
- **Line length**: 88 characters maximum
- **Indentation**: 4 spaces (no tabs)
- **Imports**: Standard library → Third-party → Local imports (blank lines between groups)
- **Naming**: snake_case for functions/variables, PascalCase for classes, UPPER_CASE for constants

### Documentation
- **Module docstrings**: First line is summary, followed by detailed description if needed
- **Function docstrings**: Use Google style with Args/Returns/Raises sections
- **Type hints**: Use modern Python typing (e.g., `Optional[str]`, `List[pd.DataFrame]`, `Tuple[str, int]`)
- **Date and time stamps**: Include date/time stamps in all documentation entries
- **Keep documentation current**: When creating new documentation, figure out if any old documentation needs updating
- **Version tracking**: Update existing docs when adding new information

### Example Function Signature
```python
def normalize_cusip(cusip: str) -> Optional[str]:
    """
    Normalize CUSIP to uppercase 9-character format.
    
    Args:
        cusip: Raw CUSIP string (may contain lowercase, spaces, extra text)
    
    Returns:
        Normalized CUSIP or None if invalid
    """
```

---

## Code Organization

### Module Structure
```
bond_pipeline/
├── config.py        # Configuration, paths, constants (includes RUNS config)
├── utils.py         # Helper functions (date parsing, validation, logging, runs utilities)
├── extract.py       # Excel file reading and date extraction
├── transform.py     # Data cleaning, normalization, deduplication
├── load.py          # Parquet writing (append/override modes)
├── pipeline.py      # Main orchestration script with CLI
└── README.md        # Module documentation

runs_pipeline/
├── __init__.py      # Package exports
├── extract.py       # RUNS Excel file reading and Date/Time parsing
├── transform.py     # End-of-day deduplication, CUSIP validation/orphan tracking
├── load.py          # Runs parquet writing (append/override modes)
└── pipeline.py      # Main orchestration script with CLI

portfolio_pipeline/
├── __init__.py      # Package exports
├── extract.py       # Portfolio Excel file reading and date extraction from filename
├── transform.py     # CUSIP normalization, NA cleaning, deduplication (Date+CUSIP+ACCOUNT+PORTFOLIO)
├── load.py          # Portfolio parquet writing (append/override modes)
└── pipeline.py      # Main orchestration script with CLI
```

### Design Principles
1. **Single Responsibility**: Each module has one clear purpose
2. **Dependency Injection**: Pass loggers, configs as constructor arguments
3. **Separation of Concerns**: Extract (read) → Transform (clean) → Load (write)
4. **Configuration Centralization**: All paths and constants in `config.py`
5. **Immutable Config**: Don't modify config values at runtime

### Class-Based Architecture
Each pipeline stage is a class with clear responsibilities:

**Bond Pipeline:**
- **ExcelExtractor**: File discovery, Excel reading, date extraction
- **DataTransformer**: Schema alignment, CUSIP normalization, NA cleaning, deduplication
- **ParquetLoader**: Append/override modes, date checking, universe creation
- **BondDataPipeline**: Orchestrates all components, CLI interface

**Runs Pipeline:**
- **RunsExtractor**: RUNS Excel reading, Date/Time parsing (from columns, not filename)
- **RunsTransformer**: End-of-day deduplication (vectorized, ~100x faster), CUSIP validation/orphan tracking (vectorized), schema alignment
– **RunsLoader**: Runs parquet writing, Date+Dealer+CUSIP primary key enforcement, granular load metrics (new rows/dates/CUSIPs/Dealers)
– **RunsDataPipeline**: Orchestrates all components, CLI interface, append-mode file selection + enhanced run summaries

**Portfolio Pipeline:**
- **PortfolioExtractor**: Portfolio Excel reading, date extraction from filename (Aggies MM.DD.YY.xlsx pattern), removes Unnamed columns
- **PortfolioTransformer**: CUSIP normalization (same as bond pipeline), row filtering (drops rows with blank SECURITY or CUSIP), NA cleaning, deduplication (Date+CUSIP+ACCOUNT+PORTFOLIO)
- **PortfolioLoader**: Portfolio parquet writing, Date+CUSIP+ACCOUNT+PORTFOLIO primary key enforcement, append/override modes
- **PortfolioDataPipeline**: Orchestrates all components, CLI interface, enhanced run summaries

**Pipeline Orchestrator:**
- **run_pipeline.py**: Unified entry point for bond, runs, and portfolio pipelines
- User can select: Bond only, Runs only, Portfolio only, Both Bond and Runs, or Individual Parquet Files
- Individual Parquet Files option allows regenerating specific parquet outputs:
  - `historical_bond_details.parquet` (also regenerates universe)
  - `bql.parquet` (standalone BQL processing)
  - `runs_timeseries.parquet` (standalone runs processing)
  - `historical_portfolio.parquet` (standalone portfolio processing)
- Shared mode selection (append/override) for selected pipelines
- After successful runs, generates `bond_data/logs/parquet_stats.log` with dataset diagnostics (df.info/head/tail)

---

## Data Pipeline Patterns

### Bond Pipeline Input Data Format
- **Excel files**: Pattern `API MM.DD.YY.xlsx` (e.g., `API 10.20.25.xlsx`)
- **Header row**: Row 3 (index 2, 0-based)
- **Columns**: 75 columns in latest files (schema evolved from 59)
- **NA values**: `#N/A Field Not Applicable`, `#N/A Invalid Security`, etc.

### BQL Workbook Input Data Format
- **Excel file**: Fixed workbook `bql.xlsx` located in Support Files directory
- **Sheet**: `bql`
- **Multi-index header**: 4-row column header structure:
  - Row 0 (index 0): Ignored
  - Row 1 (index 1): Security names (1st level)
  - Row 2 (index 2): CUSIPs (2nd level)
  - Row 3 (index 3): Ignored
  - Row 4 (index 4): Data rows start here
- **Column 0**: First column labelled `CUSIPs` contains daily timestamps
- **Header processing**: Extract names from level 1 (row 1) and CUSIPs from level 2 (row 2) of multi-index columns
- **CUSIP cleaning**: Strip whitespace, remove trailing `Corp`, enforce 9-character uppercase CUSIP
- **Output expectation**: Long-form DataFrame with columns `Date`, `Name`, `CUSIP`, `Value`

### Runs Pipeline Input Data Format
- **Excel files**: Pattern `RUNS MM.DD.YY.xlsx` (e.g., `RUNS 10.31.25.xlsx`)
- **Header row**: Row 1 (index 0, 0-based)
- **Columns**: 30 columns in latest files (schema evolved from 28 in one old file)
- **Date/Time**: Date and Time come from columns (not filename)
  - Date: MM/DD/YY format (e.g., "10/31/25") → parsed to datetime object
  - Time: HH:MM format (e.g., "15:45") → parsed to datetime.time object
- **NA values**: Same as bond pipeline (`#N/A Field Not Applicable`, `#N/A Invalid Security`, etc.)

### Portfolio Pipeline Input Data Format
- **Excel files**: Pattern `Aggies MM.DD.YY.xlsx` (e.g., `Aggies 11.04.25.xlsx`)
- **Header row**: Row 1 (index 0, 0-based)
- **Columns**: 82 columns (schema detected dynamically from latest file)
- **Date extraction**: Date extracted from filename (MM/DD/YYYY format)
- **Column filtering**: Unnamed columns (Unnamed: 0, Unnamed: 1, etc.) are automatically removed during extraction
- **Row filtering**: Rows with blank SECURITY or CUSIP are dropped during transformation
- **NA values**: Same as bond pipeline (`#N/A Field Not Applicable`, `#N/A Invalid Security`, etc.)

### Output Data Format

#### `historical_bond_details.parquet`
- **Purpose**: Time series of all bonds over time
- **Primary Key**: `Date + CUSIP` (unique combination)
- **Schema**: All 75 columns + `Date` column (first position)
- **Row count**: ~25,000+ rows spanning 2023-2025
- **Storage**: `bond_data/parquet/historical_bond_details.parquet`

#### `universe.parquet`
- **Purpose**: Current universe of all unique CUSIPs ever seen
- **Primary Key**: `CUSIP` (unique)
- **Schema**: 13 key columns only
  1. CUSIP, 2. Benchmark Cusip, 3. Custom_Sector, 4. Bloomberg Cusip, 5. Security,
  6. Benchmark, 7. Pricing Date, 8. Pricing Date (Bench), 9. Worst Date,
  10. Yrs (Worst), 11. Ticker, 12. Currency, 13. Equity Ticker
- **Update strategy**: Always rebuild from `historical_bond_details` (most recent date per CUSIP)
- **Storage**: `bond_data/parquet/universe.parquet`

#### `bql.parquet`
- **Purpose**: Long-form Bloomberg query spreads dataset
- **Primary Key**: `Date + CUSIP`
- **Schema**: Columns `Date`, `Name`, `CUSIP`, `Value`
- **Name mapping**: Derived from workbook header row; used for orphan logging
- **Storage**: `bond_data/parquet/bql.parquet`
- **Logging**: Warn when `bql` CUSIPs are missing from `universe.parquet`

#### `runs_timeseries.parquet`
- **Purpose**: Time series of dealer quotes over time (end-of-day snapshots)
- **Primary Key**: `Date + Dealer + CUSIP` (unique combination, enforced after deduplication)
- **Schema**: All 30 columns with Date and Time as first columns
- **Dealer Filtering**: Only includes dealers: BMO, BNS, NBF, RBC, TD (other dealers filtered out during load)
- **Row count**: ~19,000+ rows (after deduplication and dealer filtering) spanning 2022-2025
- **Storage**: `bond_data/parquet/runs_timeseries.parquet`
- **Deduplication**: Keep latest Time per Date+Dealer+CUSIP (end-of-day snapshot)

#### `historical_portfolio.parquet`
- **Purpose**: Time series of portfolio holdings over time
- **Primary Key**: `Date + CUSIP + ACCOUNT + PORTFOLIO` (unique combination, preserves account/portfolio detail)
- **Schema**: All 82 columns (excluding Unnamed columns) with Date column first
- **Column filtering**: Unnamed columns (Unnamed: 0, Unnamed: 1, etc.) are automatically removed
- **Row filtering**: Rows with blank SECURITY or CUSIP are automatically dropped
- **Row count**: Varies based on portfolio holdings across dates
- **Storage**: `bond_data/parquet/historical_portfolio.parquet`
- **Deduplication**: Keep last occurrence per Date+CUSIP+ACCOUNT+PORTFOLIO

### Processing Rules

#### Date Handling
- Extract from filename pattern: `API MM.DD.YY.xlsx` → `datetime(YYYY-MM-DD)`
- Store as `datetime64` in parquet (Date column first)
- Append mode: Skip dates already in parquet file
- Override mode: Delete existing parquet files, rebuild from scratch

#### CUSIP Normalization & Validation

**Bond Pipeline:**
- Convert to uppercase: `89678zab2` → `89678ZAB2`
- Remove extra text: ` 06418GAD9 Corp` → `06418GAD9`
- Validate length: Must be exactly 9 characters
- Invalid CUSIPs: Log warning but include in data
- Primary key enforcement: No duplicate Date+CUSIP combinations

**Runs Pipeline:**
- No normalization: Keep CUSIPs as-is from Excel (no uppercase conversion, no text removal)
- Validate length: Check if 9 characters using vectorized operations (but don't normalize)
- **Optimized**: Vectorized CUSIP validation (~100x faster than row-by-row iteration)
- Invalid CUSIPs: Log summary warning but include in data (same as bond pipeline)
- **Enhanced Orphan Tracking**: Log orphans with context (Security, Date, Dealer, Time, Ticker) to validation.log
- **Dealer Filtering**: Only dealers BMO, BNS, NBF, RBC, TD are stored in `runs_timeseries.parquet` (other dealers filtered out during load)
- Primary key enforcement: No duplicate Date+Dealer+CUSIP combinations (after deduplication)
- Append mode inspects existing parquet dates, skips fully-loaded files, and filters mixed files down to unseen dates before extraction.
- Pipeline summaries surface files processed/skipped plus new rows, dates, CUSIPs, and dealers added during each run.

**Portfolio Pipeline:**
- CUSIP normalization: Same as bond pipeline (uppercase, remove extra text, validate 9 characters)
- Row filtering: Drop rows where either SECURITY or CUSIP is blank/NaN/empty (done first in transformation)
- Column filtering: Remove all "Unnamed:" columns (Unnamed: 0, Unnamed: 1, etc.) during extraction
- Invalid CUSIPs: Log warning but include in data (same as bond pipeline)
- Primary key enforcement: No duplicate Date+CUSIP+ACCOUNT+PORTFOLIO combinations
- Schema evolution: Master schema (82 columns) detected dynamically from latest file
- Append mode: Skip dates already in parquet file
- Override mode: Delete existing parquet file, rebuild from scratch

#### Schema Evolution
- **Master schema**: 75 columns from latest files (set dynamically)
- **Older files**: Fill missing columns with `NULL/NaN`
- **Column order**: Preserve order from master schema
- **Date column**: Always first column in all DataFrames

#### Deduplication

**Bond Pipeline:**
- Within file: Keep last occurrence of duplicate CUSIPs
- Across files: One record per Date+CUSIP combination
- Log all duplicates to `bond_data/logs/duplicates.log`

**Runs Pipeline:**
- End-of-day snapshots: Keep latest Time per Date+Dealer+CUSIP (most recent quote of day)
- Same-time tiebreaker: If multiple rows have same latest Time, keep last row by position
- **Optimized**: Uses vectorized sort + drop_duplicates (O(n log n)) instead of row-by-row iteration (O(n*groups))
- Within file and across files: One record per Date+Dealer+CUSIP combination
- Log summary of duplicates to `bond_data/logs/duplicates.log` (combined with bond pipeline logs)

**Portfolio Pipeline:**
- Within file: Keep last occurrence of duplicate Date+CUSIP+ACCOUNT+PORTFOLIO combinations
- Across files: One record per Date+CUSIP+ACCOUNT+PORTFOLIO combination
- Log duplicates to `bond_data/logs/duplicates.log` (combined with bond and runs pipeline logs)

#### Data Cleaning
- Convert NA values to NULL: `#N/A Field Not Applicable`, `#N/A Invalid Security`, `N/A`, etc.
- Empty cells → NULL/NaN
- Preserve all other values as-is

#### Numeric Column Conversion
- **Years columns**: `Yrs Since Issue`, `Yrs (Worst)`, and `Yrs (Cvn)` are automatically converted to numeric (`float64`) during transformation
- **Spread and metric columns**: `G Sprd`, `vs BI`, `vs BCE`, `MTD Equity`, `YTD Equity`, `Retracement`, `Z Score`, `Retracement2` are automatically converted to numeric (`float64`) during transformation
- **Conversion points**: 
  - Transform step: Converts during `transform_single_file()` processing via `convert_years_to_numeric()` method
  - Load step (append): Converts new data and existing data for compatibility
  - Load step (override): Converts all data before writing
- **Result**: These columns are always stored as numeric (`float64`) in parquet files, enabling proper binning, statistical analysis, and accurate merging with other datasets
- **Error handling**: Non-numeric values are coerced to NaN (errors='coerce')

#### BQL Pipeline
- Read Excel workbook with 4-row multi-index header (`header=[0,1,2,3]`)
- Extract security names from level 1 (row 1) and CUSIPs from level 2 (row 2) of multi-index columns
- Normalize header values to derive security names and enforce 9-character CUSIPs
- Reshape wide workbook to long-form dataset (`Date`, `Name`, `CUSIP`, `Value`)
- Skip duplicate/invalid CUSIP columns and log issues for traceability
- Drop rows with non-numeric values before persisting
- Compare resulting CUSIPs against `universe.parquet` and log orphan details (CUSIP + Name)

---

## Logging Standards

### Log File Organization
```
bond_data/logs/
├── processing.log      # File extraction and loading operations
├── duplicates.log      # All duplicate CUSIPs detected
├── validation.log      # CUSIP validation warnings
├── summary.log         # Pipeline execution summary (run headers, stats)
├── parquet_stats.log   # Snapshot diagnostics for each parquet dataset (df.info/head/tail)
└── runs_today.log     # Runs today analytics execution log (merge statistics, df.info, matching details)
```

### Logging Levels
- **DEBUG**: Detailed diagnostic info (file-by-file progress)
- **INFO**: General informational messages
- **WARNING**: CUSIP validation issues, schema mismatches
- **ERROR**: File read failures, data corruption
- **CRITICAL**: System failures

### Logging Best Practices
1. **Dual logging**: File logging (detailed) + Console logging (essential only)
2. **No console spam**: Suppress detailed logs on console (console_level=logging.CRITICAL)
3. **Run metadata**: Track run_id, timestamp, mode for each execution
4. **Log rotation**: Archive logs after N runs (currently 10)
5. **Structured info**: Include Date, CUSIP, file path in log messages
6. **BQL ingestion**: Log unique CUSIP/date counts and orphan CUSIPs with security names
7. **Unicode encoding**: File handlers use UTF-8 encoding with `errors='replace'` to handle Unicode characters in bond names
8. **ASCII-safe logging**: Use `sanitize_log_message()` function to convert Unicode characters to ASCII-safe replacements before logging (prevents Windows console encoding errors)

### Example Logging Pattern
```python
# Setup logger (file only, no console)
self.logger = setup_logging(LOG_FILE_PROCESSING, 'extract', console_level=logging.CRITICAL)

# Log file processing
self.logger.info(f"Processing file: {filename}")
self.logger.info(f"Extracted {len(df)} rows for date {date_obj}")

# Log validation issues
self.logger.warning(f"Invalid CUSIP: {cusip} (length {len(cusip)})")

# Log duplicates
self.logger_dupes.info(f"Duplicate Date+CUSIP: {date} - {cusip} - {bond_name} (kept last occurrence)")
```

---

## Testing Standards

### Test Structure
```
tests/
├── conftest.py              # Shared fixtures (sample data, mocks)
├── unit/                    # Unit tests
│   ├── test_utils.py       # ✅ Bond utility & BQL helper tests
│   ├── test_extract.py     # TODO
│   ├── test_transform.py   # TODO
│   └── test_load.py        # TODO
└── integration/             # Integration tests
    └── test_pipeline.py    # TODO
```

### Testing Requirements
1. **Coverage**: Target 85%+ overall, 90%+ for critical modules
2. **Test location**: All tests in `tests/` directories within each module
3. **Naming**: Test files follow naming convention: `test_*.py`
4. **Framework**: Use pytest for testing
5. **Fixtures**: Reusable test data in `conftest.py`
6. **Isolation**: Tests must be independent (no shared state)
7. **Mocking**: Mock all file I/O, external dependencies
8. **Speed**: Full test suite should run in < 30 seconds

### Test Categories
- **Date parsing**: Valid/invalid formats, leap years, edge cases
- **CUSIP validation**: Valid/invalid lengths, characters, normalization
- **NA cleaning**: Standard patterns, mixed DataFrames
- **Schema alignment**: Old (59) vs new (75) column schemas
- **Deduplication**: Within file, across files, Date+CUSIP combinations
- **Append mode**: Skip existing dates, append new dates
- **Override mode**: Delete existing, rebuild from scratch
- **BQL ingestion**: Header normalization, wide→long reshape, orphan logging against universe

### Running Tests
```bash
# Install dependencies (first run or after updates)
poetry install

# Run all tests
poetry run pytest

# Run specific file
poetry run pytest tests/unit/test_utils.py

# Run with coverage
poetry run pytest --cov=bond_pipeline --cov-report=html

# Run specific test
poetry run pytest tests/unit/test_utils.py::TestCUSIPValidation::test_validate_cusip_valid
```

---

## File Organization Rules

### Directory Structure
```
Bond-RV-App/
├── analytics/             # Analytics scripts and processed outputs
│   ├── comb/             # Pair analytics scripts
│   │   ├── comb.py           # Consolidated pair analytics script (runs all analyses, outputs single Excel file)
│   │   ├── cr01_comb.py      # CR01 pair analytics (BQL universe vs holdings spreads)
│   │   ├── all_comb.py       # All combinations pair analytics (all CAD CUSIPs)
│   │   ├── term_comb.py      # Term combinations pair analytics (filters pairs where abs(Yrs (Cvn) difference) <= 0.8)
│   │   ├── ticker_comb.py    # Ticker combinations pair analytics (filters pairs where Ticker values match exactly)
│   │   ├── custom_sector.py  # Custom sector combinations pair analytics (filters pairs where Custom_Sector values match exactly)
│   │   ├── custom_bond_comb.py  # Custom bond combinations pair analytics (pairs all CAD CUSIPs with fixed target bond, configurable by Ticker or Security)
│   │   ├── custom_bond_vs_holdings.py  # Custom bond vs holdings pair analytics (pairs target CAD bonds from TARGET_BOND_TICKER/SECURITY with CR01-filtered holdings)
│   │   ├── port_comb.py      # Portfolio combinations pair analytics (filters to portfolio CUSIPs as cusip_2)
│   │   ├── port_executable.py  # Portfolio executable combinations pair analytics (filters cusip_1 by CR01 @ Wide Offer >= 2000, cusip_2 by portfolio)
│   │   ├── cad_cheap_vs_usd.py  # CAD cheap vs USD pair analytics (CAD/USD pairs with matching Ticker, Custom_Sector, Yrs (Cvn) diff <= 2)
│   │   ├── cad_rich_vs_usd.py   # CAD rich vs USD pair analytics (USD/CAD pairs with matching Ticker, Custom_Sector, Yrs (Cvn) diff <= 2)
│   │   ├── executable cr01 decent bid offer vs holdings.py  # CR01 pair analytics (dynamic CUSIP loading from runs_today.csv + portfolio, filters: CR01 @ Tight Bid > 2000, CR01 @ Wide Offer > 2000, Bid/Offer>3mm < 3)
│   │   ├── executable cr01 vs holdings.py  # CR01 pair analytics (dynamic CUSIP loading from runs_today.csv + portfolio, filters: CR01 @ Tight Bid > 2000, CR01 @ Wide Offer > 2000, no Bid/Offer filter)
│   │   └── all combos vs holdings.py  # All combinations vs holdings pair analytics (all portfolio CUSIPs from last date, no CR01 filters)
│   ├── runs/             # Runs analytics scripts
│   │   └── runs_today.py     # Runs today analytics (reads parquet directly, aggregates required dates, computes DoD/MTD/YTD/1yr changes)
│   └── processed_data/   # Outputs from analytics scripts
│       ├── comb.xlsx         # Consolidated Excel file with all pair analytics (from comb.py, formatted tables on separate sheets)
│       ├── comb.txt          # Text summary of pair analytics results (from comb.py)
│       ├── comb_validation.txt  # Validation log for pair analytics (from comb.py)
│       ├── cr01_pair_analytics.csv
│       ├── all_comb.csv
│       ├── term_comb.csv
│       ├── ticker_comb.csv
│       ├── custom_sector_comb.csv
│       ├── custom_bond_comb.csv
│       ├── custom_bond_vs_holdings.csv
│       ├── port_comb.csv
│       ├── port_executable.csv
│       ├── cad_cheap_vs_usd.csv
│       ├── cad_rich_vs_usd.csv
│       ├── executable cr01 decent bid offer vs holdings.csv
│       ├── executable cr01 vs holdings.csv
│       ├── all combos vs holdings.csv
│       └── runs_today.csv
├── bond_pipeline/          # Pipeline code (7 modules)
├── bond_data/              # Data directory (local only, git-ignored)
│   ├── parquet/           # Output parquet files
│   └── logs/              # Processing logs
├── .venv/                 # Poetry-managed virtual environment (git-ignored)
├── Documentation/          # Complete documentation
│   ├── Setup/             # Getting started guides
│   ├── Workflows/         # Step-by-step procedures
│   └── Architecture/      # Design documentation
├── Raw Data/              # Excel files (input)
├── References and Documentation/  # Ignored by Cursor
├── tests/                 # Test suite
├── utils/                 # Utility scripts
├── .cursorrules           # This file
├── .cursorignore          # Files to ignore
├── pyproject.toml         # Poetry project configuration
├── poetry.lock            # Locked dependency versions
├── requirements.txt       # Legacy dependency snapshot (reference only)
├── README.md              # Project overview
└── run_pipeline.py        # Simple pipeline runner
```

### File Naming Conventions
- **Modules**: `snake_case.py` (e.g., `bond_pipeline/utils.py`)
- **Test files**: `test_*.py` (e.g., `tests/unit/test_utils.py`)
- **Config files**: `.cursorrules`, `.cursorignore`, `pytest.ini`
- **Data files**: `historical_bond_details.parquet`, `universe.parquet`

### Important Paths
- **Project root**: `PROJECT_ROOT = Path(__file__).parent.parent` (from config.py)
- **Data directory**: `bond_data/` (git-ignored)
- **Parquet files**: `bond_data/parquet/` (git-tracked)
- **Logs**: `bond_data/logs/` (git-ignored)
- **Default input**: Dropbox folder (Windows-specific path in config.py)

### Path Handling Rules
- **Use `pathlib.Path`** for all path operations
- **Scripts must detect their location** and adjust paths accordingly
- **Support running from multiple directory locations** (absolute and relative paths)
- **Avoid hardcoded paths**: Use relative paths with config-based resolution
- **Analytics script path resolution**:
  - Scripts in `analytics/comb/` and `analytics/runs/` use `SCRIPT_DIR = Path(__file__).parent.resolve()`
  - Parquet files: `SCRIPT_DIR.parent.parent / "bond_data" / "parquet" / "*.parquet"` (goes up 2 levels to project root)
  - Output directory: `SCRIPT_DIR.parent / "processed_data"` (goes up 1 level to `analytics/processed_data/`)
  - Cross-references: `SCRIPT_DIR.parent / "processed_data" / "*.csv"` (for scripts reading other analytics outputs)

---

## Git & Version Control

### Git-Ignored Files
- Excel files: `*.xlsx`, `*.xls`
- Log files: `bond_data/logs/*.log`
- Virtual environment: `.venv/`
- Python cache: `__pycache__/`, `*.pyc`
- Test output: `tests/test_output/`
- References folder: `References and Documentation/` (in .cursorignore)

### Git-Tracked Files
- Parquet files: `bond_data/parquet/*.parquet` (processed data)
- Source code: `bond_pipeline/`, `utils/`
- Tests: `tests/`
- Documentation: `Documentation/`
- Config: `pyproject.toml`, `poetry.lock`, `requirements.txt`, `.cursorrules`, `.cursorignore`
- Scripts: `run_pipeline.py`

### Git Commit Protocol
- **Update `.cursorrules`** before every git commit
- **Verify file timestamps** and version consistency
- **Document changes** in commit messages with reference to `.cursorrules` updates

### Unicode & Encoding
- **Avoid unicode encoding issues** for Windows
- **Use ASCII-compatible characters** when needed
- **Test paths** on Windows file system before committing

---

## Code Modification Guidelines

### Before Making Changes
1. **Search codebase first**: Always search within the codebase to see if any existing functions and/or classes can be used before building new logic
2. **Analyze entire codebase**: Understand dependencies and relationships
3. **Map file impacts**: Identify ALL files that need modification
4. **Check existing logic**: Extend or refactor before creating new code
5. **Consider ripple effects**: Changes impact downstream modules

### Coding Principles
1. **Extend existing logic**: Never create new files when existing code can be extended
2. **Refactor first**: Improve existing structure before adding new features
3. **Minimal changes**: Make smallest change that achieves the goal
4. **Backward compatibility**: Don't break existing functionality
5. **Documentation**: Update docstrings when modifying functions

### File Creation Rules
- **List new files**: Announce file name and path before creating
- **Test files**: Always create in `tests/` folder when applicable
- **Avoid duplication**: Check if existing modules can handle the requirement
- **Justification**: Explain why new file is needed vs extending existing
- **Clean up temporary files**: Remove any migration/restore/cleanup scripts after use

### When AI Assistant Helps
1. **Read existing code**: Always read relevant files before suggesting changes
2. **Show context**: Reference existing code with `startLine:endLine:filepath` format
3. **Propose edits**: Suggest specific changes with line-by-line diffs
4. **Test changes**: Run relevant tests after modifications
5. **Check lints**: Fix any linter errors introduced
6. **Clean up temporary files**: Remove all temporary files, backups, and migration scripts before completing the task

### Example Code Modification
**Before**: Adding a new validation function
```python
# 1. Read existing utils.py to understand patterns
# 2. Check if similar functions exist
# 3. Extend existing module rather than create new file
# 4. Add function following existing naming/documentation conventions
# 5. Add tests in tests/unit/test_utils.py
# 6. Update module docstring if needed
```

---

## Execution & Workflow

### Running the Pipeline
```bash
# 1. Optionally enter Poetry shell
poetry shell

# 2. Run automated pipeline runner
poetry run python run_pipeline.py
# Prompts: 1) Pipeline selection (1=Bond, 2=Runs, 3=Portfolio, 4=Both Bond+Runs, 5=Individual Parquet), 2) Mode (override/append), 3) (Bond) Include BQL workbook

# 3. Or use direct CLI (Bond Pipeline)
cd bond_pipeline
poetry run python pipeline.py -i "../Raw Data/" -m append --process-bql
poetry run python pipeline.py -i "../Raw Data/" -m override --process-bql

# 4. Or use Python module (Bond Pipeline)
poetry run python -m bond_pipeline.pipeline -i "Raw Data/" -m append

# 5. Runs Pipeline CLI
poetry run python -m runs_pipeline.pipeline -i "Historical Runs/" -m append
poetry run python -m runs_pipeline.pipeline -i "Historical Runs/" -m override

# 6. Portfolio Pipeline CLI
poetry run python -m portfolio_pipeline.pipeline -i "AD History/" -m append
poetry run python -m portfolio_pipeline.pipeline -i "AD History/" -m override

# 7. Unified Pipeline Orchestrator (Recommended)
poetry run python run_pipeline.py
# Prompts: Select pipeline(s) or individual parquet files, choose mode, decide on BQL ingestion when running Bond pipeline
# Option 5 allows regenerating individual parquet files (historical_bond_details, bql, runs_timeseries, or historical_portfolio)
```

### Pipeline Modes

#### Append Mode (Default, Daily Use)
- **Use case**: Add new Excel files to existing parquet tables
- **Behavior**: Skip dates already in parquet, append only new dates
- **Output**: Updated `historical_bond_details.parquet` (with new dates appended)
- **Universe**: Rebuild `universe.parquet` from complete historical data

#### Override Mode (Rebuild Everything)
- **Use case**: First-time setup, schema changes, data corruption recovery
- **Behavior**: Delete existing parquet files, process all Excel files from scratch
- **Output**: New `historical_bond_details.parquet`, new `universe.parquet`
- **Speed**: Slower but ensures clean data

### Checking Results
```bash
# View output parquet files
ls bond_data/parquet/

# Check logs
cat bond_data/logs/summary.log       # Pipeline execution summary
cat bond_data/logs/processing.log    # File-by-file details
cat bond_data/logs/duplicates.log    # Duplicate CUSIPs
cat bond_data/logs/validation.log    # CUSIP validation issues

# Quick stats in Python
import pandas as pd
df = pd.read_parquet('bond_data/parquet/historical_bond_details.parquet')
print(f"Rows: {len(df)}, Columns: {len(df.columns)}, Date range: {df['Date'].min()} to {df['Date'].max()}")

# Runs pipeline stats
df_runs = pd.read_parquet('bond_data/parquet/runs_timeseries.parquet')
print(f"Rows: {len(df_runs)}, Columns: {len(df_runs.columns)}")
print(f"Date range: {df_runs['Date'].min()} to {df_runs['Date'].max()}")
print(f"Unique CUSIPs: {df_runs['CUSIP'].nunique()}, Unique Dealers: {df_runs['Dealer'].nunique()}")

# BQL spreads stats
df_bql = pd.read_parquet('bond_data/parquet/bql.parquet')
print(f"BQL rows: {len(df_bql)}, Unique CUSIPs: {df_bql['CUSIP'].nunique()}, Unique Dates: {df_bql['Date'].nunique()}")

# Portfolio pipeline stats
df_portfolio = pd.read_parquet('bond_data/parquet/historical_portfolio.parquet')
print(f"Rows: {len(df_portfolio)}, Columns: {len(df_portfolio.columns)}")
print(f"Date range: {df_portfolio['Date'].min()} to {df_portfolio['Date'].max()}")
print(f"Unique CUSIPs: {df_portfolio['CUSIP'].nunique()}, Unique Accounts: {df_portfolio['ACCOUNT'].nunique()}, Unique Portfolios: {df_portfolio['PORTFOLIO'].nunique()}")
```

---

## Common Patterns & Examples

### Excel File Pattern Matching
```python
import re
from config import FILE_PATTERN

filename = "API 10.20.25.xlsx"
match = re.match(FILE_PATTERN, filename)
# Returns: datetime(2025, 10, 20)
```

### CUSIP Normalization
```python
from utils import normalize_cusip

raw = "89678zab2"  # lowercase
normalized = normalize_cusip(raw)  # Returns: "89678ZAB2"

raw = " 06418GAD9 Corp"  # extra text
normalized = normalize_cusip(raw)  # Returns: "06418GAD9"
```

### Schema Alignment
```python
from utils import align_to_master_schema

old_schema_df = read_excel(old_file)  # 59 columns
master_schema = [... 75 columns ...]
aligned_df = align_to_master_schema(old_schema_df, master_schema)  # 75 columns with NULL in new cols
```

### Deduplication
```python
from transform import DataTransformer

transformer = DataTransformer(log_dupes, log_valid)
df_cleaned = transformer.deduplicate(df, bond_date)

# Within same file: keep last occurrence
# Across files: Date+CUSIP primary key enforcement
```

### Logging Pattern
```python
from pathlib import Path
import logging
from utils import setup_logging

log_file = Path("bond_data/logs/processing.log")
logger = setup_logging(log_file, 'module_name', console_level=logging.CRITICAL)

logger.info("Processing file: API 10.20.25.xlsx")
logger.warning("Invalid CUSIP: 1234567890 (length 10)")
logger.error("Failed to read file: missing_header.xlsx")
```

### Reading/Writing Parquet
```python
import pandas as pd
from config import HISTORICAL_PARQUET, DATE_COLUMN

# Read existing dates (for append mode)
df = pd.read_parquet(HISTORICAL_PARQUET, columns=[DATE_COLUMN])
existing_dates = set(df[DATE_COLUMN].unique())

# Write new data (append mode)
df_new.to_parquet(HISTORICAL_PARQUET, mode='append', index=False)

# Write new data (override mode)
df_new.to_parquet(HISTORICAL_PARQUET, mode='overwrite', index=False)
```

---

## Troubleshooting Common Issues

### Import Errors
```bash
# Make sure you're in project root and virtual environment is activated
cd Bond-RV-App
poetry shell

# Use module syntax for imports
python -m bond_pipeline.pipeline
```

### Virtual Environment Issues
```bash
# Reinstall dependencies
poetry install

# Spawn a Poetry-managed shell if needed
poetry shell
```

### Test Discovery Issues
```bash
# Run from project root
cd Bond-RV-App
pytest tests/

# Or specific file
pytest tests/unit/test_utils.py -v
```

### Parquet File Locking
```bash
# Close any open Python sessions reading parquet files
# Windows: Check Task Manager for Python processes
```

---

## Key Takeaways for AI Assistant

### Essential Rules
1. **Search codebase first**: Always search within the codebase to see if any existing functions and/or classes can be used before building new logic
2. **Always analyze entire codebase** before making changes
3. **Execute all code in current virtual environment** (Bond-RV-App)
4. **Create tests in tests/ folder** when applicable
5. **Never create new files** unless explicitly required by user
6. **List file names and paths** when creating new files
7. **Include .cursorrules for context** in planning
8. **Map dependencies** before suggesting edits
9. **Extend existing logic** rather than duplicating
10. **Follow modular architecture**: config → utils → extract → transform → load → pipeline
11. **Respect logging patterns**: file logging (detailed) + console (minimal)

### Workflow Rules
12. **Think deeply** about prompt applicability before acting
13. **Use judgment** when interpreting conflicting requirements
14. **Notify user** when ignoring requirements or using judgment
15. **Ask clarifying questions** until all ambiguity is eliminated
16. **Provide file change summary** after completing requests
17. **ALWAYS clean up temporary files** created during chat sessions
18. **Always update .cursorrules** after applicable changes
19. **Include date/time stamps** in all documentation
20. **Update .cursorrules before git commits**

### Quality Standards
21. **Use pathlib.Path** for all path operations
22. **Avoid unicode issues** on Windows; all console/log output must remain ASCII-safe
23. **Modular design** following industry best practices
24. **Keep documentation current** when creating new docs

---

**File Created**: January 2025  
**Last Major Update**: 2025-11-08 11:30 ET  
**Project**: Bond RV App - Data Pipeline  
**Version**: 1.0

---

**Change Log**:
- **2025-01**: Initial comprehensive `.cursorrules` file created
- **2025-01**: Added AI Assistant Core Workflow section
- **2025-01**: Added git commit protocol and unicode handling
- **2025-01**: Enhanced documentation timestamp requirements
- **2025-01**: Added path handling and encoding rules
- **2025-01**: Removed poetry environment references, switched to standard pip/virtual environment
- **2025-01**: Removed all Outlook email pipeline functionality (monitor_outlook.py, runs_miner.py, utils/outlook_monitor.py, related documentation, and pywin32 dependency)
- **2025-01**: Added runs_pipeline module documentation:
  - Added runs_pipeline/ to module structure section
  - Added runs_timeseries.parquet to output data format section
  - Documented RUNS file pattern, header row, primary key (Date+Dealer+CUSIP)
  - Documented end-of-day snapshot deduplication logic (latest Time per Date+Dealer+CUSIP)
  - Documented CUSIP orphan tracking requirements (compare with universe.parquet)
  - Added runs_pipeline execution examples to workflow section
  - Updated class-based architecture section with runs pipeline classes
- **2025-01-02**: Performance optimizations:
  - Documented vectorized deduplication (~100x faster, O(n log n) vs O(n*groups))
  - Documented vectorized CUSIP validation (~100x faster, vectorized string operations)
  - Updated logging patterns to reflect summary logging instead of row-by-row
- **2025-01-02**: Enhanced orphan CUSIP logging:
  - Documented enhanced logging with context (Security, Date, Dealer, Time, Ticker)
  - Updated orphan tracking section to reflect detailed context in logs
- **2025-01-02**: Added unified pipeline orchestrator (run_pipeline.py):
  - Documented run_pipeline.py as recommended entry point
  - Added user selection for pipeline(s) and mode
- **2025-11-07 15:30 ET**: Added BQL workbook ingestion pipeline:
  - Documented `bql.xlsx` processing and `bql.parquet` output
  - Updated CLI/orchestrator guidance with `--process-bql` flag and prompt
  - Captured orphan logging requirements (CUSIP + Name) for BQL dataset
  - Added testing coverage notes for BQL helpers in `tests/unit/test_utils.py`
- **2025-11-07 16:20 ET**: Created/activated standard `Bond-RV-App` virtual environment and installed dependencies from `requirements.txt`
- **2025-11-07 16:45 ET**: Migrated to Poetry-based workflow (`pyproject.toml`, `poetry.lock`, in-project `.venv`), updated execution/testing instructions, and removed legacy `Bond-RV-App/` virtualenv
- **2025-11-08 11:05 ET**: Added `ipykernel` dev dependency and registered `Bond RV App (.venv)` Jupyter kernel for notebook execution
- **2025-11-08 11:20 ET**: Added `nbconvert` dev dependency to support automated notebook execution for testing kernels
- **2025-11-08 11:30 ET**: Added VS Code workspace settings to pin notebooks to the Poetry-managed `.venv` interpreter
- **2025-11-08 11:35 ET**: Added `notebook` dev dependency to ensure VS Code can start a local Jupyter server from the Poetry environment
- **2025-11-09 13:30 ET**: Optimized RUNS append flow (skip fully loaded files, new load metrics), enhanced run summaries, and introduced `parquet_stats.log` diagnostics
- **2025-11-09 07:25 ET**: Added `cr01_comb.py` CR01 pair analytics script (BQL universe vs holdings spreads) - **MOVED to `analytics/cr01_comb.py` on 2025-11-09 15:30 ET**
  - Exports single CSV file `cr01_pair_analytics.csv` to `analytics/processed_data/` (overwrites on each run, no timestamps)
  - Computes pairwise spreads (universe minus holdings) with statistical metrics (Last, Avg, vs Avg, Z Score, Percentile)
- **2025-11-09 13:30 ET**: Updated BQL workbook processing to handle 4-row multi-index header:
  - Modified `read_bql_workbook` to read with `header=[0,1,2,3]` for multi-index columns
  - Updated `reshape_bql_to_long` to extract names from level 1 (row 1) and CUSIPs from level 2 (row 2)
  - Data rows start at row 4 (index 4) after header rows 0-3
  - Updated config constants (`BQL_NAME_ROW_INDEX`, `BQL_CUSIP_ROW_INDEX`, `BQL_DATA_START_ROW`)
  - Fixed Date column handling for multi-index DataFrames (find tuple column name, rename to standard)
  - Updated `.cursorrules` documentation to reflect new header structure
- **2025-11-09 14:00 ET**: Enhanced logging and pipeline features:
  - Added `sanitize_log_message()` function for ASCII-safe log output (prevents Unicode encoding errors on Windows)
  - Updated file handlers to use UTF-8 encoding with `errors='replace'` for Unicode character handling
  - Added individual parquet file regeneration option (option 4) to `run_pipeline.py`:
    - Allows regenerating `historical_bond_details.parquet`, `bql.parquet`, or `runs_timeseries.parquet` independently
    - Useful for quick fixes or testing without running full pipelines
- **2025-11-09 15:30 ET**: Created analytics folder structure and pair analytics scripts:
  - Created `analytics/` folder with `processed_data/` subfolder for CSV outputs
  - Moved `cr01_comb.py` from root to `analytics/comb/cr01_comb.py` with updated paths (script-relative path resolution)
  - Created `analytics/comb/all_comb.py` - all combinations pair analytics script:
    - Computes all pairwise spreads from BQL parquet data (all CAD CUSIPs)
    - Filters to CAD CUSIPs from `historical_bond_details.parquet` (Currency="CAD" on last date)
    - Filters to CUSIPs present in most recent 75% of dates with complete data
    - Uses vectorized numpy operations for performance optimization
    - Outputs top 80 pairs sorted by Z Score to `all_comb.csv`
    - Column names: `Bond_1`, `Bond_2`, `cusip_1`, `cusip_2` (instead of universe/holdings naming)
  - Both scripts use script-relative path resolution for portability
- **2025-11-09 16:00 ET**: Enhanced data processing and statistics:
  - Added dealer filtering for runs_timeseries.parquet (only BMO, BNS, NBF, RBC, TD dealers stored)
  - Added numeric conversion for years columns: `Yrs Since Issue`, `Yrs (Worst)`, and `Yrs (Cvn)` are always stored as numeric (`float64`) in parquet files
  - Conversion happens at multiple points (transform and load) to ensure consistency
  - Updated enhanced statistics to use `Yrs (Cvn)` instead of `Yrs (Worst)` for binning analysis
  - Statistics now include proper binning with `<1` bin to capture negative and small values
- **2025-11-09 17:00 ET**: Created runs adjusted timeseries analytics script:
  - Created `analytics/runs/runs-adjusted_ts.py` - runs timeseries aggregation script (MERGED into runs_today.py on 2025-01-XX):
    - Reads `runs_timeseries.parquet` and groups by Date, CUSIP, and Benchmark
    - Computes 23 aggregated metrics including:
      - Basic columns: Date, CUSIP, Time (latest), Bid Workout Risk (average), Security
      - Spread metrics: Tight Bid >3mm, Wide Offer >3mm, Tight Bid, Wide Offer
      - Dealer and size columns for >3mm metrics
      - CR01 calculations: CR01 @ Tight Bid, CR01 @ Wide Offer (using Bid Workout Risk avg * Size / 10000)
      - Cumulative sizes: Cumm. Bid Size, Cumm. Offer Size
      - Counts: # of Bids >3mm, # of Offers >3mm (unique dealers)
      - RBC dealer columns: Bid RBC, Ask RBC, Bid Size RBC, Offer Size RBC
    - All numeric columns properly typed as `float64`
    - Sorts by Date (earliest to latest)
    - Exports to `runs_adjusted_ts.csv` in `analytics/processed_data/` (no longer generated)
    - Displays table preview and `df.info()` for validation
    - Uses script-relative path resolution for portability
- **2025-11-09 18:00 ET**: Created runs today analytics script:
  - Created `analytics/runs/runs_today.py` - daily runs analytics with Day-over-Day changes:
    - Reads `runs_timeseries.parquet` directly and aggregates required dates only (optimized)
    - Only includes CUSIPs present on both last date and second-to-last date
    - Computes aggregated metrics including:
      - Basic columns: CUSIP, Security, Bid Workout Risk
      - Spread metrics: Tight Bid >3mm, Wide Offer >3mm, Bid/Offer>3mm, Tight Bid, Wide Offer, Bid/Offer
      - Dealer columns: Current dealers and T-1 dealers (from second-to-last date)
      - Size, CR01, cumulative, and count metrics
      - RBC dealer columns
      - 16 Day-over-Day (DoD) change columns (Last Date - Second Last Date)
      - 16 MTD change columns (Last Date - First Date of Month)
      - 16 YTD change columns (Last Date - First Date of Year)
      - 16 1yr change columns (Last Date - Closest Date ~1 Year Ago)
    - Merges external columns from `historical_portfolio.parquet` (last date): QUANTITY, POSITION CR01
    - Merges external columns from `historical_bond_details.parquet` (most recent available date): G Sprd, Yrs (Cvn), vs BI, vs BCE, MTD Equity, YTD Equity, Retracement, Yrs Since Issue, Z Score, Retracement2, Rating
    - CUSIP normalization: Normalizes CUSIPs for matching (runs CUSIPs may not be normalized, bond/portfolio CUSIPs are normalized)
    - Portfolio aggregation: Sums QUANTITY and POSITION CR01 when multiple ACCOUNT/PORTFOLIO rows exist per CUSIP
    - Bond details date handling: Uses most recent available date if exact date match not found
    - Bid/Offer calculations only populate when both source columns have values
    - All numeric columns properly typed as `float64`
    - Sorts by CUSIP
    - Exports to `runs_today.csv` in `analytics/processed_data/`
    - Comprehensive logging to `bond_data/logs/runs_today.log`:
      - Merge statistics (match counts, percentages, unmatched CUSIPs)
      - Full `df.info()` output for input parquet and final CSV (verbose mode)
      - Column summaries and sample data
      - CUSIP normalization details
    - Uses script-relative path resolution for portability
    - Configuration section at top for easy column management
- **2025-01-XX XX:XX ET**: Merged runs-adjusted_ts.py into runs_today.py and optimized date processing:
  - Merged `analytics/runs/runs-adjusted_ts.py` into `analytics/runs/runs_today.py` (single flow)
  - Removed intermediate `runs_adjusted_ts.csv` output file (no longer generated)
  - Optimized to process only required dates (last, second-to-last, MTD ref, YTD ref, 1yr ref) instead of all dates
  - Reads directly from `runs_timeseries.parquet` instead of intermediate CSV
  - Single workflow: aggregation → today analysis → CSV output
  - Keeps `compute_group_metrics()` and `ensure_ascii()` as helper functions
  - Improved performance by filtering parquet data before grouping/aggregation
  - Deleted `runs-adjusted_ts.py` file
- **2025-01-XX XX:XX ET**: Enhanced runs today analytics with MTD, YTD, and 1yr changes:
  - Added Month-to-Date (MTD) change columns for all DoD columns (compares last date with first available date of current month)
  - Added Year-to-Date (YTD) change columns for all DoD columns (compares last date with first available date of current year)
  - Added 1-year (1yr) change columns for all DoD columns (compares last date with closest available date approximately 1 year ago)
  - Naming convention: `DoD Chg {column}` → `MTD Chg {column}`, `YTD Chg {column}`, `1yr Chg {column}`
  - All MTD, YTD, and 1yr columns properly typed as `float64`
  - Column ordering: DoD columns first, then MTD, then YTD, then 1yr (grouped by change type)
  - Handles cases where reference dates don't exist (returns NA values)
  - Updated module and function docstrings to reflect new change calculations
- **2025-01-XX XX:XX ET**: Created portfolio combinations pair analytics script:
  - Created `analytics/comb/port_comb.py` - portfolio pair analytics script:
    - Similar to `all_comb.py` but filters pairs where `cusip_2` is in portfolio CUSIP list
    - Computes all pairwise combinations first, then filters to portfolio CUSIPs, then selects top 80 by Z Score
    - Portfolio CUSIP list defined as constant (59 CUSIPs, normalized to uppercase)
    - Shows bond names (not just CUSIPs) in warning messages for missing portfolio CUSIPs from BQL data
    - Uses same filtering logic as `all_comb.py` (CAD CUSIPs, recent dates, complete data)
    - Outputs to `port_comb.csv` in `analytics/processed_data/`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Created portfolio executable combinations pair analytics script:
  - Created `analytics/comb/port_executable.py` - portfolio executable pair analytics script:
    - Similar to `port_comb.py` but adds additional filter on `cusip_1`
    - Filters `cusip_1` to CUSIPs from `runs_today.csv` where `CR01 @ Wide Offer >= 2000`
    - Filters `cusip_2` to portfolio CUSIPs (same as `port_comb.py`)
    - Both filters applied (AND condition) after computing all pairs
    - Raises error if `CR01 @ Wide Offer` column missing or no matching CUSIPs found
    - Excludes rows with NaN/null `CR01 @ Wide Offer` values
    - Outputs to `port_executable.csv` in `analytics/processed_data/`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Created CAD cheap vs USD pair analytics script:
  - Created `analytics/comb/cad_cheap_vs_usd.py` - CAD vs USD pair analytics script:
    - Based on `all_comb.py` but removes CAD-only filter from BQL data
    - Computes all pairwise combinations first, then filters results
    - Filters to pairs where `cusip_1` has Currency="CAD" and `cusip_2` has Currency="USD"
    - Requires matching Ticker values (from last date in historical_bond_details.parquet)
    - Requires matching Custom_Sector values (from last date)
    - Requires absolute difference in "Yrs (Cvn)" <= 2 (from last date)
    - Excludes CUSIPs with missing/null Currency, Ticker, Custom_Sector, or Yrs (Cvn)
    - Outputs top 80 pairs sorted by Z Score to `cad_cheap_vs_usd.csv`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Created CAD rich vs USD pair analytics script:
  - Created `analytics/comb/cad_rich_vs_usd.py` - USD vs CAD pair analytics script:
    - Same as `cad_cheap_vs_usd.py` but with swapped Currency roles
    - Filters to pairs where `cusip_1` has Currency="USD" and `cusip_2` has Currency="CAD"
    - Requires matching Ticker values (from last date in historical_bond_details.parquet)
    - Requires matching Custom_Sector values (from last date)
    - Requires absolute difference in "Yrs (Cvn)" <= 2 (from last date)
    - Excludes CUSIPs with missing/null Currency, Ticker, Custom_Sector, or Yrs (Cvn)
    - Outputs top 80 pairs sorted by Z Score to `cad_rich_vs_usd.csv`
    - Uses script-relative path resolution for portability
    - Spread calculation: `cusip_1_values - cusip_2_values` (USD - CAD, opposite of cheap version)
- **2025-01-XX XX:XX ET**: Created term combinations pair analytics script:
  - Created `analytics/comb/term_comb.py` - term combinations pair analytics script:
    - Based on `all_comb.py` but adds pre-filtering by "Yrs (Cvn)" difference
    - Filters pairs where `abs(yrs_cvn_1 - yrs_cvn_2) <= 0.8` before computing spreads
    - Filters to CAD CUSIPs with valid "Yrs (Cvn)" data on last date
    - Excludes CUSIPs without "Yrs (Cvn)" data
    - Outputs top 80 pairs sorted by Z Score to `term_comb.csv`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Created ticker combinations pair analytics script:
  - Created `analytics/comb/ticker_comb.py` - ticker combinations pair analytics script:
    - Based on `all_comb.py` but adds pre-filtering by matching Ticker values
    - Filters pairs where `ticker_1 == ticker_2` (exact match, case-sensitive) before computing spreads
    - Filters to CAD CUSIPs with valid Ticker data on last date
    - Excludes CUSIPs without Ticker data
    - Outputs top 80 pairs sorted by Z Score to `ticker_comb.csv`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Created custom sector combinations pair analytics script:
  - Created `analytics/comb/custom_sector.py` - custom sector combinations pair analytics script:
    - Based on `all_comb.py` but adds pre-filtering by matching Custom_Sector values
    - Filters pairs where `custom_sector_1 == custom_sector_2` (exact match, case-sensitive) before computing spreads
    - Filters to CAD CUSIPs with valid Custom_Sector data on last date
    - Excludes CUSIPs without Custom_Sector data
    - Outputs top 80 pairs sorted by Z Score to `custom_sector_comb.csv`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Created custom bond combinations pair analytics script:
  - Created `analytics/comb/custom_bond_comb.py` - custom bond combinations pair analytics script:
    - Pairs all CAD CUSIPs (cusip_1) with a fixed target bond (cusip_2)
    - Target bond configurable via `TARGET_BOND_TICKER` (takes precedence) or `TARGET_BOND_SECURITY`
    - Searches `universe.parquet` for target bond(s) by Ticker (exact match) or Security name
    - If Ticker specified and multiple bonds match, uses all matching CUSIPs (creates pairs for each)
    - Falls back to Security name if Ticker not found (with warning)
    - Displays Security name in Bond_2 column
    - Filters to CAD CUSIPs present in most recent 75% of dates
    - Validates target bond(s) have BQL data and are in recent dates
    - Outputs all pairs sorted by Z Score to `custom_bond_comb.csv`
    - Uses script-relative path resolution for portability
    - Configuration section at top: `TARGET_BOND_TICKER`, `TARGET_BOND_SECURITY`, `RECENT_DATE_PERCENT`, `TOP_N_PAIRS`
- **2025-01-XX XX:XX ET**: Created custom bond vs holdings pair analytics script:
  - Created `analytics/comb/custom_bond_vs_holdings.py` - custom bond vs holdings pair analytics script:
    - Pairs target CAD bonds (cusip_1/Bond_1) from `TARGET_BOND_TICKER/SECURITY` with CR01-filtered holdings (cusip_2/Bond_2)
    - cusip_1: Filters to CAD CUSIPs first, then finds target bond(s) via TARGET_BOND_TICKER/SECURITY in universe.parquet
    - cusip_2: Loads holdings with CR01 @ Tight Bid > 2000 from runs_today.csv, matched with portfolio (last date)
    - Both cusip_1 and cusip_2 filtered to recent dates (75% of dates) using BQL data
    - Spread calculation: cusip_1 - cusip_2
    - BQL data required only for cusip_1 (target bonds); holdings filtered to recent dates but don't require BQL data themselves
    - Outputs all pairs sorted by Z Score to `custom_bond_vs_holdings.csv`
    - Uses script-relative path resolution for portability
    - Configuration section at top: `TARGET_BOND_TICKER`, `TARGET_BOND_SECURITY`, `RECENT_DATE_PERCENT`, `TOP_N_PAIRS`
- **2025-01-XX XX:XX ET**: Added codebase search requirement:
  - Added rule to "Before Making Changes" section: Always search codebase first to see if existing functions/classes can be used before building new logic
  - Added rule to "Essential Rules" section in Key Takeaways to reinforce this principle
  - Ensures reuse of existing code and prevents unnecessary duplication
- **2025-01-XX XX:XX ET**: Replaced SCM with BNS dealer name throughout runs pipeline:
  - Added SCM to BNS replacement in transform step (before deduplication)
  - Added SCM to BNS replacement in load step (when reading existing data and before writing)
  - Updated ALLOWED_DEALERS to include BNS: ['BMO', 'BNS', 'NBF', 'RBC', 'TD']
  - Updated RUNS_KNOWN_DEALERS to replace SCM with BNS for validation consistency
  - Updated documentation to reflect BNS in allowed dealers list
- **2025-01-XX XX:XX ET**: Reorganized analytics folder structure:
  - Created `analytics/comb/` subfolder for all pair analytics scripts (9 scripts)
  - Created `analytics/runs/` subfolder for runs analytics scripts (2 scripts)
  - Updated all path references in analytics scripts:
    - Parquet paths: `SCRIPT_DIR.parent.parent` (2 levels up to project root)
    - Output directory: `SCRIPT_DIR.parent / "processed_data"` (1 level up to `analytics/processed_data/`)
    - Cross-references: `SCRIPT_DIR.parent / "processed_data" / "*.csv"` (for scripts reading other analytics outputs)
  - All scripts tested and verified to run correctly from new locations
- **2025-01-XX XX:XX ET**: Added Portfolio Pipeline implementation:
  - Created `portfolio_pipeline/` module with extract, transform, load, and pipeline modules
  - Added portfolio configuration to `bond_pipeline/config.py`:
    - `PORTFOLIO_INPUT_DIR` - Input directory path (AD History)
    - `PORTFOLIO_FILE_PATTERN` - Filename pattern regex (Aggies MM.DD.YY.xlsx)
    - `PORTFOLIO_HEADER_ROW` - Header row index (0)
    - `PORTFOLIO_PARQUET` - Output parquet file path
    - `PORTFOLIO_PRIMARY_KEY` - Primary key columns ['Date', 'CUSIP', 'ACCOUNT', 'PORTFOLIO']
  - Added `extract_portfolio_date_from_filename()` function to `bond_pipeline/utils.py`
  - Portfolio extract module:
    - Reads Excel files with header row 0 (row 1 in Excel)
    - Extracts date from filename (Aggies MM.DD.YY.xlsx pattern)
    - Automatically removes Unnamed columns (Unnamed: 0, Unnamed: 1, etc.)
  - Portfolio transform module:
    - CUSIP normalization (same as bond pipeline: uppercase, validate 9 chars)
    - Row filtering: Drops rows where either SECURITY or CUSIP is blank/NaN/empty
    - Deduplication: Date+CUSIP+ACCOUNT+PORTFOLIO primary key (keeps last occurrence)
    - NA cleaning and schema alignment (82 columns, dynamically detected)
  - Portfolio load module:
    - Append/override modes with date checking
    - Date+CUSIP+ACCOUNT+PORTFOLIO primary key enforcement
  - Portfolio pipeline orchestrator:
    - CLI interface matching bond/runs pipeline patterns
    - Enhanced run summaries with statistics
  - Updated `run_pipeline.py`:
    - Added option 3: "Portfolio Pipeline only"
    - Updated option 5: "Individual Parquet Files" (was option 4)
    - Added option 4 in individual parquet files: `historical_portfolio.parquet`
  - Updated `bond_pipeline/utils.py`:
    - Added portfolio statistics section to `log_enhanced_parquet_stats()`:
      - Date range
      - Unique CUSIPs per Date (formatted table)
      - CUSIPs not in universe.parquet (Date, CUSIP, Security) - formatted table
    - Updated `log_parquet_diagnostics()` to include `historical_portfolio.parquet`
  - Output: `historical_portfolio.parquet` with Date+CUSIP+ACCOUNT+PORTFOLIO primary key
  - All 82 columns stored (excluding Unnamed columns)
  - Rows with blank SECURITY or CUSIP automatically filtered out
- **2025-01-XX XX:XX ET**: Created dynamic CR01 pair analytics scripts:
  - Created `analytics/comb/executable cr01 decent bid offer vs holdings.py`:
    - Dynamically loads CR01 holdings CUSIPs from runs_today.csv (CR01 @ Tight Bid > 2000, Bid/Offer>3mm < 3) matched with portfolio (last date)
    - Dynamically loads CR01 universe CUSIPs from runs_today.csv (CR01 @ Wide Offer > 2000, Bid/Offer>3mm < 3) matched with portfolio (last date)
    - Computes all pairwise spreads (universe minus holdings) with statistical metrics
    - Outputs to `executable cr01 decent bid offer vs holdings.csv`
    - Uses script-relative path resolution for portability
  - Created `analytics/comb/executable cr01 vs holdings.py`:
    - Same as above but removes Bid/Offer>3mm filters (only CR01 thresholds)
    - Outputs to `executable cr01 vs holdings.csv`
  - Created `analytics/comb/all combos vs holdings.py`:
    - Loads all CUSIPs from historical_portfolio.parquet (last date) for both universe and holdings
    - No CR01 filters - computes all pairwise combinations within portfolio
    - Outputs to `all combos vs holdings.csv`
    - Uses script-relative path resolution for portability
- **2025-01-XX XX:XX ET**: Consolidated pair analytics into single Excel output:
  - Created `analytics/comb/comb.py` - consolidated script that runs all 11 pair analytics types
  - Replaced individual CSV file outputs with single `comb.xlsx` Excel file
  - Each analysis type gets its own sheet with formatted Excel tables (filters, banded rows, auto-fitted columns)
  - Tab names use shorter versions (max 31 characters) mapped via `EXCEL_TAB_NAMES` configuration
  - Numeric columns formatted to 1 decimal place
  - Empty analyses automatically skipped (no empty sheets)
  - Still generates `comb.txt` (text summary) and `comb_validation.txt` (validation log) for reference
  - All individual analysis functions updated to return DataFrames instead of writing CSVs
  - Excel tables use `TableStyleMedium9` style with filters and banded rows enabled
- **2025-01-XX XX:XX ET**: Added "xxcy_diff" column to CAD/USD pair analytics:
  - Added `get_cad_equiv_swap_mapping()` helper function to load CAD Equiv Swap values from `historical_bond_details.parquet` (last date)
  - Updated `run_cad_cheap_vs_usd_analysis()` and `run_cad_rich_vs_usd_analysis()` to include "xxcy_diff" column
  - Column calculation: `xxcy_diff = CAD Equiv Swap (cusip_1) - CAD Equiv Swap (cusip_2)`
  - Column positioned immediately after "Last" column in results DataFrame
  - Values set to `None` (NaN) if either CUSIP lacks CAD Equiv Swap data
  - Column formatted to 1 decimal place (via `format_numeric_columns()`)
  - Appears in Excel output (`comb.xlsx`), text summary (`comb.txt`), and console output
- **2025-11-14 18:30 ET**: Enhanced bond pipeline numeric conversion and runs_today.py merge functionality:
  - Extended `convert_years_to_numeric()` in `bond_pipeline/transform.py` to convert spread/metric columns to `float64`:
    - Added: `G Sprd`, `vs BI`, `vs BCE`, `MTD Equity`, `YTD Equity`, `Retracement`, `Z Score`, `Retracement2`
    - These columns are now stored as numeric in `historical_bond_details.parquet` instead of object/string
  - Updated `bond_pipeline/load.py` to convert numeric columns in both append and override modes
  - Added merge functionality to `analytics/runs/runs_today.py`:
    - Merges QUANTITY and POSITION CR01 from `historical_portfolio.parquet` (last date, aggregated by CUSIP)
    - Merges 11 columns from `historical_bond_details.parquet` (most recent available date)
    - CUSIP normalization for matching (runs CUSIPs normalized before merge)
    - Flexible configuration at top of file for easy column management
  - Added comprehensive logging to `runs_today.py`:
    - Creates `bond_data/logs/runs_today.log` with detailed merge statistics
    - Logs CUSIP normalization details and match statistics
    - Logs full `df.info()` output for input parquet and final CSV (verbose mode)
    - Logs column-level match percentages and sample matched/unmatched CUSIPs
    - Logs final DataFrame summary and sample data rows

