# Universal Pattern Discovery Framework

## When to Use

- Use this framework when you need to uncover repeatable trading patterns aligned to a target CAGR and must enforce stringent clarification before analysis begins.
- Apply it for exploratory research projects that involve bar construction, labeling, meta-labeling, and multi-stage validation across assets.
- Reference it to ensure agents probe risk constraints, data availability, and output preferences prior to launching discovery workflows.
- Consult it when reviewing pattern-mining results to verify that multiple testing corrections, out-of-sample checks, and stress scenarios were addressed.
- For simple descriptive analytics or ad-hoc pattern spotting, lighter tools suffice; once you're formalizing a discovery pipeline, treat this document as mandatory guidance.

## ⚠️ CRITICAL: AI MUST CLARIFY SPECIFICATIONS BEFORE IMPLEMENTATION

**Before implementing ANY pattern discovery or analysis, the AI MUST activate the Precision Clarification Engine and ask/clarify the following specifications:**

### **PHASE 1: PRECISION CLARIFICATION ENGINE - UNIVERSAL OBJECTIVE**

```
"To ensure we precisely target your specific CAGR objective for any asset/strategy, I need to clarify the following. I will ask 2-3 questions for simple tasks, and 5-7 for complex ones, aiming for 95% confidence before proceeding. I will also probe for edge cases and challenge assumptions.

Please provide specific answers for each point, or confirm the default if it aligns with your goals.

**MANDATORY SPECIFICATION QUESTIONS (Universal Framework):**

#### **1. Core Objective Parameters (REQUIRED)**
- **Target CAGR**: `[AI MUST ASK: "What is your exact target Compound Annual Growth Rate? (e.g., 4%, 15%, 25%)"]`
- **Target Asset/Strategy**: `[AI MUST ASK: "Which specific asset or strategy are we analyzing? (e.g., SPX, CAD IG ER, BTC, Custom Portfolio)"]`
- **Leverage Allowed**: `[AI MUST ASK: "Is the use of leverage permitted? (Default: No)"]`
- **Shorting Allowed**: `[AI MUST ASK: "Is short selling allowed for this strategy? (Default: No)"]`
- **Positioning Type**: `[AI MUST ASK: "What type of positioning is required? (Binary, Percentage-based, Kelly Criterion, Custom)"]`
- **Minimum Holding Period**: `[AI MUST ASK: "What is the minimum number of days a trade must be held? (Default: 1 day)"]`
- **Maximum Holding Period**: `[AI MUST ASK: "Is there a maximum holding period for trades? (Default: No Max)"]`
- **Signal to Trade Lag**: `[AI MUST ASK: "What is the timing relationship between signal generation and trade execution? (Default: Signal t+0, Trade t+1 day)"]`

#### **2. Data & Market Context (REQUIRED)**
- **Data Frequency**: `[AI MUST ASK: "What is the frequency of your data? (Daily, Hourly, Minute, Tick)"]`
- **Date Range**: `[AI MUST ASK: "What is the specific historical date range for analysis? (Default: Whole available range)"]`
- **Data Source**: `[AI MUST ASK: "What is the primary source of your data? (CSV, API, Database, Bloomberg)"]`
- **Volume Data Availability**: `[AI MUST ASK: "Do you have reliable volume data? (Crucial for certain bar types)"]`
- **Alternative Data Sources**: `[AI MUST ASK: "Do you have access to alternative data? (Macro indicators, sentiment, flow data, etc.)"]`

#### **3. Trading Objectives & Constraints (REQUIRED)**
- **Risk Tolerance**: `[AI MUST ASK: "What is your maximum acceptable drawdown? (e.g., 5%, 10%, 20%)"]`
- **Implementation Language**: `[AI MUST ASK: "What programming language should the implementation be in? (Default: Python)"]`
- **Real-time Trading**: `[AI MUST ASK: "Is this strategy intended for real-time trading or purely backtesting? (Default: Backtesting only)"]`
- **ML Complexity**: `[AI MUST ASK: "What level of Machine Learning complexity are you comfortable with? (Simple, Advanced, Both)"]`

#### **4. Pattern Discovery & Validation Scope (REQUIRED)**
- **Pattern Types to Prioritize**: `[AI MUST ASK: "Which categories of patterns should be prioritized? (Technical, Fundamental, Macro, Cross-asset, All)"]`
- **Statistical Significance Threshold**: `[AI MUST ASK: "What p-value threshold do you require for statistical significance? (Default: 0.05)"]`
- **Out-of-Sample Validation**: `[AI MUST ASK: "Do you require rigorous out-of-sample validation? (Default: Yes)"]`
- **Walk-Forward Period**: `[AI MUST ASK: "What walk-forward period do you prefer for validation? (Default: 252 days training, 63 days testing)"]`

#### **5. AI Interaction & Output Preferences (REQUIRED)**
- **Output Format**: `[AI MUST ASK: "What is your preferred output format for results and code? (Markdown, Jupyter Notebook, Python script)"]`
- **Level of Detail**: `[AI MUST ASK: "What level of detail do you require in explanations? (High-level, Detailed, Expert-level)"]`
- **Interactivity**: `[AI MUST ASK: "Do you prefer interactive elements (plots, dashboards) in the output?"]`
- **Assumptions**: `[AI MUST ASK: "Are there any specific assumptions I should make or avoid?"]`
```

### **AI IMPLEMENTATION PROTOCOL (Enhanced with Optimized Prompt Engineer)**

**The AI MUST follow this sequence, integrating the Optimized Prompt Engineer phases:**

1. **PHASE 1: PRECISION CLARIFICATION ENGINE**:
   - **ASK ALL MANDATORY SPECIFICATION QUESTIONS** (from section 1 above) before any implementation.
   - **CONFIRM EACH ANSWER** with the user, ensuring 95% confidence in understanding.
   - **PROBE FOR EDGE CASES** and challenge assumptions related to the specific CAGR objective and constraints.

2. **PHASE 2: ELITE PERSPECTIVE ANALYSIS - Strategy-Specific Analysis**:
   - Once specifications are clear, activate this phase to analyze the CAGR objective from a top 0.1% quantitative researcher perspective.
   - **MARKET INEFFICIENCY ANALYSIS**:
     - What specific inefficiency in the target market are we exploiting to achieve the target CAGR with these constraints?
     - How does this compare to institutional strategies for this asset class?
     - What competitive advantage does this pattern provide given the positioning and holding period constraints?
     - What are the hidden risks and edge cases for the target CAGR under these conditions?
   - **STRATEGIC PATTERN SELECTION**:
     - Which pattern types are most likely to align with the target CAGR, constraints, and asset characteristics?
     - How do these patterns complement each other to achieve the target?
     - What market regimes are most conducive to these patterns generating the target CAGR?

3. **PHASE 3: PARADIGM CHALLENGE - Breakthrough Discovery**:
   - Challenge conventional thinking about achieving the target CAGR with the given constraints.
   - **ASSUMPTION VALIDATION**:
     - Are there any implicit assumptions in the CAGR target or constraints that need to be re-evaluated?
     - What if the market structure changes? How robust is the CAGR target?
   - **ALTERNATIVE PERSPECTIVES**:
     - Could a non-obvious combination of patterns yield superior results?
     - Are there any "black swan" patterns that could be identified?

4. **PHASE 4: CONCEPTUAL VISUALIZATION - Complex Method Mapping**:
   - Visually map the proposed pattern discovery and strategy generation process, especially for complex ML or multi-scale techniques, to ensure clarity.

5. **PHASE 5: NOBEL LAUREATE RESEARCH METHODOLOGY**:
   - Apply scientific rigor to the pattern discovery process for the CAGR objective.
   - **FUNDAMENTAL RESEARCH QUESTIONS**:
     - What fundamental market principles are we testing to achieve the target CAGR?
     - How can we design experiments to prove/disprove hypotheses about patterns leading to the target CAGR?
   - **SCIENTIFIC RIGOR REQUIREMENTS**:
     - Null hypothesis testing for each pattern identified.
     - Multiple testing corrections for pattern discovery.
     - Out-of-sample validation with proper statistical tests, specifically targeting the CAGR objective.

6. **CUSTOMIZE THE FRAMEWORK** based on all clarified specifications and insights from the above phases.

7. **PROVIDE DETAILED IMPLEMENTATION PLAN** with specific parameters and chosen techniques.

8. **ONLY THEN PROCEED** with pattern discovery, backtesting, and strategy generation.

---

## **Universal Pattern Discovery & Strategy Generation Approach**

This framework leverages advanced methodologies that can be applied to ANY asset class, strategy type, or CAGR target. The focus is on identifying robust patterns that specifically meet the return target while respecting all specified constraints.

### **1. Advanced Financial Data Engineering & Sampling**

#### **Information-Driven Bars (When to Use)**
```
Data Type → Use Case → Recommended Bar Type

High-Frequency Data (Tick Level):
├── Cross-asset comparison → Dollar Bars
├── Volume analysis → Volume Bars  
├── Market microstructure → Tick Bars
└── Imbalance detection → Imbalance Bars

Daily/Intraday Data:
├── Standard analysis → Time Bars (existing)
├── Volatility analysis → Dollar Bars
└── Activity-based → Volume Bars

Low-Liquidity Assets:
├── Always use → Dollar Bars
└── Avoid → Tick Bars (too sparse)
```

#### **Implementation Framework**
```python
# Universal Information-Driven Bar Implementation
class UniversalInformationBars:
    def __init__(self, bar_type='dollar', threshold=1000000):
        self.bar_type = bar_type
        self.threshold = threshold
        self.bars = []
    
    def create_bars(self, data):
        """Create information-driven bars based on use case"""
        
        if self.bar_type == 'dollar':
            return self.create_dollar_bars(data)
        elif self.bar_type == 'volume':
            return self.create_volume_bars(data)
        elif self.bar_type == 'tick':
            return self.create_tick_bars(data)
        elif self.bar_type == 'imbalance':
            return self.create_imbalance_bars(data)
    
    def create_dollar_bars(self, data):
        """Dollar bars - Use for cross-asset comparison and analysis"""
        cumulative_dollar_volume = (data['price'] * data['volume']).cumsum()
        bars = []
        
        current_bar_start = 0
        for i in range(len(data)):
            if cumulative_dollar_volume.iloc[i] - cumulative_dollar_volume.iloc[current_bar_start] >= self.threshold:
                bar_data = data.iloc[current_bar_start:i+1]
                bars.append(self.create_bar_summary(bar_data))
                current_bar_start = i + 1
        
        return pd.DataFrame(bars)
    
    def create_bar_summary(self, bar_data):
        """Create OHLCV summary for a bar"""
        return {
            'timestamp': bar_data.index[-1],
            'open': bar_data['price'].iloc[0],
            'high': bar_data['price'].max(),
            'low': bar_data['price'].min(),
            'close': bar_data['price'].iloc[-1],
            'volume': bar_data['volume'].sum(),
            'dollar_volume': (bar_data['price'] * bar_data['volume']).sum()
        }
```

### **2. Advanced Labeling & Meta-Labeling Framework**

#### **Triple-Barrier Method (When to Use)**
```
Market Condition → Volatility → Labeling Method

High Volatility (VIX > 30):
├── Use Triple-Barrier with dynamic thresholds
├── Barrier: 2-3x ATR
└── Time horizon: 5-10 days

Medium Volatility (VIX 15-30):
├── Use Triple-Barrier with moderate thresholds
├── Barrier: 1.5-2x ATR
└── Time horizon: 10-20 days

Low Volatility (VIX < 15):
├── Use Triple-Barrier with tight thresholds
├── Barrier: 1-1.5x ATR
└── Time horizon: 20-30 days

Trending Markets:
├── Use asymmetric barriers
├── Up barrier: 1.5x ATR
└── Down barrier: 2x ATR

Mean Reverting Markets:
├── Use symmetric barriers
├── Both barriers: 1.5x ATR
└── Shorter time horizon
```

#### **Implementation**
```python
# Universal Triple-Barrier Method Implementation
class UniversalTripleBarrierLabeling:
    def __init__(self, upper_multiplier=2.0, lower_multiplier=2.0, 
                 max_holding_period=20, min_holding_period=1):
        self.upper_multiplier = upper_multiplier
        self.lower_multiplier = lower_multiplier
        self.max_holding_period = max_holding_period
        self.min_holding_period = min_holding_period
    
    def create_labels(self, prices, volatility_estimator='atr', 
                     volatility_window=20, regime_detector=None):
        """Create triple-barrier labels with dynamic thresholds"""
        
        labels = []
        
        for i in range(len(prices)):
            if i < volatility_window:
                continue
            
            # Calculate dynamic volatility
            vol = self.calculate_volatility(prices, i, volatility_window, volatility_estimator)
            
            # Adjust barriers based on market regime
            if regime_detector:
                regime = regime_detector.get_regime(prices.iloc[:i+1])
                upper_mult, lower_mult = self.adjust_barriers_for_regime(regime)
            else:
                upper_mult, lower_mult = self.upper_multiplier, self.lower_multiplier
            
            # Calculate barriers
            current_price = prices.iloc[i]
            upper_barrier = current_price + (upper_mult * vol)
            lower_barrier = current_price - (lower_mult * vol)
            
            # Find label outcome
            label = self.find_label_outcome(prices, i, upper_barrier, lower_barrier)
            labels.append(label)
        
        return pd.DataFrame(labels)
```

### **3. Machine Learning Enhancements for Pattern Discovery**

#### **Purged Cross-Validation (When to Use)**
```
Data Characteristics → CV Method

Overlapping Labels:
├── Use Purged Cross-Validation
├── Purge period: 1-5 days
└── Avoid Standard K-Fold

Serial Correlation:
├── Use Purged Cross-Validation
├── Embargo period: 1-3 days
└── Avoid Standard K-Fold

High-Frequency Data:
├── Use Purged Cross-Validation
├── Longer purge/embargo periods
└── Avoid Standard K-Fold

Low-Frequency Data (Daily+):
├── Purged CV still recommended
├── Shorter purge/embargo periods
└── Standard K-Fold acceptable if no overlap

Independent Samples:
├── Use Standard K-Fold
└── Purged CV not necessary
```

#### **Implementation**
```python
# Universal Purged Cross-Validation Implementation
class UniversalPurgedCrossValidation:
    def __init__(self, n_splits=5, purge_period=1, embargo_period=1):
        self.n_splits = n_splits
        self.purge_period = purge_period
        self.embargo_period = embargo_period
    
    def split(self, X, y=None, groups=None):
        """Generate purged cross-validation splits"""
        n_samples = len(X)
        indices = np.arange(n_samples)
        
        # Calculate split sizes
        split_size = n_samples // self.n_splits
        
        for i in range(self.n_splits):
            # Calculate test indices
            test_start = i * split_size
            test_end = (i + 1) * split_size if i < self.n_splits - 1 else n_samples
            
            test_indices = indices[test_start:test_end]
            
            # Calculate purge period
            purge_start = max(0, test_start - self.purge_period)
            purge_end = min(n_samples, test_end + self.purge_period)
            purge_indices = indices[purge_start:purge_end]
            
            # Calculate embargo period
            embargo_start = max(0, test_start - self.embargo_period)
            embargo_end = min(n_samples, test_end + self.embargo_period)
            embargo_indices = indices[embargo_start:embargo_end]
            
            # Calculate train indices (excluding purge and embargo)
            train_indices = np.setdiff1d(indices, np.union1d(purge_indices, embargo_indices))
            
            yield train_indices, test_indices
```

### **4. Advanced Statistical Pattern Recognition**

#### **Fractal Analysis & Chaos Theory**
```python
# Universal Fractal Analysis Implementation
class UniversalFractalAnalysis:
    def __init__(self):
        self.analysis_results = {}
    
    def fractal_pattern_analysis(self, data):
        """Advanced fractal analysis for pattern detection"""
        from scipy.stats import entropy
        import numpy as np
        
        # Hurst Exponent for long-memory patterns
        def hurst_exponent(ts):
            lags = range(2, 20)
            tau = [np.sqrt(np.std(np.subtract(ts[lag:], ts[:-lag]))) for lag in lags]
            poly = np.polyfit(np.log(lags), np.log(tau), 1)
            return poly[0] * 2.0
        
        # Fractal Dimension using Box-Counting
        def fractal_dimension(data, max_box_size=20):
            box_sizes = np.logspace(0.5, np.log10(max_box_size), 10).astype(int)
            counts = []
            for box_size in box_sizes:
                count = 0
                for i in range(0, len(data) - box_size, box_size):
                    if np.max(data[i:i+box_size]) - np.min(data[i:i+box_size]) > 0:
                        count += 1
                counts.append(count)
            return -np.polyfit(np.log(box_sizes), np.log(counts), 1)[0]
        
        return {
            'hurst': hurst_exponent(data),
            'fractal_dim': fractal_dimension(data),
            'entropy': entropy(data)
        }
```

### **5. Advanced Regime Detection & Change Point Analysis**

#### **Hidden Markov Models for Regime Detection**
```python
# Universal HMM Regime Detection
class UniversalHMMRegimeDetection:
    def __init__(self, n_states=3):
        self.n_states = n_states
        self.model = None
    
    def detect_regimes(self, data):
        """Detect market regimes using HMM"""
        from hmmlearn import hmm
        import numpy as np
        
        # Gaussian HMM for regime detection
        model = hmm.GaussianHMM(n_components=self.n_states, covariance_type="full", n_iter=100)
        model.fit(data.reshape(-1, 1))
        
        # Predict regimes
        regimes = model.predict(data.reshape(-1, 1))
        regime_probs = model.predict_proba(data.reshape(-1, 1))
        
        # Regime characteristics
        regime_stats = {}
        for i in range(self.n_states):
            regime_data = data[regimes == i]
            regime_stats[f'regime_{i}'] = {
                'mean': np.mean(regime_data),
                'std': np.std(regime_data),
                'duration_mean': np.mean(np.diff(np.where(np.diff(regimes) != 0)[0])),
                'transition_prob': model.transmat_[i]
            }
        
        return {
            'regimes': regimes,
            'regime_probabilities': regime_probs,
            'regime_statistics': regime_stats,
            'transition_matrix': model.transmat_,
            'model': model
        }
```

### **6. Pattern Evolution & Adaptive Modification**

#### **Pattern Lifecycle Tracking**
```python
# Universal Pattern Lifecycle Management
class UniversalPatternLifecycle:
    def __init__(self, pattern_id, birth_date):
        self.pattern_id = pattern_id
        self.birth_date = birth_date
        self.current_phase = 'birth'
        self.performance_history = []
        self.parameter_history = []
        self.usage_count = 0
        self.last_used = None
    
    def update_performance(self, performance):
        """Update pattern performance and lifecycle phase"""
        self.performance_history.append({
            'date': datetime.now(),
            'performance': performance
        })
        
        # Determine lifecycle phase
        if len(self.performance_history) < 10:
            self.current_phase = 'birth'
        elif self.get_recent_performance() > 0.1:
            self.current_phase = 'maturity'
        elif self.get_recent_performance() < -0.05:
            self.current_phase = 'decay'
        else:
            self.current_phase = 'stable'
    
    def get_recent_performance(self, days=30):
        """Get recent performance over specified days"""
        recent_perfs = [p['performance'] for p in self.performance_history 
                      if (datetime.now() - p['date']).days <= days]
        return np.mean(recent_perfs) if recent_perfs else 0
```

### **7. Multi-Scale Analysis & Wavelet Pattern Detection**

#### **Wavelet Analysis for Multi-Resolution Patterns**
```python
# Universal Wavelet Pattern Analysis
class UniversalWaveletAnalysis:
    def __init__(self):
        self.wavelet_results = {}
    
    def wavelet_pattern_analysis(self, data):
        """Wavelet-based pattern detection across multiple scales"""
        import pywt
        import numpy as np
        
        # Continuous Wavelet Transform
        def continuous_wavelet_transform(data, wavelet='morlet', scales=None):
            if scales is None:
                scales = np.logspace(0, 2, 20)  # 20 scales from 1 to 100
            
            coefficients, frequencies = pywt.cwt(data, scales, wavelet)
            return coefficients, frequencies
        
        # Discrete Wavelet Transform
        def discrete_wavelet_transform(data, wavelet='db4', level=4):
            coeffs = pywt.wavedec(data, wavelet, level=level)
            return coeffs
        
        # Multi-scale pattern detection
        def detect_multiscale_patterns(data):
            wavelets = ['db4', 'haar', 'coif2', 'bior2.2']
            patterns = {}
            
            for wavelet in wavelets:
                coeffs = pywt.wavedec(data, wavelet, level=4)
                
                patterns[wavelet] = {
                    'approximation': coeffs[0],  # Low frequency trend
                    'details': coeffs[1:],      # High frequency details
                    'energy_distribution': [np.sum(c**2) for c in coeffs],
                    'pattern_strength': np.sum([np.sum(c**2) for c in coeffs[1:]])
                }
            
            return patterns
        
        return {
            'cwt_coeffs': continuous_wavelet_transform(data),
            'dwt_coeffs': discrete_wavelet_transform(data),
            'multiscale_patterns': detect_multiscale_patterns(data)
        }
```

### **8. Pattern Synthesis Engine**

#### **Pattern Combination & Generation**
```python
# Universal Pattern Synthesis Engine
class UniversalPatternSynthesizer:
    def __init__(self, patterns):
        self.patterns = patterns
        self.synthesis_history = []
    
    def combine_patterns(self, pattern1, pattern2, combination_method='weighted_average'):
        """Create new pattern by combining existing ones"""
        if combination_method == 'weighted_average':
            weights = self.optimize_combination_weights(pattern1, pattern2)
            combined_pattern = self.weighted_combination(pattern1, pattern2, weights)
        
        elif combination_method == 'genetic_crossover':
            combined_pattern = self.genetic_crossover(pattern1, pattern2)
        
        elif combination_method == 'neural_fusion':
            combined_pattern = self.neural_pattern_fusion(pattern1, pattern2)
        
        return combined_pattern
    
    def generate_pattern_variations(self, base_pattern, n_variations=10):
        """Generate variations of a base pattern"""
        variations = []
        
        for i in range(n_variations):
            mutated_params = self.mutate_parameters(base_pattern.get_parameters())
            
            variation = base_pattern.copy()
            variation.update_parameters(mutated_params)
            variation.id = f"{base_pattern.id}_variation_{i}"
            
            variations.append(variation)
        
        return variations
```

### **9. Genetic Algorithm Optimization**

#### **Multi-Objective Optimization**
```python
# Universal Multi-Objective Genetic Algorithm
class UniversalMultiObjectiveGA:
    def __init__(self, population_size=100, generations=50, 
                 objectives=['sharpe_ratio', 'total_return'], 
                 constraints=None):
        self.population_size = population_size
        self.generations = generations
        self.objectives = objectives
        self.constraints = constraints or {}
        self.population = []
        self.pareto_front = []
        self.convergence_history = []
    
    def initialize_population(self, parameter_space):
        """Initialize random population"""
        self.population = []
        
        for _ in range(self.population_size):
            individual = self.create_random_individual(parameter_space)
            self.population.append(individual)
        
        return self.population
    
    def evolve(self, data, parameter_space):
        """Main evolution loop"""
        self.initialize_population(parameter_space)
        
        for generation in range(self.generations):
            # Evaluate objectives for all individuals
            for individual in self.population:
                individual['objectives'] = self.evaluate_objectives(individual, data)
            
            # Find Pareto front
            current_pareto_front = self.find_pareto_front(self.population)
            self.pareto_front = current_pareto_front
            
            # Track convergence
            if current_pareto_front:
                avg_objectives = np.mean([ind['objectives'] for ind in current_pareto_front], axis=0)
                self.convergence_history.append(avg_objectives)
            
            # Create new population
            new_population = []
            new_population.extend(current_pareto_front)
            
            # Generate offspring
            while len(new_population) < self.population_size:
                parent1 = self.tournament_selection(self.population)
                parent2 = self.tournament_selection(self.population)
                
                child1, child2 = self.crossover(parent1, parent2)
                child1 = self.mutate(child1, parameter_space)
                child2 = self.mutate(child2, parameter_space)
                
                new_population.extend([child1, child2])
            
            self.population = new_population[:self.population_size]
        
        return self.pareto_front
```

### **10. VectorBT Backtesting & Validation Framework**

#### **Precise Backtesting Implementation**
```python
# Universal VectorBT Integration
class UniversalVectorBTIntegration:
    def __init__(self, precision_requirement='high', validation_level='comprehensive'):
        self.precision_requirement = precision_requirement
        self.validation_level = validation_level
        self.portfolio = None
        self.validation_results = {}
    
    def create_precise_portfolio(self, prices, signals, config=None):
        """Create VectorBT portfolio with precision requirements"""
        
        # Default configuration based on precision requirement
        if self.precision_requirement == 'high':
            default_config = {
                'init_cash': 100000,
                'fees': 0.001,
                'slippage': 0.0005,
                'freq': '1D',
                'call_seq': 'auto',
                'size_type': 'amount',
                'direction': 'longonly',
                'accumulate': False,
                'cash_sharing': False,
                'update_value': True,
                'ffill_val_price': True
            }
        else:
            default_config = {
                'init_cash': 100000,
                'fees': 0.001,
                'freq': '1D'
            }
        
        # Merge with provided configuration
        if config:
            default_config.update(config)
        
        # Create VectorBT portfolio
        import vectorbt as vbt
        
        self.portfolio = vbt.Portfolio.from_signals(
            close=prices,
            entries=signals == 1,
            exits=signals == -1,
            **default_config
        )
        
        return self.portfolio
    
    def validate_precision(self, manual_calculator=None, tolerance=0.02):
        """Validate precision against manual calculations"""
        
        if self.validation_level == 'comprehensive':
            validation_results = self.run_comprehensive_validation(manual_calculator, tolerance)
        else:
            validation_results = self.run_basic_validation(manual_calculator, tolerance)
        
        self.validation_results = validation_results
        return validation_results
```

---

## **Universal Decision Framework**

### **Master Decision Tree: When to Use What**

```
Your Goal → Data Type → Market Condition → Recommended Approach

Pattern Discovery:
├── High-frequency data → Volatile markets → Information-driven bars + Fractional differentiation + Purged CV
├── Daily data → Stable markets → Time bars + Standard differentiation + Walk-forward analysis
└── Low-frequency data → Trending markets → Dollar bars + Fractional differentiation + Regime analysis

Strategy Development:
├── Single strategy → Any market → Triple-barrier labeling + Meta-labeling + Random Forest
├── Multiple strategies → Volatile markets → Ensemble methods + Purged CV + Stress testing
└── Portfolio strategies → Any market → Multi-objective GA + Risk management + VectorBT

Validation & Testing:
├── Research phase → Any data → Comprehensive validation + Monte Carlo + Bootstrap
├── Development phase → Any data → Basic validation + Walk-forward + Stress testing
└── Production phase → Any data → Full validation + Automated testing + QuantStats

Risk Management:
├── Tail risk focus → High volatility → VaR + Expected Shortfall + Stress testing
├── Drawdown focus → Any market → Drawdown metrics + Regime analysis + Stress testing
└── Volatility focus → Any market → Volatility metrics + GARCH + Stress testing
```

### **Implementation Priority Matrix**

```
Phase 1 (Essential - Implement First):
├── Information-driven bars (dollar bars for most assets)
├── Fractional differentiation (d=0.3 for memory preservation)
├── Triple-barrier labeling (dynamic thresholds)
├── Purged cross-validation (prevent data leakage)
└── VectorBT integration (precise backtesting)

Phase 2 (Important - Implement Second):
├── Meta-labeling framework (position sizing)
├── Walk-forward analysis (out-of-sample validation)
├── Advanced risk metrics (tail risk, drawdown)
├── Monte Carlo validation (statistical significance)
└── Feature importance analysis (MDA, SFI)

Phase 3 (Enhancement - Implement Third):
├── Genetic algorithm optimization (multi-objective)
├── Stress testing framework (scenario analysis)
├── Sequential bootstrap (non-IID data)
├── Clustered feature importance (multicollinearity)
└── Regime analysis (market condition adaptation)

Phase 4 (Advanced - Implement Last):
├── Deep learning patterns (LSTM, autoencoders)
├── Reinforcement learning (adaptive strategies)
├── Alternative data integration (sentiment, flow)
├── High-frequency microstructure (tick data)
└── Multi-asset correlation (cross-market analysis)
```

---

## **Next Steps for AI**

1. **Activate Phase 1: Precision Clarification Engine** and ask the user all mandatory specification questions listed above.
2. **Confirm each answer** with the user.
3. **Proceed through Phases 2-5** to gain deeper insights and refine the approach.
4. **Present a detailed implementation plan** based on the clarified specifications and the chosen advanced techniques.
5. **Await user confirmation** before proceeding with any code generation or pattern discovery.

---

**This universal framework can be applied to ANY asset class, strategy type, or CAGR target. Simply specify your requirements and the AI will customize the approach accordingly.**
