# Financial Data Structures and Engineering

## When to Use

- Use this file when architecting data ingestion pipelines or evaluating whether your raw market, fundamental, or alternative data is structured correctly for ML workflows.
- Apply it before building features that depend on information-driven bars, multi-asset alignment, or advanced sampling techniques.
- Reference it when onboarding engineers so they understand why time bars fail, how to create dollar/volume bars, and how to manage multi-product series.
- Consult it during data-quality audits or vendor comparisons; the guide highlights prerequisites, edge cases, and implementation pitfalls.
- For quick reminders of specific functions, you can jump to the relevant section, but treat this document as the foundational manual for data structuring decisions.

**Comprehensive Guide to Data Structuring for Algorithmic Trading and Financial Machine Learning**

---

## Table of Contents

1. [Introduction](#introduction)
2. [Essential Types of Financial Data](#essential-types-of-financial-data)
3. [The Problem with Standard Time Bars](#the-problem-with-standard-time-bars)
4. [Information-Driven Bars](#information-driven-bars)
5. [Advanced Bar Types](#advanced-bar-types)
6. [Dealing with Multi-Product Series](#dealing-with-multi-product-series)
7. [Sampling Techniques](#sampling-techniques)
8. [Alternative Data Sources](#alternative-data-sources)
9. [Implementation Examples](#implementation-examples)
10. [Best Practices](#best-practices)

---

## Introduction

The foundation of any successful machine learning application in finance begins with proper data structuring. Raw financial data arrives in various forms and at irregular intervals, presenting unique challenges that do not exist in other domains. As López de Prado emphasizes, **"The quality of the data structure determines the quality of the features, which in turn determines the quality of the model."**

This document provides an exhaustive treatment of financial data structures, drawing from both *Advances in Financial Machine Learning* by Marcos López de Prado and *Machine Learning for Algorithmic Trading* by Stefan Jansen. We will explore not only the theoretical foundations but also practical implementation details with extensive examples.

---

## Essential Types of Financial Data

Financial data can be categorized into four fundamental types, each with distinct characteristics, update frequencies, and use cases.

### 1. Fundamental Data

Fundamental data represents the economic and financial health of companies, economies, and markets. This data is typically released on a periodic basis (quarterly, annually) and is relatively low-frequency compared to market data.

**Characteristics:**
- **Low Frequency:** Updated quarterly (10-Q), annually (10-K), or at specific events
- **Structured Format:** Standardized financial statements (balance sheet, income statement, cash flow)
- **Backward-Looking:** Reflects historical performance
- **Subject to Restatements:** Can be revised after initial release

**Examples:**
- **Corporate Financials:** Revenue, earnings per share (EPS), EBITDA, debt-to-equity ratio
- **Macroeconomic Indicators:** GDP growth, unemployment rate, inflation (CPI, PPI)
- **Industry Metrics:** Same-store sales, capacity utilization, inventory levels

**Data Sources (from Jansen):**
- **SEC EDGAR:** Free access to all US public company filings
- **Financial data APIs:** Quandl, Alpha Vantage, IEX Cloud
- **Commercial providers:** Bloomberg, Refinitiv, FactSet

**Use Cases:**
- Long-term investment strategies
- Fundamental factor models
- Credit risk assessment
- Macro regime classification

### 2. Market Data

Market data is the high-frequency information generated by the trading process itself. This is the most granular and voluminous type of financial data.

**Characteristics:**
- **High Frequency:** Can arrive in microseconds
- **Irregular Intervals:** Not evenly spaced in time
- **Large Volume:** Billions of records per day for major exchanges
- **Multiple Levels:** Trade data, quotes, order book depth

**Types of Market Data:**

| Data Type | Description | Typical Fields | Frequency |
|-----------|-------------|----------------|-----------|
| **Trades (Ticks)** | Executed transactions | Price, volume, timestamp, buyer/seller initiated | Every transaction |
| **Quotes** | Best bid and offer | Bid price, ask price, bid size, ask size, timestamp | Every quote update |
| **Order Book (Level 2)** | Full depth of market | All bid/ask prices and sizes at each level | Every order book change |
| **Order Book (Level 3)** | Individual orders | Order ID, price, size, side, timestamp | Every order event |

**Example Trade Data Structure:**
```
Timestamp            | Symbol | Price   | Volume | Side
---------------------|--------|---------|--------|------
2025-01-15 09:30:00.123 | AAPL   | 150.25  | 100    | B
2025-01-15 09:30:00.456 | AAPL   | 150.26  | 50     | S
2025-01-15 09:30:01.789 | AAPL   | 150.24  | 200    | B
```

**Data Sources:**
- **Exchange Data Feeds:** Nasdaq TotalView-ITCH, NYSE OpenBook
- **Data Vendors:** Tick Data, Kibot, AlgoSeek
- **APIs:** Interactive Brokers, Alpaca, Polygon.io

### 3. Analytics

Analytics are derived data products created by processing and analyzing other data sources. These are typically produced by analysts, rating agencies, or automated systems.

**Examples:**
- **Credit Ratings:** Moody's, S&P, Fitch ratings
- **Analyst Estimates:** Consensus earnings forecasts, price targets
- **Sentiment Scores:** News sentiment, social media sentiment
- **Technical Indicators:** Pre-calculated RSI, MACD, Bollinger Bands
- **Volatility Surfaces:** Implied volatility from options markets

**Considerations:**
- Often proprietary and expensive
- May contain look-ahead bias if not carefully constructed
- Can be crowded (many traders using same signals)
- Quality varies significantly by provider

### 4. Alternative Data

Alternative data encompasses any non-traditional data source that can provide informational edge. This is the fastest-growing category and represents a major opportunity for alpha generation.

**Categories of Alternative Data (from Jansen):**

#### Individual-Generated Data
Data created by individuals through their online and offline activities.

**Examples:**
- **Social Media:** Twitter sentiment, Reddit discussions, StockTwits
- **Search Trends:** Google Trends, Bing search volume
- **Product Reviews:** Amazon reviews, Yelp ratings, app store reviews
- **Geolocation Data:** Foot traffic to retail stores (SafeGraph, Foursquare)

**Use Case Example:**
Analyzing foot traffic data to predict same-store sales growth before official earnings announcements.

#### Business Process Data
Data generated as a byproduct of business operations.

**Examples:**
- **Credit Card Transactions:** Aggregated spending patterns
- **Supply Chain Data:** Shipping manifests, customs records
- **Email Receipts:** Purchase confirmations (Second Measure, Earnest)
- **Job Postings:** LinkedIn, Indeed, Glassdoor hiring trends

**Use Case Example:**
Monitoring job postings in R&D departments to predict innovation cycles and future product launches.

#### Sensor Data
Data collected by physical or digital sensors.

**Examples:**
- **Satellite Imagery:** Parking lot occupancy, agricultural yield estimation, oil storage levels
- **Weather Data:** Temperature, precipitation, extreme events
- **IoT Sensors:** Industrial equipment usage, energy consumption
- **Ship Tracking:** AIS data for commodity shipping (Kpler, Vortexa)

**Use Case Example:**
Using satellite imagery to count cars in retail parking lots to predict quarterly sales before earnings.

**Evaluation Criteria for Alternative Data:**

| Criterion | Description | Questions to Ask |
|-----------|-------------|------------------|
| **Exclusivity** | How many others have access? | Is this data widely available or proprietary? |
| **Actionability** | Can it be traded on? | Is there sufficient liquidity and time to act? |
| **Frequency** | How often is it updated? | Does update frequency match strategy horizon? |
| **History** | How much historical data exists? | Is there enough data for backtesting? |
| **Legal/Ethical** | Is usage compliant? | Are there insider trading or privacy concerns? |
| **Cost** | What is the total cost? | Does potential alpha justify the expense? |

---

## The Problem with Standard Time Bars

Time-based sampling (e.g., 1-minute, 5-minute, daily bars) is the most common method for aggregating tick data. However, this approach has fundamental flaws that make it suboptimal for machine learning applications.

### Statistical Properties of Time Bars

López de Prado identifies several critical issues with time bars:

**1. Non-Homogeneous Information Content**

The amount of information arriving in a fixed time interval varies dramatically throughout the trading day.

**Example:**
```
Time Period          | Number of Trades | Information Content
---------------------|------------------|--------------------
09:30 - 09:35 (open) | 15,000          | Very High
12:00 - 12:05 (lunch)| 500             | Very Low
15:55 - 16:00 (close)| 12,000          | Very High
```

A 5-minute bar at market open contains 30x more information than a 5-minute bar at lunch, yet they are treated identically by the model.

**2. Violation of IID Assumption**

Returns from time bars are not independently and identically distributed (IID), which violates a key assumption of many statistical models.

**Empirical Evidence:**
- **Serial Correlation:** Time bar returns exhibit significant autocorrelation
- **Heteroskedasticity:** Volatility clusters and changes throughout the day
- **Non-Normality:** Fat tails and skewness in return distributions

**3. Oversampling During Low Activity**

During periods of low trading activity, time bars oversample noise rather than signal.

**4. Undersampling During High Activity**

During periods of high activity (news events, market stress), time bars undersample, missing important price discovery.

### Mathematical Formulation

For a time bar of length Δt, the return is:

r_t = log(P_t) - log(P_{t-Δt})

Where P_t is the price at time t.

The problem is that the **information arrival rate λ(t)** is not constant:

λ(t) ≠ constant

This leads to returns with time-varying properties:

Var(r_t | t) = σ²(t) ≠ constant

---

## Information-Driven Bars

To address the limitations of time bars, López de Prado proposes **information-driven bars**, which sample based on market activity rather than clock time. The key insight is to sample when a certain amount of information has been revealed, not when a certain amount of time has passed.

### Tick Bars

**Definition:** Sample every T transactions (ticks), regardless of time elapsed.

**Construction Algorithm:**
```
1. Initialize: bar_ticks = [], tick_count = 0, T = threshold
2. For each tick in data stream:
   a. Add tick to bar_ticks
   b. tick_count += 1
   c. If tick_count == T:
      - Create bar from bar_ticks
      - Reset: bar_ticks = [], tick_count = 0
```

**Example:**
If T = 1000, a new bar is created every 1000 transactions.

```python
def create_tick_bars(ticks, threshold):
    """
    Create tick bars from tick data
    
    Parameters:
    -----------
    ticks : pd.DataFrame
        Tick data with columns: timestamp, price, volume
    threshold : int
        Number of ticks per bar
    
    Returns:
    --------
    pd.DataFrame : Tick bars with OHLCV data
    """
    bars = []
    tick_count = 0
    bar_ticks = []
    
    for idx, tick in ticks.iterrows():
        bar_ticks.append(tick)
        tick_count += 1
        
        if tick_count >= threshold:
            bar = {
                'timestamp': bar_ticks[-1]['timestamp'],
                'open': bar_ticks[0]['price'],
                'high': max([t['price'] for t in bar_ticks]),
                'low': min([t['price'] for t in bar_ticks]),
                'close': bar_ticks[-1]['price'],
                'volume': sum([t['volume'] for t in bar_ticks]),
                'tick_count': tick_count
            }
            bars.append(bar)
            bar_ticks = []
            tick_count = 0
    
    return pd.DataFrame(bars)
```

**Advantages:**
- More uniform information content per bar
- Adapts to changing market activity
- Better statistical properties than time bars

**Disadvantages:**
- Sensitive to order splitting (large orders broken into many small ticks)
- Doesn't account for trade size

### Volume Bars

**Definition:** Sample every V shares/contracts traded, regardless of number of ticks or time.

**Construction Algorithm:**
```
1. Initialize: bar_data = [], cumulative_volume = 0, V = threshold
2. For each tick in data stream:
   a. Add tick to bar_data
   b. cumulative_volume += tick.volume
   c. If cumulative_volume >= V:
      - Create bar from bar_data
      - Reset: bar_data = [], cumulative_volume = 0
```

**Example:**
If V = 100,000, a new bar is created every 100,000 shares traded.

```python
def create_volume_bars(ticks, threshold):
    """
    Create volume bars from tick data
    
    Parameters:
    -----------
    ticks : pd.DataFrame
        Tick data with columns: timestamp, price, volume
    threshold : float
        Volume threshold per bar
    
    Returns:
    --------
    pd.DataFrame : Volume bars with OHLCV data
    """
    bars = []
    cumulative_volume = 0
    bar_ticks = []
    
    for idx, tick in ticks.iterrows():
        bar_ticks.append(tick)
        cumulative_volume += tick['volume']
        
        if cumulative_volume >= threshold:
            bar = {
                'timestamp': bar_ticks[-1]['timestamp'],
                'open': bar_ticks[0]['price'],
                'high': max([t['price'] for t in bar_ticks]),
                'low': min([t['price'] for t in bar_ticks]),
                'close': bar_ticks[-1]['price'],
                'volume': cumulative_volume,
                'tick_count': len(bar_ticks)
            }
            bars.append(bar)
            bar_ticks = []
            cumulative_volume = 0
    
    return pd.DataFrame(bars)
```

**Advantages:**
- More robust to order splitting than tick bars
- Accounts for trade size
- Better reflects actual market participation

**Disadvantages:**
- Doesn't account for price changes
- A 100-share trade at $10 has same weight as 100-share trade at $1000

### Dollar Bars

**Definition:** Sample every D dollars traded, calculated as Σ(price × volume).

**Mathematical Formula:**

Create new bar when: Σ(P_i × V_i) ≥ D

Where:
- P_i = price of tick i
- V_i = volume of tick i
- D = dollar threshold

**Construction Algorithm:**
```
1. Initialize: bar_data = [], cumulative_dollars = 0, D = threshold
2. For each tick in data stream:
   a. Add tick to bar_data
   b. cumulative_dollars += tick.price * tick.volume
   c. If cumulative_dollars >= D:
      - Create bar from bar_data
      - Reset: bar_data = [], cumulative_dollars = 0
```

**Example:**
If D = $1,000,000, a new bar is created every $1 million traded.

```python
def create_dollar_bars(ticks, threshold):
    """
    Create dollar bars from tick data
    
    Parameters:
    -----------
    ticks : pd.DataFrame
        Tick data with columns: timestamp, price, volume
    threshold : float
        Dollar value threshold per bar
    
    Returns:
    --------
    pd.DataFrame : Dollar bars with OHLCV data
    """
    bars = []
    cumulative_dollars = 0
    bar_ticks = []
    
    for idx, tick in ticks.iterrows():
        bar_ticks.append(tick)
        cumulative_dollars += tick['price'] * tick['volume']
        
        if cumulative_dollars >= threshold:
            bar = {
                'timestamp': bar_ticks[-1]['timestamp'],
                'open': bar_ticks[0]['price'],
                'high': max([t['price'] for t in bar_ticks]),
                'low': min([t['price'] for t in bar_ticks]),
                'close': bar_ticks[-1]['price'],
                'volume': sum([t['volume'] for t in bar_ticks]),
                'dollar_volume': cumulative_dollars,
                'tick_count': len(bar_ticks)
            }
            bars.append(bar)
            bar_ticks = []
            cumulative_dollars = 0
    
    return pd.DataFrame(bars)
```

**Advantages:**
- **Most robust** of the three basic information-driven bars
- Accounts for both price and volume
- Normalizes across different price regimes
- Recommended by López de Prado as the default choice

**Statistical Properties:**

López de Prado demonstrates that dollar bars have superior statistical properties:

| Property | Time Bars | Tick Bars | Volume Bars | Dollar Bars |
|----------|-----------|-----------|-------------|-------------|
| **Return Normality** | Poor | Fair | Good | Excellent |
| **Serial Correlation** | High | Medium | Low | Lowest |
| **Heteroskedasticity** | High | Medium | Low | Lowest |
| **Information Homogeneity** | Poor | Fair | Good | Excellent |

---

## Advanced Bar Types

Beyond the basic information-driven bars, López de Prado introduces more sophisticated bar types that can capture even more nuanced market dynamics.

### Imbalance Bars

Imbalance bars are constructed based on the cumulative signed order flow, where each trade is classified as buyer-initiated or seller-initiated.

**Tick Imbalance Bars (TIBs)**

**Definition:** Sample when the cumulative signed tick imbalance exceeds a threshold.

**Algorithm:**
```
1. Initialize: θ_t = 0 (cumulative imbalance), T = threshold
2. For each tick:
   a. Determine tick direction: b_t ∈ {-1, +1}
   b. θ_t += b_t
   c. If |θ_t| >= T:
      - Create bar
      - Reset θ_t = 0
```

**Tick Classification (Tick Rule):**
```python
def classify_tick(price, prev_price):
    """
    Classify tick as buy (+1) or sell (-1) using tick rule
    
    Parameters:
    -----------
    price : float
        Current trade price
    prev_price : float
        Previous trade price
    
    Returns:
    --------
    int : +1 for buy, -1 for sell
    """
    if price > prev_price:
        return 1  # Buy
    elif price < prev_price:
        return -1  # Sell
    else:
        return 0  # No change (use previous classification)
```

**Full Implementation:**
```python
def create_tick_imbalance_bars(ticks, threshold):
    """
    Create tick imbalance bars
    
    Parameters:
    -----------
    ticks : pd.DataFrame
        Tick data with columns: timestamp, price, volume
    threshold : float
        Imbalance threshold
    
    Returns:
    --------
    pd.DataFrame : Tick imbalance bars
    """
    bars = []
    bar_ticks = []
    cumulative_imbalance = 0
    prev_price = None
    prev_sign = 1
    
    for idx, tick in ticks.iterrows():
        # Classify tick
        if prev_price is None:
            sign = 1
        elif tick['price'] > prev_price:
            sign = 1
        elif tick['price'] < prev_price:
            sign = -1
        else:
            sign = prev_sign
        
        bar_ticks.append({**tick, 'sign': sign})
        cumulative_imbalance += sign
        
        if abs(cumulative_imbalance) >= threshold:
            bar = {
                'timestamp': bar_ticks[-1]['timestamp'],
                'open': bar_ticks[0]['price'],
                'high': max([t['price'] for t in bar_ticks]),
                'low': min([t['price'] for t in bar_ticks]),
                'close': bar_ticks[-1]['price'],
                'volume': sum([t['volume'] for t in bar_ticks]),
                'buy_volume': sum([t['volume'] for t in bar_ticks if t['sign'] > 0]),
                'sell_volume': sum([t['volume'] for t in bar_ticks if t['sign'] < 0]),
                'imbalance': cumulative_imbalance
            }
            bars.append(bar)
            bar_ticks = []
            cumulative_imbalance = 0
        
        prev_price = tick['price']
        prev_sign = sign
    
    return pd.DataFrame(bars)
```

**Volume Imbalance Bars (VIBs)**

Similar to TIBs, but weighted by volume:

θ_t = Σ(b_i × V_i)

**Dollar Imbalance Bars (DIBs)**

Weighted by dollar volume:

θ_t = Σ(b_i × P_i × V_i)

### Run Bars

Run bars are created when there is a sequence (run) of buy or sell trades.

**Tick Run Bars**

**Definition:** Sample when the number of consecutive buys or sells exceeds a threshold.

**Algorithm:**
```
1. Initialize: run_length = 0, current_sign = None, T = threshold
2. For each tick:
   a. Determine tick sign: b_t
   b. If b_t == current_sign:
      - run_length += 1
   c. Else:
      - run_length = 1
      - current_sign = b_t
   d. If run_length >= T:
      - Create bar
      - Reset run_length = 0
```

**Use Case:**
Run bars are particularly useful for identifying momentum and detecting potential reversals.

---

## Dealing with Multi-Product Series

When working with futures contracts, indices, or portfolios, we often need to combine data from multiple instruments into a single continuous series.

### The ETF Trick

**Problem:** Futures contracts expire and need to be rolled. This creates discontinuities in the price series.

**Solution:** Use an ETF or index that tracks the same underlying as a reference for returns.

**Example:**
For crude oil futures (CL), use USO (United States Oil Fund ETF) to calculate returns:

```python
def etf_trick(futures_prices, etf_prices):
    """
    Calculate continuous returns using ETF as reference
    
    Parameters:
    -----------
    futures_prices : pd.Series
        Futures contract prices (with rolls)
    etf_prices : pd.Series
        ETF prices (continuous)
    
    Returns:
    --------
    pd.Series : Continuous futures series
    """
    # Calculate ETF returns
    etf_returns = etf_prices.pct_change()
    
    # Apply ETF returns to futures prices
    continuous_futures = futures_prices.iloc[0] * (1 + etf_returns).cumprod()
    
    return continuous_futures
```

### PCA Weights

**Problem:** Combine multiple related instruments into a single synthetic series.

**Solution:** Use Principal Component Analysis (PCA) to determine optimal weights.

**Algorithm:**
```
1. Collect price series for N related instruments
2. Calculate returns matrix R (T × N)
3. Compute covariance matrix Σ = R^T R
4. Perform eigendecomposition: Σ = Q Λ Q^T
5. Use first eigenvector as weights
```

**Implementation:**
```python
import numpy as np
from sklearn.decomposition import PCA

def pca_weights(returns_df):
    """
    Calculate PCA weights for combining multiple series
    
    Parameters:
    -----------
    returns_df : pd.DataFrame
        Returns for multiple instruments (columns)
    
    Returns:
    --------
    np.array : Weights for first principal component
    """
    # Fit PCA
    pca = PCA(n_components=1)
    pca.fit(returns_df.dropna())
    
    # Get first principal component weights
    weights = pca.components_[0]
    
    # Normalize to sum to 1
    weights = weights / np.sum(np.abs(weights))
    
    return weights

# Example usage
# returns_df has columns: ['CL_1', 'CL_2', 'CL_3'] for different futures contracts
weights = pca_weights(returns_df)
synthetic_returns = (returns_df * weights).sum(axis=1)
```

### Single Future Roll

**Panama Method:** Roll on a specific date each month (e.g., 5 days before expiration).

**Proportional Method:** Gradually shift weight from front contract to back contract.

```python
def proportional_roll(front_prices, back_prices, roll_days=5):
    """
    Create continuous series using proportional roll
    
    Parameters:
    -----------
    front_prices : pd.Series
        Front month contract prices
    back_prices : pd.Series
        Back month contract prices
    roll_days : int
        Number of days over which to roll
    
    Returns:
    --------
    pd.Series : Continuous price series
    """
    continuous = pd.Series(index=front_prices.index)
    
    # Determine roll period
    roll_start = front_prices.index[-roll_days]
    
    for date in front_prices.index:
        if date < roll_start:
            # Before roll: 100% front contract
            continuous[date] = front_prices[date]
        elif date >= roll_start and date < front_prices.index[-1]:
            # During roll: proportional mix
            days_into_roll = (date - roll_start).days
            weight_back = days_into_roll / roll_days
            weight_front = 1 - weight_back
            continuous[date] = (weight_front * front_prices[date] + 
                              weight_back * back_prices[date])
        else:
            # After roll: 100% back contract
            continuous[date] = back_prices[date]
    
    return continuous
```

---

## Sampling Techniques

### Sampling for Reduction

When dealing with high-frequency data, it may be necessary to reduce the data volume for computational efficiency.

**Strategies:**
1. **Fixed-Interval Sampling:** Sample every N-th observation
2. **Stratified Sampling:** Ensure representation from different time periods
3. **Importance Sampling:** Sample more frequently during high-volatility periods

### Event-Based Sampling

Sample when specific market events occur.

**Examples:**
- **CUSUM Filter:** Sample when cumulative sum of price changes exceeds threshold
- **Volatility Breakout:** Sample when realized volatility spikes
- **Volume Surge:** Sample when volume exceeds moving average by threshold

**CUSUM Filter Implementation:**
```python
def cusum_filter(prices, threshold):
    """
    Apply CUSUM filter to detect significant price movements
    
    Parameters:
    -----------
    prices : pd.Series
        Price series
    threshold : float
        Threshold for cumulative sum (in price units or %)
    
    Returns:
    --------
    pd.DatetimeIndex : Timestamps where events occurred
    """
    events = []
    s_pos = 0  # Positive cumulative sum
    s_neg = 0  # Negative cumulative sum
    diff = prices.diff()
    
    for timestamp, change in diff.items():
        if pd.isna(change):
            continue
            
        s_pos = max(0, s_pos + change)
        s_neg = min(0, s_neg + change)
        
        if s_pos > threshold:
            events.append(timestamp)
            s_pos = 0
            s_neg = 0
        elif s_neg < -threshold:
            events.append(timestamp)
            s_pos = 0
            s_neg = 0
    
    return pd.DatetimeIndex(events)
```

---

## Alternative Data Sources

### Working with Alternative Data

Jansen provides extensive coverage of alternative data sources and how to integrate them into trading strategies.

**Example: Sentiment Analysis from News**

```python
from textblob import TextBlob
import pandas as pd

def calculate_news_sentiment(news_df):
    """
    Calculate sentiment scores from news headlines
    
    Parameters:
    -----------
    news_df : pd.DataFrame
        News data with columns: timestamp, headline, source
    
    Returns:
    --------
    pd.DataFrame : Sentiment scores by timestamp
    """
    sentiments = []
    
    for idx, row in news_df.iterrows():
        blob = TextBlob(row['headline'])
        sentiment = {
            'timestamp': row['timestamp'],
            'polarity': blob.sentiment.polarity,  # -1 to 1
            'subjectivity': blob.sentiment.subjectivity,  # 0 to 1
            'source': row['source']
        }
        sentiments.append(sentiment)
    
    sentiment_df = pd.DataFrame(sentiments)
    
    # Aggregate by time period (e.g., daily)
    daily_sentiment = sentiment_df.groupby(
        sentiment_df['timestamp'].dt.date
    ).agg({
        'polarity': 'mean',
        'subjectivity': 'mean'
    })
    
    return daily_sentiment
```

**Example: Satellite Imagery Analysis**

```python
# Pseudocode for satellite imagery analysis
def analyze_parking_lot_occupancy(image_path, roi_coordinates):
    """
    Analyze parking lot occupancy from satellite imagery
    
    Parameters:
    -----------
    image_path : str
        Path to satellite image
    roi_coordinates : list
        Coordinates of region of interest (parking lot)
    
    Returns:
    --------
    float : Estimated occupancy rate (0 to 1)
    """
    # Load image
    image = load_image(image_path)
    
    # Extract region of interest
    roi = extract_roi(image, roi_coordinates)
    
    # Detect cars using computer vision
    car_count = detect_cars(roi)
    
    # Estimate total parking spaces
    total_spaces = estimate_parking_spaces(roi)
    
    # Calculate occupancy
    occupancy_rate = car_count / total_spaces
    
    return occupancy_rate
```

---

## Implementation Examples

### Complete Pipeline: From Ticks to Features

```python
import pandas as pd
import numpy as np

class DataStructure:
    """
    Complete data structuring pipeline for financial ML
    """
    
    def __init__(self, tick_data):
        """
        Initialize with tick data
        
        Parameters:
        -----------
        tick_data : pd.DataFrame
            Raw tick data with columns: timestamp, price, volume
        """
        self.tick_data = tick_data.sort_values('timestamp')
        self.bars = None
        
    def create_bars(self, bar_type='dollar', threshold=1000000):
        """
        Create information-driven bars
        
        Parameters:
        -----------
        bar_type : str
            Type of bars: 'time', 'tick', 'volume', 'dollar'
        threshold : float
            Threshold for bar creation
        """
        if bar_type == 'dollar':
            self.bars = self._create_dollar_bars(threshold)
        elif bar_type == 'volume':
            self.bars = self._create_volume_bars(threshold)
        elif bar_type == 'tick':
            self.bars = self._create_tick_bars(threshold)
        else:
            raise ValueError(f"Unknown bar type: {bar_type}")
        
        return self.bars
    
    def _create_dollar_bars(self, threshold):
        """Create dollar bars"""
        bars = []
        cumulative_dollars = 0
        bar_ticks = []
        
        for idx, tick in self.tick_data.iterrows():
            bar_ticks.append(tick)
            cumulative_dollars += tick['price'] * tick['volume']
            
            if cumulative_dollars >= threshold:
                bar = self._aggregate_bar(bar_ticks)
                bars.append(bar)
                bar_ticks = []
                cumulative_dollars = 0
        
        return pd.DataFrame(bars)
    
    def _aggregate_bar(self, ticks):
        """Aggregate ticks into a single bar"""
        return {
            'timestamp': ticks[-1]['timestamp'],
            'open': ticks[0]['price'],
            'high': max([t['price'] for t in ticks]),
            'low': min([t['price'] for t in ticks]),
            'close': ticks[-1]['price'],
            'volume': sum([t['volume'] for t in ticks]),
            'tick_count': len(ticks),
            'vwap': sum([t['price'] * t['volume'] for t in ticks]) / 
                    sum([t['volume'] for t in ticks])
        }
    
    def add_features(self):
        """Add technical features to bars"""
        if self.bars is None:
            raise ValueError("Must create bars first")
        
        # Returns
        self.bars['returns'] = self.bars['close'].pct_change()
        self.bars['log_returns'] = np.log(self.bars['close']).diff()
        
        # Volatility
        self.bars['volatility'] = self.bars['returns'].rolling(20).std()
        
        # Volume features
        self.bars['volume_ma'] = self.bars['volume'].rolling(20).mean()
        self.bars['volume_ratio'] = self.bars['volume'] / self.bars['volume_ma']
        
        # Price features
        self.bars['high_low_ratio'] = self.bars['high'] / self.bars['low']
        self.bars['close_open_ratio'] = self.bars['close'] / self.bars['open']
        
        return self.bars

# Example usage
tick_data = pd.read_csv('tick_data.csv')
ds = DataStructure(tick_data)
bars = ds.create_bars(bar_type='dollar', threshold=1000000)
features = ds.add_features()
```

---

## Best Practices

### 1. Choose the Right Bar Type

**Recommendations:**
- **Default:** Dollar bars for most applications
- **High-Frequency:** Tick imbalance bars or volume imbalance bars
- **Low-Frequency:** Daily time bars are acceptable
- **Futures:** Account for roll dates and use continuous series

### 2. Determine Optimal Threshold

**Method:** Analyze the distribution of bar intervals

```python
def optimize_bar_threshold(tick_data, bar_type='dollar'):
    """
    Find optimal threshold by analyzing bar interval distribution
    
    Target: Approximately constant number of bars per day
    """
    # Test multiple thresholds
    thresholds = np.logspace(3, 7, 20)  # 1K to 10M
    results = []
    
    for threshold in thresholds:
        bars = create_bars(tick_data, bar_type, threshold)
        bars_per_day = bars.groupby(bars['timestamp'].dt.date).size()
        
        results.append({
            'threshold': threshold,
            'mean_bars_per_day': bars_per_day.mean(),
            'std_bars_per_day': bars_per_day.std(),
            'cv': bars_per_day.std() / bars_per_day.mean()  # Coefficient of variation
        })
    
    results_df = pd.DataFrame(results)
    
    # Choose threshold with lowest coefficient of variation
    optimal = results_df.loc[results_df['cv'].idxmin()]
    
    return optimal['threshold']
```

### 3. Handle Missing Data

**Strategies:**
- **Forward Fill:** Use last known value (appropriate for prices)
- **Interpolation:** Linear or cubic for smooth series
- **Drop:** Remove bars with missing data (if infrequent)
- **Model-Based:** Use Kalman filter or EM algorithm

### 4. Store Data Efficiently

**Use HDF5 for large datasets:**

```python
# Save bars to HDF5
bars.to_hdf('data/bars.h5', key='dollar_bars', mode='w', complevel=9)

# Load specific date range
bars = pd.read_hdf('data/bars.h5', key='dollar_bars',
                   where='timestamp >= "2024-01-01" and timestamp < "2024-02-01"')
```

### 5. Validate Data Quality

```python
def validate_bars(bars):
    """
    Validate bar data quality
    
    Checks:
    - No missing values in OHLC
    - High >= Low
    - High >= Open, Close
    - Low <= Open, Close
    - Timestamps are monotonic
    """
    issues = []
    
    # Check for missing values
    if bars[['open', 'high', 'low', 'close']].isnull().any().any():
        issues.append("Missing values in OHLC")
    
    # Check high/low relationship
    if (bars['high'] < bars['low']).any():
        issues.append("High < Low detected")
    
    # Check high is highest
    if ((bars['high'] < bars['open']) | (bars['high'] < bars['close'])).any():
        issues.append("High is not highest price")
    
    # Check low is lowest
    if ((bars['low'] > bars['open']) | (bars['low'] > bars['close'])).any():
        issues.append("Low is not lowest price")
    
    # Check timestamp order
    if not bars['timestamp'].is_monotonic_increasing:
        issues.append("Timestamps not monotonic")
    
    if issues:
        print("Data quality issues found:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print("Data quality validation passed")
        return True
```

---

## Summary

Proper data structuring is the foundation of successful financial machine learning. Key takeaways:

1. **Information-driven bars** (tick, volume, dollar) are superior to time bars for ML applications
2. **Dollar bars** are the recommended default due to their superior statistical properties
3. **Alternative data** represents a major opportunity for alpha generation
4. **Multi-product series** require careful handling to avoid discontinuities
5. **Event-based sampling** can capture important market dynamics
6. **Data quality validation** is essential before model training

The techniques presented in this document provide a robust foundation for structuring financial data that will lead to better features, better models, and ultimately better trading strategies.

---

## References

1. López de Prado, M. (2018). *Advances in Financial Machine Learning*. Wiley. Chapter 2: Financial Data Structures.
2. Jansen, S. (2020). *Machine Learning for Algorithmic Trading* (2nd Edition). Packt Publishing. Chapters 2-3: Market and Fundamental Data, Alternative Data.
3. Easley, D., López de Prado, M., & O'Hara, M. (2012). "Flow Toxicity and Liquidity in a High-Frequency World." *Review of Financial Studies*, 25(5), 1457-1493.
4. Prado, M. L. D., & Leinweber, D. (2019). "Detection of False Investment Strategies Using Unsupervised Learning Methods." *Quantitative Finance*, 19(9), 1555-1565.

