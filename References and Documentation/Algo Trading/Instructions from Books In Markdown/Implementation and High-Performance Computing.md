# Implementation and High-Performance Computing

## When to Use

- Use this guide when promoting research code into production-grade pipelines that demand vectorization, multiprocessing, or distributed execution.
- Apply it before delegating performance-sensitive tasks to agents so they follow the atoms-and-molecules pattern, async tooling, and deployment practices outlined here.
- Reference it while planning infrastructure upgrades; it summarizes trade-offs between CPUs, GPUs, Dask clusters, and asynchronous workflows.
- Consult it during reliability reviews to verify monitoring, failover, and environment management checkpoints are covered.
- For small prototypes you may operate without these patterns, but any scalable or latency-sensitive workload should align with this document.

**Production-Ready Implementation for Financial ML**

---

## Introduction

Moving from research to production requires efficient implementation, parallel processing, and robust deployment practices.

---

## Vectorization Techniques

### NumPy Vectorization

```python
import numpy as np

# Bad: Loop-based
def calculate_returns_loop(prices):
    returns = np.zeros(len(prices) - 1)
    for i in range(1, len(prices)):
        returns[i-1] = (prices[i] - prices[i-1]) / prices[i-1]
    return returns

# Good: Vectorized
def calculate_returns_vectorized(prices):
    return np.diff(prices) / prices[:-1]

# Benchmark
prices = np.random.randn(10000).cumsum() + 100

%timeit calculate_returns_loop(prices)
# 10 ms

%timeit calculate_returns_vectorized(prices)
# 100 μs (100x faster!)
```

### Pandas Vectorization

```python
import pandas as pd

# Vectorized operations
def vectorized_features(prices):
    """Calculate features using vectorized operations"""
    df = pd.DataFrame({'price': prices})
    
    # All vectorized
    df['returns'] = df['price'].pct_change()
    df['sma_20'] = df['price'].rolling(20).mean()
    df['volatility'] = df['returns'].rolling(20).std()
    df['rsi'] = calculate_rsi_vectorized(df['price'])
    
    return df
```

---

## Multiprocessing Patterns

### The Atoms and Molecules Approach

López de Prado's framework for parallel processing:

```python
import multiprocessing as mp
import pandas as pd

def process_molecule(molecule, func, **kwargs):
    """
    Process a molecule (chunk of work)
    
    Parameters:
    -----------
    molecule : list
        Indices to process
    func : function
        Function to apply
    **kwargs : dict
        Additional arguments
    
    Returns:
    --------
    Results for this molecule
    """
    return func(molecule, **kwargs)

def mp_pandas_obj(func, pd_obj, num_threads=None, **kwargs):
    """
    Parallelize pandas operations
    
    Parameters:
    -----------
    func : function
        Function to apply (must accept 'molecule' argument)
    pd_obj : tuple
        ('molecule', pd.Index or pd.Series)
    num_threads : int
        Number of threads (default: CPU count)
    **kwargs : dict
        Additional arguments for func
    
    Returns:
    --------
    Concatenated results
    """
    if num_threads is None:
        num_threads = mp.cpu_count()
    
    # Split work into molecules
    molecule_key, molecule_data = pd_obj
    atoms = molecule_data.index if isinstance(molecule_data, pd.Series) else molecule_data
    molecules = np.array_split(atoms, min(num_threads, len(atoms)))
    
    # Process in parallel
    pool = mp.Pool(processes=num_threads)
    results = []
    
    for molecule in molecules:
        result = pool.apply_async(
            process_molecule,
            args=(molecule, func),
            kwds=kwargs
        )
        results.append(result)
    
    pool.close()
    pool.join()
    
    # Concatenate results
    results = [r.get() for r in results]
    
    if isinstance(results[0], pd.DataFrame):
        return pd.concat(results)
    elif isinstance(results[0], pd.Series):
        return pd.concat(results)
    else:
        return pd.Series(results)
```

### Example: Parallel Feature Calculation

```python
def calculate_features_single(molecule, prices):
    """Calculate features for a subset of dates"""
    features = pd.DataFrame(index=molecule)
    
    for date in molecule:
        # Get historical data up to this date
        hist_prices = prices[:date]
        
        # Calculate features
        features.loc[date, 'sma_20'] = hist_prices[-20:].mean()
        features.loc[date, 'volatility'] = hist_prices[-20:].std()
    
    return features

# Parallel execution
features = mp_pandas_obj(
    calculate_features_single,
    ('molecule', prices.index),
    num_threads=8,
    prices=prices
)
```

---

## Asynchronous Processing

### Using asyncio for I/O-bound tasks

```python
import asyncio
import aiohttp

async def fetch_price(session, symbol):
    """Fetch price asynchronously"""
    url = f"https://api.example.com/price/{symbol}"
    async with session.get(url) as response:
        return await response.json()

async def fetch_all_prices(symbols):
    """Fetch prices for multiple symbols concurrently"""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_price(session, symbol) for symbol in symbols]
        results = await asyncio.gather(*tasks)
        return results

# Usage
symbols = ['AAPL', 'GOOGL', 'MSFT', 'AMZN']
prices = asyncio.run(fetch_all_prices(symbols))
```

---

## High-Performance Computing (HPC)

### Distributed Computing with Dask

```python
import dask.dataframe as dd
from dask.distributed import Client

# Initialize cluster
client = Client(n_workers=4, threads_per_worker=2)

# Load large dataset
df = dd.read_csv('large_dataset.csv')

# Parallel operations
df['returns'] = df['price'].pct_change()
df['sma'] = df['price'].rolling(20).mean()

# Compute
result = df.compute()
```

### GPU Acceleration with CuPy

```python
import cupy as cp

# GPU-accelerated operations
prices_gpu = cp.array(prices)
returns_gpu = cp.diff(prices_gpu) / prices_gpu[:-1]

# Transfer back to CPU
returns = cp.asnumpy(returns_gpu)
```

---

## Production Deployment

### Deployment Checklist

1. **Code Quality**
   - Unit tests
   - Integration tests
   - Code review
   - Documentation

2. **Data Pipeline**
   - Automated data collection
   - Data validation
   - Error handling
   - Backup systems

3. **Model Deployment**
   - Model versioning
   - A/B testing
   - Gradual rollout
   - Rollback capability

4. **Monitoring**
   - Performance tracking
   - Error logging
   - Alert systems
   - Dashboard

5. **Risk Management**
   - Position limits
   - Loss limits
   - Circuit breakers
   - Manual override

### Production Code Structure

```python
class TradingStrategy:
    """Production trading strategy"""
    
    def __init__(self, config):
        self.config = config
        self.model = self.load_model()
        self.positions = {}
    
    def load_model(self):
        """Load trained model"""
        import joblib
        return joblib.load(self.config['model_path'])
    
    def get_data(self):
        """Fetch latest data"""
        # Implement data fetching
        pass
    
    def generate_signals(self, data):
        """Generate trading signals"""
        features = self.engineer_features(data)
        predictions = self.model.predict(features)
        return predictions
    
    def execute_trades(self, signals):
        """Execute trades based on signals"""
        for symbol, signal in signals.items():
            if signal > 0 and symbol not in self.positions:
                self.buy(symbol, signal)
            elif signal < 0 and symbol in self.positions:
                self.sell(symbol)
    
    def run(self):
        """Main execution loop"""
        while True:
            try:
                # Get data
                data = self.get_data()
                
                # Generate signals
                signals = self.generate_signals(data)
                
                # Execute trades
                self.execute_trades(signals)
                
                # Wait
                time.sleep(self.config['interval'])
                
            except Exception as e:
                self.log_error(e)
                self.send_alert(e)
```

---

## Monitoring and Maintenance

### Performance Monitoring

```python
class PerformanceMonitor:
    """Monitor strategy performance"""
    
    def __init__(self):
        self.trades = []
        self.equity_curve = []
    
    def record_trade(self, trade):
        """Record trade execution"""
        self.trades.append(trade)
    
    def calculate_metrics(self):
        """Calculate performance metrics"""
        returns = pd.Series([t['pnl'] for t in self.trades])
        
        metrics = {
            'sharpe': returns.mean() / returns.std() * np.sqrt(252),
            'max_dd': self.calculate_max_drawdown(),
            'win_rate': (returns > 0).mean(),
            'avg_win': returns[returns > 0].mean(),
            'avg_loss': returns[returns < 0].mean()
        }
        
        return metrics
    
    def send_daily_report(self):
        """Send daily performance report"""
        metrics = self.calculate_metrics()
        
        # Send email/slack notification
        self.notify(f"Daily Report: {metrics}")
```

---

## Best Practices

1. **Vectorize operations** - Avoid loops
2. **Use multiprocessing** for CPU-bound tasks
3. **Use asyncio** for I/O-bound tasks
4. **Test thoroughly** before production
5. **Monitor continuously** in production
6. **Implement fail-safes** - Stop-losses, circuit breakers
7. **Version everything** - Code, models, data
8. **Document extensively** - For future you

---

## References

1. López de Prado, M. (2018). *Advances in Financial Machine Learning*. Wiley. Chapters 20-22.
2. Jansen, S. (2020). *Machine Learning for Algorithmic Trading*. Packt Publishing.
